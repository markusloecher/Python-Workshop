% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{multirow}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Introduction to Data Science in Python},
  pdfauthor={Prof.~Dr.~Markus Löcher},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Introduction to Data Science in Python}
\author{Prof.~Dr.~Markus Löcher}
\date{05/28/2023}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[interior hidden, frame hidden, enhanced, breakable, borderline west={3pt}{0pt}{shadecolor}, boxrule=0pt, sharp corners]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

Python workshop taught by Professor Markus Loecher (HWR Berlin). The
workshop will run in-person June 6 - 9 in room VC 14-280 (from 10am to
1pm and 2 to 5pm).

\hypertarget{about-this-python-workshop}{%
\section*{About this python workshop}\label{about-this-python-workshop}}
\addcontentsline{toc}{section}{About this python workshop}

\hypertarget{goals}{%
\section*{Goals}\label{goals}}
\addcontentsline{toc}{section}{Goals}

This 4 day workshop is intended to introduce participants to the python
language. It is designed to provide the solid foundation needed to
conduct data analysis and visualization for data science. While no
previous experience is required, some basic programming or data science
experience is helpful.

I will lean heavily on the book
\href{https://wesmckinney.com/book/}{Python for Data Analysis} (as well
as the
\href{https://jakevdp.github.io/PythonDataScienceHandbook/}{Python Data
Science Handbook}).

The first day will focus on the fundamentals of data types and flow
structures while the ultimate goal of the course will be to introduce
you to \emph{statistical thinking, data literacy and modeling}.

\hypertarget{datacamp}{%
\section*{DataCamp}\label{datacamp}}
\addcontentsline{toc}{section}{DataCamp}

\href{https://app.datacamp.com/}{DataCamp} is a pretty good resource for
students to learn coding and data analysis skills. By completing the
DataCamp courses listed below we would be able to significantly shorten
the time we spend on basics and open up more space for data science
concepts.

\begin{itemize}
\tightlist
\item
  \href{https://app.datacamp.com/learn/courses/intro-to-python-for-data-science}{Introduction
  to Python}
\item
  \href{https://app.datacamp.com/learn/courses/intermediate-python}{Intermediate
  Python}
\end{itemize}

If you have extra time: *
\href{https://app.datacamp.com/learn/courses/data-manipulation-with-pandas}{Data
Manipulation with pandas}

And much more advanced and totally optional:

\begin{itemize}
\tightlist
\item
  \href{https://www.datacamp.com/courses/python-data-science-toolbox-part-1}{Python
  Data Science Toolbox (Part 1)}
\item
  \href{https://www.datacamp.com/courses/python-data-science-toolbox-part-2}{Python
  Data Science Toolbox (Part 2)}
\item
  \href{https://www.datacamp.com/courses/statistical-thinking-in-python-part-1}{Statistical
  Thinking in Python (Part 1)}
\item
  \href{https://www.datacamp.com/courses/statistical-thinking-in-python-part-2}{Statistical
  Thinking in Python (Part 2)}
\end{itemize}

\hypertarget{coding-environment}{%
\section*{Coding Environment}\label{coding-environment}}
\addcontentsline{toc}{section}{Coding Environment}

The most convenient environment for you to code in might be
\href{https://colab.research.google.com/}{Google Colab}, for which you
probably need a gmail account. It does not hurt to look at the 2-minute
intro video. If you prefer a real IDE, I would recommend
\href{https://visualstudio.microsoft.com/downloads/}{Visual Studio} or
\href{https://www.jetbrains.com/pycharm/download/}{PyCharm}. (I will not
be able to help much with the latter though)

\hypertarget{agenda}{%
\section*{Agenda}\label{agenda}}
\addcontentsline{toc}{section}{Agenda}

\hypertarget{day-1-basic-python-programming}{%
\subsection*{Day 1: Basic python
programming}\label{day-1-basic-python-programming}}
\addcontentsline{toc}{subsection}{Day 1: Basic python programming}

\begin{itemize}
\tightlist
\item
  basic data types: lists, tuples, dictionaries, strings
\item
  control structures (for, if else, while)
\item
  functions
\item
  numpy arrays: slicing and subsetting, axis
\item
  Probabilistic Simulations
\item
  basic plots
\end{itemize}

\hypertarget{day-2-pandas-and-visualization}{%
\subsection*{Day 2: pandas and
visualization}\label{day-2-pandas-and-visualization}}
\addcontentsline{toc}{subsection}{Day 2: pandas and visualization}

\begin{itemize}
\tightlist
\item
  pandas Data Frames: slicing and subsetting
\item
  Counting and Summary Statistics
\item
  Handling Files
\item
  Grouped Operations
\item
  plotting with pandas
\item
  Contingency Tables as models
\end{itemize}

\hypertarget{day-3-statistical-modeling}{%
\subsection*{Day 3: Statistical
Modeling}\label{day-3-statistical-modeling}}
\addcontentsline{toc}{subsection}{Day 3: Statistical Modeling}

\begin{itemize}
\tightlist
\item
  A/B Testing and sampling distributions
\item
  Hypothesis Testing

  \begin{itemize}
  \tightlist
  \item
    parametric
  \item
    permutation
  \item
    the bootstrap
  \end{itemize}
\item
  regression

  \begin{itemize}
  \tightlist
  \item
    simple and multiple
  \item
    logistic
  \item
    categorical variables and interactions
  \item
    regularization
  \end{itemize}
\end{itemize}

\hypertarget{day-4-machine-learning}{%
\subsection*{Day 4: Machine Learning}\label{day-4-machine-learning}}
\addcontentsline{toc}{subsection}{Day 4: Machine Learning}

\begin{itemize}
\tightlist
\item
  Basic ML tools

  \begin{itemize}
  \tightlist
  \item
    Cross Validation
  \item
    sklearn
  \item
    Data Cleaning
  \end{itemize}
\item
  Classification and Regression Trees
\item
  Random Forests and Boosting
\item
  Exlainable ML

  \begin{itemize}
  \tightlist
  \item
    Partial dependence plots
  \item
    SHAP values
  \end{itemize}
\end{itemize}

\hypertarget{if-time-permits}{%
\subsection*{If time permits}\label{if-time-permits}}
\addcontentsline{toc}{subsection}{If time permits}

\begin{itemize}
\tightlist
\item
  Word embeddings such as word2vec\\
\item
  Sentiment analysis
\item
  Internet scraping
\item
  Topic models
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{lecturer}{%
\chapter*{Lecturer}\label{lecturer}}
\addcontentsline{toc}{chapter}{Lecturer}

\href{http://www.hwr-berlin.de/fachbereich-wirtschaftswissenschaften/kontakt/personen/kontakt-info/2184/}{Prof.~Dr.~Markus
Loecher}

Professor for Mathematics and Statistics

\href{http://www.hwr-berlin.de}{Berlin School of Economics and Law}

\href{https://www.linkedin.com/in/loecher/}{linkedin}

\href{https://www.researchgate.net/profile/Markus-Loecher}{researchgate}

\href{https://markusloecher.github.io/}{my blog}

\bookmarksetup{startatroot}

\hypertarget{data-types}{%
\chapter{Data Types}\label{data-types}}

Knowing about data types in Python is crucial for several reasons:

\begin{itemize}
\item
  \textbf{Correctness and reliability}: Understanding data types helps
  ensure that your code operates correctly and produces reliable
  results. By explicitly defining and handling data types, you can avoid
  unexpected errors or inconsistencies in your code.
\item
  \textbf{Memory optimization}: Different data types have varying memory
  requirements. By choosing appropriate data types, you can optimize
  memory usage and improve the performance of your code. For example,
  using integers instead of floating-point numbers can save memory if
  decimal precision is not necessary.
\item
  \textbf{Data manipulation and operations}: Each data type in Python
  has its own set of operations and methods. Understanding data types
  allows you to perform specific operations and manipulate data
  effectively. For example, you can concatenate strings, perform
  arithmetic calculations with numbers, or iterate over elements in a
  list.
\item
  \textbf{Input validation and error handling}: When receiving input
  from users or external sources, it is important to validate and handle
  the data appropriately. Knowing the expected data types allows you to
  validate inputs, handle errors gracefully, and provide meaningful
  feedback to users.
\item
  \textbf{Interoperability and integration}: Python integrates with
  various libraries, frameworks, and external systems. Understanding
  data types helps you exchange data seamlessly between different
  components of your code or integrate with external systems. For
  example, when interacting with a database, you need to understand how
  Python data types map to the database's data types.
\item
  \textbf{Code readability and maintainability}: Explicitly defining
  data types in your code improves readability and makes it easier for
  other developers to understand and maintain your code. It helps convey
  your intentions and makes the code self-documenting, reducing the
  chances of misinterpretation or confusion.
\end{itemize}

Overall, having knowledge of data types in Python enables you to write
robust, efficient, and understandable code that operates correctly with
the data it handles. It empowers you to make informed decisions about
memory usage, data manipulation, input validation, and code integration,
leading to better overall programming proficiency.

\hypertarget{scalar-types}{%
\section{Scalar Types}\label{scalar-types}}

Python has a small set of built-in types for handling numerical data,
strings, Boolean (True or False) values, and dates and time. These
``single value'' types are sometimes called scalar types, and we refer
to them in this book as scalars . See Standard Python scalar types for a
list of the main scalar types. Date and time handling will be discussed
separately, as these are provided by the datetime module in the standard
library.

\begin{table}

\caption{\textbf{?(caption)}}\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

Standard Python scalar types

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

Type

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

Description

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

None

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

The Python ``null'' value (only one instance of the None object exists)

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

str

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

String type; holds Unicode strings

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

bytes

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

Raw binary data

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

float

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

Double-precision floating-point number (note there is no separate double
type)

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

bool

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

A Boolean True or False value

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

int

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

Arbitrary precision integer

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

}

\end{minipage}%

\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=} \DecValTok{5}
\BuiltInTok{type}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
int
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y}\OperatorTok{=}\FloatTok{7.5}
\BuiltInTok{type}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
float
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{type}\NormalTok{(x}\OperatorTok{+}\NormalTok{y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
float
\end{verbatim}

\hypertarget{numeric-type}{%
\subsection{Numeric Type}\label{numeric-type}}

The primary Python types for numbers are int and float. An int can store
arbitrarily large numbers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ival }\OperatorTok{=} \DecValTok{17239871}
\NormalTok{ival }\OperatorTok{**} \DecValTok{6}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
26254519291092456596965462913230729701102721
\end{verbatim}

\hypertarget{booleans}{%
\subsection{Booleans}\label{booleans}}

can take only two values: \texttt{True} and \texttt{False}.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\DecValTok{4} \OperatorTok{==} \DecValTok{5}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\DecValTok{4} \OperatorTok{\textless{}} \DecValTok{5}\NormalTok{)}
\NormalTok{b }\OperatorTok{=} \DecValTok{4} \OperatorTok{!=} \DecValTok{5}
\BuiltInTok{print}\NormalTok{(b)}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{int}\NormalTok{(b))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
False
True
True
1
\end{verbatim}

\hypertarget{strings}{%
\section{Strings}\label{strings}}

Many people use Python for its built-in string handling capabilities.
You can write string literals using either single quotes ' or double
quotes `` (double quotes are generally favored):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a }\OperatorTok{=} \StringTok{\textquotesingle{}one way of writing a string\textquotesingle{}}
\NormalTok{b }\OperatorTok{=} \StringTok{"another way"}

\BuiltInTok{type}\NormalTok{(a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
str
\end{verbatim}

For multiline strings with line breaks, you can use triple quotes,
either \texttt{\textquotesingle{}\textquotesingle{}\textquotesingle{}}
or \texttt{"""}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c }\OperatorTok{=} \StringTok{"""}
\StringTok{This is a longer string that}
\StringTok{spans multiple lines}
\StringTok{"""}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c.count(}\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
3
\end{verbatim}

\hypertarget{strings-built-in-methods}{%
\subsubsection{\texorpdfstring{Strings built-in
\emph{methods}}{Strings built-in methods}}\label{strings-built-in-methods}}

Note: All string methods return new values. They do not change the
original string.

Method

Description

capitalize()

Converts the first character to upper case

casefold()

Converts string into lower case

center()

Returns a centered string

count()

Returns the number of times a specified value occurs in a string

encode()

Returns an encoded version of the string

endswith()

Returns true if the string ends with the specified value

expandtabs()

Sets the tab size of the string

find()

Searches the string for a specified value and returns the position of
where it was found

format()

Formats specified values in a string

format\_map()

Formats specified values in a string

index()

Searches the string for a specified value and returns the position of
where it was found

isalnum()

Returns True if all characters in the string are alphanumeric

isalpha()

Returns True if all characters in the string are in the alphabet

isascii()

Returns True if all characters in the string are ascii characters

isdecimal()

Returns True if all characters in the string are decimals

isdigit()

Returns True if all characters in the string are digits

isidentifier()

Returns True if the string is an identifier

islower()

Returns True if all characters in the string are lower case

isnumeric()

Returns True if all characters in the string are numeric

isprintable()

Returns True if all characters in the string are printable

isspace()

Returns True if all characters in the string are whitespaces

istitle()

Returns True if the string follows the rules of a title

isupper()

Returns True if all characters in the string are upper case

join()

Converts the elements of an iterable into a string

ljust()

Returns a left justified version of the string

lower()

Converts a string into lower case

lstrip()

Returns a left trim version of the string

maketrans()

Returns a translation table to be used in translations

partition()

Returns a tuple where the string is parted into three parts

replace()

Returns a string where a specified value is replaced with a specified
value

rfind()

Searches the string for a specified value and returns the last position
of where it was found

rindex()

Searches the string for a specified value and returns the last position
of where it was found

rjust()

Returns a right justified version of the string

rpartition()

Returns a tuple where the string is parted into three parts

rsplit()

Splits the string at the specified separator, and returns a list

rstrip()

Returns a right trim version of the string

split()

Splits the string at the specified separator, and returns a list

splitlines()

Splits the string at line breaks and returns a list

startswith()

Returns true if the string starts with the specified value

strip()

Returns a trimmed version of the string

swapcase()

Swaps cases, lower case becomes upper case and vice versa

title()

Converts the first character of each word to upper case

translate()

Returns a translated string

upper()

Converts a string into upper case

zfill()

Fills the string with a specified number of 0 values at the beginning

\hypertarget{methods}{%
\subsection{Methods}\label{methods}}

Many operations/functions in python are specific to the data type even
though we use the same syntax:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(x}\OperatorTok{+}\NormalTok{y)}
\BuiltInTok{print}\NormalTok{(a}\OperatorTok{+}\NormalTok{b)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
12.5
one way of writing a stringanother way
\end{verbatim}

\hypertarget{type-conversion}{%
\subsubsection{Type conversion}\label{type-conversion}}

We can often convert from one type to another if it makes sense:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{str}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'5'
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{float}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
5.0
\end{verbatim}

\hypertarget{immutable-objects}{%
\section{Immutable Objects}\label{immutable-objects}}

Mutable objects are those that allow you to change their value or data
in place without affecting the object's identity. In contrast, immutable
objects don't allow this kind of operation. You'll just have the option
of creating new objects of the same type with different values.

In Python, mutability is a characteristic that may profoundly influence
your decision when choosing which data type to use in solving a given
programming problem. Therefore, you need to know how mutable and
immutable objects work in Python.

In Python, variables don't have an associated type or size, as they're
labels attached to objects in memory. They point to the memory position
where concrete objects live. In other words, a Python variable is a name
that refers to or holds a reference to a concrete object. In contrast,
Python objects are concrete pieces of information that live in specific
memory positions on your computer.

The main takeaway here is that variables and objects are two different
animals in Python:

\begin{itemize}
\tightlist
\item
  \textbf{Variables} hold references to objects.
\item
  \textbf{Objects} live in concrete memory positions.
\end{itemize}

\href{https://realpython.com/python-mutable-vs-immutable-types/}{Read
more about this topic}

Strings and tuples are immutable:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a }\OperatorTok{=} \StringTok{"this is a string"}

\NormalTok{a[}\DecValTok{10}\NormalTok{] }\OperatorTok{=} \StringTok{"f"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
TypeError: 'str' object does not support item assignment
\end{verbatim}

\hypertarget{tuples}{%
\section{Tuples}\label{tuples}}

A tuple is a fixed-length, immutable sequence of Python objects which,
once assigned, cannot be changed. The easiest way to create one is with
a comma-separated sequence of values wrapped in parentheses:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tup }\OperatorTok{=}\NormalTok{ (}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(tup)}
\NormalTok{tup }\OperatorTok{=}\NormalTok{ (}\DecValTok{4}\NormalTok{, }\StringTok{"Ray"}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(tup)}
\CommentTok{\#In many contexts, the parentheses can be omitted}
\NormalTok{tup }\OperatorTok{=} \DecValTok{4}\NormalTok{, }\StringTok{"Ray"}\NormalTok{, }\DecValTok{6}
\BuiltInTok{print}\NormalTok{(tup)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(4, 5, 6)
(4, 'Ray', 6)
(4, 'Ray', 6)
\end{verbatim}

Elements can be accessed with square brackets {[}{]} as with most other
sequence types. As in C, C++, Java, and many other languages, sequences
are 0-indexed in Python:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tup[}\DecValTok{0}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#but you cannot change the value:}
\NormalTok{tup[}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
TypeError: 'tuple' object does not support item assignment
\end{verbatim}

You can concatenate tuples using the \texttt{+} operator to produce
longer tuples:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DecValTok{4}\NormalTok{, }\VariableTok{None}\NormalTok{, }\StringTok{\textquotesingle{}foo\textquotesingle{}}\NormalTok{) }\OperatorTok{+}\NormalTok{ (}\DecValTok{6}\NormalTok{, }\DecValTok{0}\NormalTok{) }\OperatorTok{+}\NormalTok{ (}\StringTok{\textquotesingle{}bar\textquotesingle{}}\NormalTok{,)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(4, None, 'foo', 6, 0, 'bar')
\end{verbatim}

Multiplying a tuple by an integer, as with lists, has the effect of
concatenating that many copies of the tuple:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\StringTok{\textquotesingle{}foo\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}bar\textquotesingle{}}\NormalTok{) }\OperatorTok{*} \DecValTok{4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
('foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'bar')
\end{verbatim}

\hypertarget{unpacking-tuples}{%
\subsubsection{Unpacking tuples}\label{unpacking-tuples}}

If you try to assign to a tuple-like expression of variables, Python
will attempt to unpack the value on the righthand side of the equals
sign:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tup }\OperatorTok{=}\NormalTok{ (}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\NormalTok{a, b, c }\OperatorTok{=}\NormalTok{ tup}
\end{Highlighting}
\end{Shaded}

\bookmarksetup{startatroot}

\hypertarget{lists-and-loops}{%
\chapter{Lists and Loops}\label{lists-and-loops}}

In this lesson we will get to know and become experts in:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Lists

  \begin{itemize}
  \tightlist
  \item
    DataCamp,
    \href{https://app.datacamp.com/learn/courses/intro-to-python-for-data-science}{Introduction
    to Python}, Chap 2
  \end{itemize}
\item
  Loops

  \begin{itemize}
  \tightlist
  \item
    DataCamp,
    \href{https://campus.datacamp.com/courses/intermediate-python}{Intermediate
    Python}, Chap 4
  \end{itemize}
\item
  Conditions

  \begin{itemize}
  \tightlist
  \item
    DataCamp,
    \href{https://campus.datacamp.com/courses/intermediate-python}{Intermediate
    Python}, Chap 3
  \end{itemize}
\end{enumerate}

\hypertarget{list}{%
\subsection{List}\label{list}}

In contrast with tuples, lists are variable length and their contents
can be modified in place. Lists are mutable. You can define them using
square brackets \texttt{{[}{]}} or using the \texttt{list} type
function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fam }\OperatorTok{=}\NormalTok{ [}\FloatTok{1.73}\NormalTok{, }\FloatTok{1.68}\NormalTok{, }\FloatTok{1.71}\NormalTok{, }\FloatTok{1.89}\NormalTok{]}
\NormalTok{fam }\OperatorTok{=} \BuiltInTok{list}\NormalTok{((}\FloatTok{1.73}\NormalTok{, }\FloatTok{1.68}\NormalTok{, }\FloatTok{1.71}\NormalTok{, }\FloatTok{1.89}\NormalTok{))}
\NormalTok{fam}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1.73, 1.68, 1.71, 1.89]
\end{verbatim}

You can \texttt{sort}, \texttt{append},\texttt{insert}, concatenate,
\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#sorting is in place!}
\NormalTok{fam.sort()}
\NormalTok{fam}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fam.append(}\FloatTok{2.05}\NormalTok{)}
\NormalTok{fam}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fam }\OperatorTok{+}\NormalTok{ fam}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1.73, 1.68, 1.71, 1.89, 1.73, 1.68, 1.71, 1.89]
\end{verbatim}

Lists can

\begin{itemize}
\tightlist
\item
  Contain any type
\item
  Contain different types
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fam2 }\OperatorTok{=}\NormalTok{ [}\StringTok{"liz"}\NormalTok{, }\FloatTok{1.73}\NormalTok{, }\StringTok{"emma"}\NormalTok{, }\FloatTok{1.68}\NormalTok{, }\StringTok{"mom"}\NormalTok{, }\FloatTok{1.71}\NormalTok{, }\StringTok{"dad"}\NormalTok{, }\FloatTok{1.89}\NormalTok{]}
\NormalTok{fam2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['liz', 1.73, 'emma', 1.68, 'mom', 1.71, 'dad', 1.89]
\end{verbatim}

\hypertarget{list-of-lists}{%
\subsection{List of lists}\label{list-of-lists}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fam3 }\OperatorTok{=}\NormalTok{ [[}\StringTok{"liz"}\NormalTok{, }\FloatTok{1.73}\NormalTok{],}
\NormalTok{         [}\StringTok{"emma"}\NormalTok{, }\FloatTok{1.68}\NormalTok{],}
\NormalTok{         [}\StringTok{"mom"}\NormalTok{, }\FloatTok{1.71}\NormalTok{],}
\NormalTok{         [}\StringTok{"dad"}\NormalTok{, }\FloatTok{1.89}\NormalTok{]]}
\NormalTok{fam3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[['liz', 1.73], ['emma', 1.68], ['mom', 1.71], ['dad', 1.89]]
\end{verbatim}

\hypertarget{slicing}{%
\subsection{Slicing}\label{slicing}}

You can select sections of most sequence types by using slice notation,
which in its basic form consists of start:stop passed to the indexing
operator {[}{]}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fam[}\DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1.68, 1.71]
\end{verbatim}

While the element at the start index is included, the stop index is not
included, so that the number of elements in the result is stop - start.

Either the start or stop can be omitted, in which case they default to
the start of the sequence and the end of the sequence, respectively:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(fam[}\DecValTok{1}\NormalTok{:])}
\BuiltInTok{print}\NormalTok{(fam[:}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1.68, 1.71, 1.89]
[1.73, 1.68, 1.71]
\end{verbatim}

Negative indices slice the sequence relative to the end:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fam[}\OperatorTok{{-}}\DecValTok{4}\NormalTok{:]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1.73, 1.68, 1.71, 1.89]
\end{verbatim}

Slicing semantics takes a bit of getting used to, especially if you're
coming from R or MATLAB. See this helpful illustration of slicing with
positive and negative integers. In the figure, the indices are shown at
the ``bin edges'' to help show where the slice selections start and stop
using positive or negative indices.

Illustration of Python slicing conventions

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{manipulating-lists-of-lists}{%
\subsection{Manipulating lists of
lists}\label{manipulating-lists-of-lists}}

The following list of lists contains names of sections in a house and
their area.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Extract the area corresponding to kitchen
\item
  String Tasks:

  \begin{itemize}
  \tightlist
  \item
    Extract the first letters of each string
  \item
    Capitalize all strings
  \item
    Replace all occurrences of ``room'' with ``rm''
  \item
    count the number of ``l'' in ``hallway''
  \end{itemize}
\item
  Insert a ``home office'' with area 10.75 after living room
\item
  Append the total area to the end of the list
\item
  \textbf{Boolean} operations:

  \begin{itemize}
  \tightlist
  \item
    Generate one True and one False by comparing areas
  \item
    Generate one True and one False by comparing names
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{house }\OperatorTok{=}\NormalTok{ [[}\StringTok{\textquotesingle{}hallway\textquotesingle{}}\NormalTok{, }\FloatTok{11.25}\NormalTok{],}
\NormalTok{ [}\StringTok{\textquotesingle{}kitchen\textquotesingle{}}\NormalTok{, }\FloatTok{18.0}\NormalTok{],}
\NormalTok{ [}\StringTok{\textquotesingle{}living room\textquotesingle{}}\NormalTok{, }\FloatTok{20.0}\NormalTok{],}
\NormalTok{ [}\StringTok{\textquotesingle{}bedroom\textquotesingle{}}\NormalTok{, }\FloatTok{10.75}\NormalTok{],}
\NormalTok{ [}\StringTok{\textquotesingle{}bathroom\textquotesingle{}}\NormalTok{, }\FloatTok{9.5}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\hypertarget{automation-by-iterating}{%
\subsection{Automation by iterating}\label{automation-by-iterating}}

\textbf{for loops} are a powerful way of automating MANY otherwise
tedious tasks that repeat.

The syntax template is (watch the indentation!):

\begin{verbatim}
for var in seq :
     expression


::: {.cell execution_count=23}
``` {.python .cell-code}
#We can use lists:
for f in fam:
    print(f)
```

::: {.cell-output .cell-output-stdout}
```
1.73
1.68
1.71
1.89
```
:::
:::


::: {.cell execution_count=24}
``` {.python .cell-code}
#or iterators
for i in range(len(fam)):
    print(fam[i])
```

::: {.cell-output .cell-output-stdout}
```
1.73
1.68
1.71
1.89
```
:::
:::


::: {.cell execution_count=26}
``` {.python .cell-code}
#or enumerate
for i,f in enumerate(fam):
    print(fam[i], f)

```

::: {.cell-output .cell-output-stdout}
```
1.73 1.73
1.68 1.68
1.71 1.71
1.89 1.89
```
:::
:::


### Iterators (Optional)

An iterator is an object that contains a countable number of values.

An iterator is an object that can be iterated upon, meaning that you can traverse through all the values.

Technically, in Python, an iterator is an object which implements the iterator protocol, which consist of the methods `iter()` and `next()`.

::: {.cell execution_count=5}
``` {.python .cell-code}
mytuple = ("apple", "banana", "cherry")
myit = iter(mytuple)

print(next(myit))
print(next(myit))

```

::: {.cell-output .cell-output-stdout}
```
apple
banana
```
:::
:::


::: {.cell execution_count=6}
``` {.python .cell-code}
mystr = "banana"
myit = iter(mystr)

print(next(myit))
print(next(myit))
```

::: {.cell-output .cell-output-stdout}
```
b
a
```
:::
:::


Strings are also iterable objects, containing a sequence of characters:

::: {.cell execution_count=4}
``` {.python .cell-code}
#funny iterators
print(range(5))
list(range(5))
```

::: {.cell-output .cell-output-stdout}
```
range(0, 5)
```
:::

::: {.cell-output .cell-output-display execution_count=4}
```
[0, 1, 2, 3, 4]
```
:::
:::


1. Repeat the tasks 2 and 4 from above by using a for loop
    - using `enumerate`
    - using `range`
2. Create two separates new lists which contain only the names and areas separately
3. [Clever Carl](https://nrich.maths.org/2478#:~:text=Gauss%20added%20the%20rows%20pairwise,quantity%20in%20a%20clever%20way.): Compute 
$$
\sum_{i=1}^{100}{i}
$$ 


::: {.cell execution_count=27}
``` {.python .cell-code}

```

::: {.cell-output .cell-output-display execution_count=27}
```
[0, 1, 2, 3, 4]
```
:::
:::


### Conditions

The syntax template is (watch the indentation!):
\end{verbatim}

if condition: expression

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ f }\KeywordTok{in}\NormalTok{ fam:}
    \ControlFlowTok{if}\NormalTok{ f }\OperatorTok{\textgreater{}} \FloatTok{1.8}\NormalTok{:}
        \BuiltInTok{print}\NormalTok{(f)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
1.89
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find the \textbf{max} of the areas by using \texttt{if} inside a for
  loop
\item
  Print those elements of the list with

  \begin{itemize}
  \tightlist
  \item
    area \(> 15\)
  \item
    strings that contain ``room'' (or ``rm'' after your substitution)
  \end{itemize}
\end{enumerate}

\bookmarksetup{startatroot}

\hypertarget{dictionaries-and-functions}{%
\chapter{Dictionaries and Functions}\label{dictionaries-and-functions}}

In this lesson we will get to know and become experts in:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{Functions}{Functions}

  \begin{itemize}
  \tightlist
  \item
    DataCamp,
    \href{https://app.datacamp.com/learn/courses/intro-to-python-for-data-science}{Introduction
    to Python}, Chap 3
  \end{itemize}
\item
  \protect\hyperlink{Dictionaries}{Dictionaries}

  \begin{itemize}
  \tightlist
  \item
    DataCamp,
    \href{https://campus.datacamp.com/courses/intermediate-python}{Intermediate
    Python}, Chap 2
  \end{itemize}
\item
  \protect\hyperlink{Introduction-to-numpy}{Introduction to numpy}

  \begin{itemize}
  \tightlist
  \item
    DataCamp,
    \href{https://app.datacamp.com/learn/courses/intro-to-python-for-data-science}{Introduction
    to Python}, Chap 4
  \end{itemize}
\end{enumerate}

\hypertarget{functions}{%
\subsection{Functions}\label{functions}}

\href{https://wesmckinney.com/book/python-builtin.html\#functions}{Functions}
are essential building blocks to \textbf{reuse code} and to
\textbf{modularize code}.

We have already seen and used many \textbf{built-in functions/methods}
such as \texttt{print()}, \texttt{len()}, \texttt{max()},
\texttt{round()}, \texttt{index()}, \texttt{capitalize()}, etc..

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{areas }\OperatorTok{=}\NormalTok{ [}\FloatTok{11.25}\NormalTok{, }\FloatTok{18.0}\NormalTok{, }\FloatTok{20.0}\NormalTok{, }\FloatTok{10.75}\NormalTok{, }\FloatTok{10.75}\NormalTok{, }\FloatTok{9.5}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{max}\NormalTok{(areas))}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{len}\NormalTok{(areas))}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{round}\NormalTok{(}\FloatTok{10.75}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(areas.index(}\FloatTok{18.0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
20.0
6
10.8
1
\end{verbatim}

But of course we want to define our own functions as well ! As a rule of
thumb, if you anticipate needing to repeat the same or very similar code
more than once, it may be worth writing a reusable function. Functions
can also help make your code more readable by giving a name to a group
of Python statements.

For example, we computed the BMI previously as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{height }\OperatorTok{=} \FloatTok{1.79}
\NormalTok{weight }\OperatorTok{=} \FloatTok{68.7}
\NormalTok{bmi }\OperatorTok{=}\NormalTok{ weight}\OperatorTok{/}\NormalTok{height}\OperatorTok{**}\DecValTok{2}
\BuiltInTok{print}\NormalTok{(bmi)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
21.44127836209856
\end{verbatim}

Functions are declared with the \texttt{def} keyword. A function
contains a block of code with an optional use of the \texttt{return}
keyword:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ compute\_bmi(height, weight):}
    \ControlFlowTok{return}\NormalTok{ weight}\OperatorTok{/}\NormalTok{height}\OperatorTok{**}\DecValTok{2}

\NormalTok{compute\_bmi(}\FloatTok{1.79}\NormalTok{, }\FloatTok{68.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
21.44127836209856
\end{verbatim}

Each function can have \emph{positional} arguments and \emph{keyword}
arguments. Keyword arguments are most commonly used to specify default
values or optional arguments. For example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ compute\_bmi(height, weight, ndigits}\OperatorTok{=}\DecValTok{2}\NormalTok{):}
    \ControlFlowTok{return} \BuiltInTok{round}\NormalTok{(weight}\OperatorTok{/}\NormalTok{height}\OperatorTok{**}\DecValTok{2}\NormalTok{, ndigits)}

\BuiltInTok{print}\NormalTok{(compute\_bmi(}\FloatTok{1.79}\NormalTok{, }\FloatTok{68.7}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(compute\_bmi(}\FloatTok{1.79}\NormalTok{, }\FloatTok{68.7}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
21.44
21.4413
\end{verbatim}

\hypertarget{multiple-return-values}{%
\subsubsection{Multiple Return Values}\label{multiple-return-values}}

are easily possible in python:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ compute\_bmi(height, weight, ndigits}\OperatorTok{=}\DecValTok{2}\NormalTok{):}
\NormalTok{    bmi }\OperatorTok{=} \BuiltInTok{round}\NormalTok{(weight}\OperatorTok{/}\NormalTok{height}\OperatorTok{**}\DecValTok{2}\NormalTok{, ndigits)}
    \CommentTok{\#https://www.cdc.gov/healthyweight/assessing/index.html\#:\textasciitilde{}:text=If\%20your\%20BMI\%20is\%20less,falls\%20within\%20the\%20obese\%20range.}
    \ControlFlowTok{if}\NormalTok{ bmi }\OperatorTok{\textless{}} \FloatTok{18.5}\NormalTok{:}
\NormalTok{        status}\OperatorTok{=}\StringTok{"underweight"}
    \ControlFlowTok{elif}\NormalTok{ bmi }\OperatorTok{\textless{}=} \FloatTok{24.9}\NormalTok{:}
\NormalTok{        status}\OperatorTok{=}\StringTok{"healthy"}
    \ControlFlowTok{elif}\NormalTok{ bmi }\OperatorTok{\textless{}=} \FloatTok{29.9}\NormalTok{:}
\NormalTok{        status}\OperatorTok{=}\StringTok{"underweight"}
    \ControlFlowTok{elif}\NormalTok{ bmi }\OperatorTok{\textgreater{}=} \DecValTok{30}\NormalTok{:}\CommentTok{\#note that a simple else would suffice here!}
\NormalTok{        status}\OperatorTok{=}\StringTok{"obese"}
    \ControlFlowTok{return}\NormalTok{ bmi, status}

\BuiltInTok{print}\NormalTok{(compute\_bmi(}\FloatTok{1.79}\NormalTok{, }\FloatTok{68.7}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(compute\_bmi(}\FloatTok{1.79}\NormalTok{, }\DecValTok{55}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(21.44, 'healthy')
(17.17, 'underweight')
\end{verbatim}

Recall from the previous lab how we

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  found the largest room,
\item
  computed the sum of integers from 1 to 100
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#find the maximum area:}
\NormalTok{areas }\OperatorTok{=}\NormalTok{ [}\FloatTok{11.25}\NormalTok{, }\FloatTok{18.0}\NormalTok{, }\FloatTok{20.0}\NormalTok{, }\FloatTok{10.75}\NormalTok{, }\FloatTok{10.75}\NormalTok{, }\FloatTok{9.5}\NormalTok{]}
\NormalTok{currentMax }\OperatorTok{=}\NormalTok{ areas[}\DecValTok{0}\NormalTok{] }\CommentTok{\# initialize to the first area seen}

\ControlFlowTok{for}\NormalTok{ a }\KeywordTok{in}\NormalTok{ areas:}
  \ControlFlowTok{if}\NormalTok{ a }\OperatorTok{\textgreater{}}\NormalTok{ currentMax:}
\NormalTok{    currentMax }\OperatorTok{=}\NormalTok{ a}

\BuiltInTok{print}\NormalTok{(}\StringTok{"The max is:"}\NormalTok{, currentMax)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The max is: 20.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Clever IDB students: Compute the sum from 1 to 100:}
\NormalTok{Total }\OperatorTok{=}\DecValTok{0}

\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{101}\NormalTok{):}\CommentTok{\#strictly speaking we are adding the first  0 }
\NormalTok{  Total }\OperatorTok{=}\NormalTok{ Total }\OperatorTok{+}\NormalTok{ i}
  \CommentTok{\#Total += i}

\BuiltInTok{print}\NormalTok{(Total)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tasks}{%
\subsubsection{Tasks}\label{tasks}}

Write your own function

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  to find the min and max of a list
\item
  to compute the Gauss sum with defaukt values \(m=1, n=100\)
\end{enumerate}

\[
\sum_{i=m}^{n}{i}
\]

\hypertarget{namespaces-and-scope}{%
\subsubsection{Namespaces and Scope}\label{namespaces-and-scope}}

Functions seem straightforward. But one of the more confusing aspects in
the beginning is the concept that we can have \textbf{multiple
instances} of the same variable!

Functions can access variables created inside the function as well as
those outside the function in higher (or even global) scopes. An
alternative and more descriptive name describing a variable scope in
Python is a \emph{namespace}. Any variables that are assigned within a
function by default are assigned to the local namespace. The local
namespace is created when the function is called and is immediately
populated by the function's arguments. After the function is finished,
the local namespace is destroyed.

Examples:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{height }\OperatorTok{=} \FloatTok{1.79}
\NormalTok{weight }\OperatorTok{=} \FloatTok{68.7}
\NormalTok{bmi }\OperatorTok{=}\NormalTok{ weight}\OperatorTok{/}\NormalTok{height}\OperatorTok{**}\DecValTok{2}
\CommentTok{\#print("height, weight, bmi OUTSIDE the function:",height, weight,bmi)}

\KeywordTok{def}\NormalTok{ compute\_bmi(h, w):}
\NormalTok{    height }\OperatorTok{=}\NormalTok{ h}
\NormalTok{    weight }\OperatorTok{=}\NormalTok{ w}
\NormalTok{    bmi }\OperatorTok{=} \BuiltInTok{round}\NormalTok{(weight}\OperatorTok{/}\NormalTok{height}\OperatorTok{**}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\NormalTok{    status}\OperatorTok{=}\StringTok{"healthy"}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"height, weight, bmi INSIDE the function:"}\NormalTok{,height, weight,bmi)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"status:"}\NormalTok{, status)}
    \ControlFlowTok{return}\NormalTok{ bmi}

\NormalTok{compute\_bmi(}\FloatTok{1.55}\NormalTok{, }\DecValTok{50}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"height, weight, bmi OUTSIDE the function:"}\NormalTok{,height, weight,bmi)}
\CommentTok{\#print(status)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
height, weight, bmi INSIDE the function: 1.55 50 20.81
status: healthy
height, weight, bmi OUTSIDE the function: 1.79 68.7 21.44127836209856
\end{verbatim}

\hypertarget{dictionaries}{%
\subsection{Dictionaries}\label{dictionaries}}

A
\href{https://wesmckinney.com/book/python-builtin.html\#dict}{dictionary}
is basically a \textbf{lookup table}. It stores a collection of
key-value pairs, where key and value are Python objects. Each key is
associated with a value so that a value can be conveniently retrieved,
inserted, modified, or deleted given a particular key.

The dictionary or \texttt{dict} may be the most important built-in
Python data structure. In other programming languages, dictionaries are
sometimes called \emph{hash maps} or \emph{associative arrays}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#This was the house defined as a list of lists:}
\NormalTok{house }\OperatorTok{=}\NormalTok{ [[}\StringTok{\textquotesingle{}hallway\textquotesingle{}}\NormalTok{, }\FloatTok{11.25}\NormalTok{],}
\NormalTok{ [}\StringTok{\textquotesingle{}kitchen\textquotesingle{}}\NormalTok{, }\FloatTok{18.0}\NormalTok{],}
\NormalTok{ [}\StringTok{\textquotesingle{}living room\textquotesingle{}}\NormalTok{, }\FloatTok{20.0}\NormalTok{],}
\NormalTok{ [}\StringTok{\textquotesingle{}bedroom\textquotesingle{}}\NormalTok{, }\FloatTok{10.75}\NormalTok{],}
\NormalTok{ [}\StringTok{\textquotesingle{}bathroom\textquotesingle{}}\NormalTok{, }\FloatTok{9.5}\NormalTok{]]}

\CommentTok{\#Remember all the disadvantages of accessing elements}

\CommentTok{\#Better as a lookup table:}
\NormalTok{house }\OperatorTok{=}\NormalTok{ \{}\StringTok{\textquotesingle{}hallway\textquotesingle{}}\NormalTok{: }\FloatTok{11.25}\NormalTok{,}
    \StringTok{\textquotesingle{}kitchen\textquotesingle{}}\NormalTok{: }\FloatTok{18.0}\NormalTok{,}
    \StringTok{\textquotesingle{}living room\textquotesingle{}}\NormalTok{: }\FloatTok{20.0}\NormalTok{,}
    \StringTok{\textquotesingle{}bedroom\textquotesingle{}}\NormalTok{: }\FloatTok{10.75}\NormalTok{,}
    \StringTok{\textquotesingle{}bathroom\textquotesingle{}}\NormalTok{: }\FloatTok{9.5}\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europe }\OperatorTok{=}\NormalTok{ \{}\StringTok{\textquotesingle{}spain\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}madrid\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}france\textquotesingle{}}\NormalTok{ : }\StringTok{\textquotesingle{}paris\textquotesingle{}}\NormalTok{\}}
\BuiltInTok{print}\NormalTok{(europe[}\StringTok{"spain"}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"france"} \KeywordTok{in}\NormalTok{ europe)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"paris"} \KeywordTok{in}\NormalTok{ europe)}\CommentTok{\#only checks the keys!}
\NormalTok{europe[}\StringTok{"germany"}\NormalTok{] }\OperatorTok{=} \StringTok{"berlin"}
\BuiltInTok{print}\NormalTok{(europe.keys())}
\BuiltInTok{print}\NormalTok{(europe.values())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
madrid
True
False
dict_keys(['spain', 'france', 'germany'])
dict_values(['madrid', 'paris', 'berlin'])
\end{verbatim}

If you need to iterate over both the keys and values, you can use the
\texttt{items} method to iterate over the keys and values as 2-tuples:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#print(list(europe.items()))}

\ControlFlowTok{for}\NormalTok{ country, capital }\KeywordTok{in}\NormalTok{ europe.items():}
    \BuiltInTok{print}\NormalTok{(capital, }\StringTok{"is the capital of"}\NormalTok{, country)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
madrid is the capital of spain
paris is the capital of france
berlin is the capital of germany
\end{verbatim}

\textbf{Note}: You can use integers as keys as well. However -unlike in
lists- one should not think of them as positional indices!

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Assume you have a basement:}
\NormalTok{house[}\DecValTok{0}\NormalTok{] }\OperatorTok{=} \FloatTok{21.5}
\NormalTok{house}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'hallway': 11.25,
 'kitchen': 18.0,
 'living room': 20.0,
 'bedroom': 10.75,
 'bathroom': 9.5,
 0: 21.5}
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#And there is a difference between the string and the integer index!}
\NormalTok{house[}\StringTok{"0"}\NormalTok{] }\OperatorTok{=} \FloatTok{30.5}
\NormalTok{house}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'hallway': 11.25,
 'kitchen': 18.0,
 'living room': 20.0,
 'bedroom': 10.75,
 'bathroom': 9.5,
 0: 21.5}
\end{verbatim}

Categorize a list of words by their first letters as a dictionary of
lists:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{words }\OperatorTok{=}\NormalTok{ [}\StringTok{"apple"}\NormalTok{, }\StringTok{"bat"}\NormalTok{, }\StringTok{"bar"}\NormalTok{, }\StringTok{"atom"}\NormalTok{, }\StringTok{"book"}\NormalTok{]}

\NormalTok{by\_letter }\OperatorTok{=}\NormalTok{ \{\}}

\ControlFlowTok{for}\NormalTok{ word }\KeywordTok{in}\NormalTok{ words:}
\NormalTok{     letter }\OperatorTok{=}\NormalTok{ word[}\DecValTok{0}\NormalTok{]}
     \ControlFlowTok{if}\NormalTok{ letter }\KeywordTok{not} \KeywordTok{in}\NormalTok{ by\_letter:}
\NormalTok{        by\_letter[letter] }\OperatorTok{=}\NormalTok{ [word]}
     \ControlFlowTok{else}\NormalTok{:}
\NormalTok{         by\_letter[letter].append(word)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}
\end{verbatim}

\hypertarget{tasks-1}{%
\subsubsection{Tasks}\label{tasks-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find the maximum of the areas of the houses
\item
  Remove the two last entries.
\item
  Write a function named word\_count that takes a string as input and
  returns a dictionary with each word in the string as a key and the
  number of times it appears as the value.
\end{enumerate}

\hypertarget{introduction-to-numpy}{%
\subsection{Introduction to numpy}\label{introduction-to-numpy}}

NumPy, short for Numerical Python, is one of the most important
foundational packages for numerical computing in Python.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Vectorized, fast mathematical operations.
\item
  Key features of NumPy is its N-dimensional array object, or ndarray
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{height }\OperatorTok{=}\NormalTok{ [}\FloatTok{1.79}\NormalTok{, }\FloatTok{1.85}\NormalTok{, }\FloatTok{1.95}\NormalTok{, }\FloatTok{1.55}\NormalTok{]}
\NormalTok{weight }\OperatorTok{=}\NormalTok{ [}\DecValTok{70}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{85}\NormalTok{, }\DecValTok{65}\NormalTok{]}

\CommentTok{\#bmi = weight/height**2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{height }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.79}\NormalTok{, }\FloatTok{1.85}\NormalTok{, }\FloatTok{1.95}\NormalTok{, }\FloatTok{1.55}\NormalTok{])}
\NormalTok{weight }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{70}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{85}\NormalTok{, }\DecValTok{65}\NormalTok{])}

\NormalTok{bmi }\OperatorTok{=}\NormalTok{ weight}\OperatorTok{/}\NormalTok{height}\OperatorTok{**}\DecValTok{2}
\NormalTok{np.}\BuiltInTok{round}\NormalTok{(bmi,}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([21.84700852, 23.37472608, 22.35371466, 27.05515088])
\end{verbatim}

\bookmarksetup{startatroot}

\hypertarget{intro-to-numpy}{%
\chapter{Intro to numpy}\label{intro-to-numpy}}

In this lecture we will get to know and become experts in:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{Introduction-to-numpy}{Introduction to numpy}

  \begin{itemize}
  \tightlist
  \item
    DataCamp,
    \href{https://app.datacamp.com/learn/courses/intro-to-python-for-data-science}{Introduction
    to Python}, Chap 4
  \item
    \protect\hyperlink{Multiple-Dimensions}{Multiple Dimensions}
  \item
    \protect\hyperlink{Data-Summaries-in-numpy}{Data Summaries in numpy}
  \end{itemize}
\item
  Introduction to \textbf{Simulating Probabilistic Events}

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{Generating-Data-in-numpy}{Generating Data in
    numpy}
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ numpy.random }\ImportTok{import}\NormalTok{ default\_rng}
\end{Highlighting}
\end{Shaded}

\hypertarget{introduction-to-numpy-1}{%
\section{Introduction to numpy}\label{introduction-to-numpy-1}}

NumPy, short for Numerical Python, is one of the most important
foundational packages for numerical computing in Python.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Vectorized, fast mathematical operations.
\item
  Key features of NumPy is its N-dimensional array object, or ndarray
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{height }\OperatorTok{=}\NormalTok{ [}\FloatTok{1.79}\NormalTok{, }\FloatTok{1.85}\NormalTok{, }\FloatTok{1.95}\NormalTok{, }\FloatTok{1.55}\NormalTok{]}
\NormalTok{weight }\OperatorTok{=}\NormalTok{ [}\DecValTok{70}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{85}\NormalTok{, }\DecValTok{65}\NormalTok{]}

\CommentTok{\#bmi = weight/height**2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{height }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{1.79}\NormalTok{, }\FloatTok{1.85}\NormalTok{, }\FloatTok{1.95}\NormalTok{, }\FloatTok{1.55}\NormalTok{])}
\NormalTok{weight }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{70}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{85}\NormalTok{, }\DecValTok{65}\NormalTok{])}

\NormalTok{bmi }\OperatorTok{=}\NormalTok{ weight}\OperatorTok{/}\NormalTok{height}\OperatorTok{**}\DecValTok{2}
\NormalTok{bmi}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([21.84700852, 23.37472608, 22.35371466, 27.05515088])
\end{verbatim}

\hypertarget{multiple-dimensions}{%
\subsection{Multiple Dimensions}\label{multiple-dimensions}}

are handled naturally by numpy, e.g.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hw1 }\OperatorTok{=}\NormalTok{ np.array([height, weight])}
\BuiltInTok{print}\NormalTok{(hw1)}
\BuiltInTok{print}\NormalTok{(hw1.shape)}
\NormalTok{hw2 }\OperatorTok{=}\NormalTok{ hw1.transpose()}
\BuiltInTok{print}\NormalTok{(hw2)}
\BuiltInTok{print}\NormalTok{(hw2.shape)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 1.79  1.85  1.95  1.55]
 [70.   80.   85.   65.  ]]
(2, 4)
[[ 1.79 70.  ]
 [ 1.85 80.  ]
 [ 1.95 85.  ]
 [ 1.55 65.  ]]
(4, 2)
\end{verbatim}

\hypertarget{accessing-array-elements}{%
\subsection{Accessing array elements}\label{accessing-array-elements}}

is similar to lists but allows for multidimensional index:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(hw2[}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
70.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(hw2[:,}\DecValTok{0}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1.79 1.85 1.95 1.55]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(hw2[}\DecValTok{0}\NormalTok{])}
\CommentTok{\#equivalent to}
\BuiltInTok{print}\NormalTok{(hw2[}\DecValTok{0}\NormalTok{,:])}
\CommentTok{\#shape:}
\BuiltInTok{print}\NormalTok{(hw2[}\DecValTok{0}\NormalTok{].shape)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[ 1.79 70.  ]
[ 1.79 70.  ]
(2,)
\end{verbatim}

To select a subset of the rows in a particular order, you can simply
pass a list or ndarray of integers specifying the desired order:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(hw2[[}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 1.95 85.  ]
 [ 1.79 70.  ]
 [ 1.85 80.  ]]
\end{verbatim}

Negative indices

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(hw2)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Using negative indices selects rows from the end:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(hw2[[}\OperatorTok{{-}}\DecValTok{2}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[ 1.79 70.  ]
 [ 1.85 80.  ]
 [ 1.95 85.  ]
 [ 1.55 65.  ]]
Using negative indices selects rows from the end:
[[ 1.95 85.  ]
 [ 1.55 65.  ]]
\end{verbatim}

You can pass multiple slices just like you can pass multiple indexes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hw2[:}\DecValTok{2}\NormalTok{,:}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1.79],
       [1.85]])
\end{verbatim}

\hypertarget{reshaping}{%
\subsubsection{Reshaping}\label{reshaping}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.arange(}\DecValTok{32}\NormalTok{).reshape((}\DecValTok{8}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11],
       [12, 13, 14, 15],
       [16, 17, 18, 19],
       [20, 21, 22, 23],
       [24, 25, 26, 27],
       [28, 29, 30, 31]])
\end{verbatim}

\hypertarget{boolean-indexing}{%
\subsubsection{Boolean indexing}\label{boolean-indexing}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{height\_gt\_185 }\OperatorTok{=}\NormalTok{ hw2[:,}\DecValTok{0}\NormalTok{]}\OperatorTok{\textgreater{}}\FloatTok{1.85}
\BuiltInTok{print}\NormalTok{(height\_gt\_185)}
\BuiltInTok{print}\NormalTok{(hw2[height\_gt\_185,}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[False False  True False]
[85.]
\end{verbatim}

\texttt{numpy} arrays cannot contain elements with different types. If
you try to build such a list, some of the elements' types are changed to
end up with a homogeneous list. This is known as \textbf{type coercion}.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(np.array([}\VariableTok{True}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{]))}
\BuiltInTok{print}\NormalTok{(np.array([}\StringTok{"True"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{]))}
\BuiltInTok{print}\NormalTok{(np.array([}\FloatTok{1.3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1 1 2]
['True' '1' '2']
[1.3 1.  2. ]
\end{verbatim}

Lots of extra useful functions!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.zeros((}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\CommentTok{\#np.ones((2,3))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[0., 0., 0.],
       [0., 0., 0.]])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.eye(}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.column\_stack([height, weight])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[ 1.79, 70.  ],
       [ 1.85, 80.  ],
       [ 1.95, 85.  ],
       [ 1.55, 65.  ]])
\end{verbatim}

\hypertarget{data-summaries-in-numpy}{%
\section{Data Summaries in numpy}\label{data-summaries-in-numpy}}

We can compute simple statistics:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(np.mean(hw2))}
\BuiltInTok{print}\NormalTok{(np.mean(hw2, axis}\OperatorTok{=}\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
38.3925
[ 1.785 75.   ]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(np.unique([}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{]))}

\BuiltInTok{print}\NormalTok{(np.unique([}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{], return\_counts}\OperatorTok{=}\VariableTok{True}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1 2 3]
(array([1, 2, 3]), array([3, 4, 2], dtype=int64))
\end{verbatim}

\hypertarget{introduction-to-simulating-probabilistic-events}{%
\section{\texorpdfstring{Introduction to \textbf{Simulating
Probabilistic
Events}}{Introduction to Simulating Probabilistic Events}}\label{introduction-to-simulating-probabilistic-events}}

\hypertarget{generating-data-in-numpy}{%
\subsection{Generating Data in numpy}\label{generating-data-in-numpy}}

Meet your
\href{https://numpy.org/doc/stable/reference/random/index.html}{friends}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{np.random.permutation}: Return a random permutation of a
  sequence, or return a permuted range
\item
  \texttt{np.random.integers}: Draw random integers from a given
  low-to-high range
\item
  \texttt{np.random.choice}: Generates a random sample from a given 1-D
  array
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Do this (new version)}
\ImportTok{from}\NormalTok{ numpy.random }\ImportTok{import}\NormalTok{ default\_rng}
\NormalTok{rng }\OperatorTok{=}\NormalTok{ default\_rng()}

\NormalTok{x}\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{10}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(x)}
\BuiltInTok{print}\NormalTok{(rng.permutation(x))}
\BuiltInTok{print}\NormalTok{(rng.permutation(}\BuiltInTok{list}\NormalTok{(}\StringTok{\textquotesingle{}intelligence\textquotesingle{}}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[0 1 2 3 4 5 6 7 8 9]
[6 7 9 4 1 0 3 8 2 5]
['t' 'c' 'n' 'l' 'e' 'n' 'i' 'e' 'e' 'l' 'i' 'g']
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(rng.integers(}\DecValTok{0}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{5}\NormalTok{))}
\BuiltInTok{print}\NormalTok{(rng.integers(}\DecValTok{0}\NormalTok{,}\DecValTok{10}\NormalTok{,(}\DecValTok{5}\NormalTok{,}\DecValTok{2}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[7 9 7 9 4]
[[9 0]
 [8 6]
 [6 7]
 [0 5]
 [1 5]]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rng.choice(x,}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([8, 5, 1, 4])
\end{verbatim}

\hypertarget{examples}{%
\subsection{Examples:}\label{examples}}

\begin{itemize}
\tightlist
\item
  Spotify playlist
\item
  Movie List
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movies\_list }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}The Godfather\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}The Wizard of Oz\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Citizen Kane\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}The Shawshank Redemption\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Pulp Fiction\textquotesingle{}}\NormalTok{]}

\CommentTok{\# pick a random choice from a list of strings.}
\NormalTok{movie }\OperatorTok{=}\NormalTok{ rng.choice(movies\_list,}\DecValTok{2}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(movie)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['The Shawshank Redemption' 'The Godfather']
\end{verbatim}

\hypertarget{birthday-paradox}{%
\section{Birthday ``Paradox''}\label{birthday-paradox}}

Please enter your birthday on google drive
https://forms.gle/CeqyRZ4QzWRmJFvs9

How many people do you think will share a birthday? Would that be a
rare, highly unusual event?

How can we find out how likely it is that across \(n\) folks in a room
at least two share a birthday?

Hint: can we put our random number generators to task ?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Can you simulate 25 birthdays?}
\ImportTok{from}\NormalTok{ numpy.random }\ImportTok{import}\NormalTok{ default\_rng }
\NormalTok{rng }\OperatorTok{=}\NormalTok{ default\_rng()}


\CommentTok{\#initialize it to be the empty list:}
\NormalTok{shardBday }\OperatorTok{=}\NormalTok{ []}

\NormalTok{n }\OperatorTok{=} \DecValTok{40}

\NormalTok{PossibleBDays }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{1}\NormalTok{,}\DecValTok{366}\NormalTok{)}
\CommentTok{\#now "draw" 25 random bDays:}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1000}\NormalTok{):}\CommentTok{\# is the 1000 an important number ??}
\CommentTok{\#no it only determines the precision of my estimate !!}
\NormalTok{  ran25Bdays }\OperatorTok{=}\NormalTok{ rng.choice(PossibleBDays, n, replace }\OperatorTok{=} \VariableTok{True}\NormalTok{)}
  \CommentTok{\#it is of utmost importance to allow for same birthdays !! }
  \CommentTok{\#rng.choice(PossibleBDays, 366, replace = False)}
\NormalTok{  x , cts }\OperatorTok{=}\NormalTok{ np.unique(ran25Bdays ,return\_counts}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{  shardBday }\OperatorTok{=}\NormalTok{ np.append(shardBday, np.}\BuiltInTok{sum}\NormalTok{(cts}\OperatorTok{\textgreater{}}\DecValTok{1}\NormalTok{))}\CommentTok{\#keep this !!}
  \CommentTok{\#shardBday = np.sum(cts\textgreater{}1)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#np.sum(shardBday\textgreater{}0)/1000}
\NormalTok{np.mean(shardBday }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{)}

\CommentTok{\#shardBday = 2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.893
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{5} \OperatorTok{!=} \DecValTok{3} \CommentTok{\#not equal}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
True
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Boolean indexing !!}
\NormalTok{x[cts }\OperatorTok{\textgreater{}} \DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([ 71, 192])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\DecValTok{23}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
182
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can you design a coin flip with an arbitary probability p = 0.25}
\CommentTok{\#simulate 365 days with a 1/4 chance of being sunny}

\CommentTok{\#fair coin}
\NormalTok{coins }\OperatorTok{=}\NormalTok{ np.random.randint(}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{365}\NormalTok{)}

\NormalTok{np.unique(coins, return\_counts}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(array([0, 1]), array([189, 176], dtype=int64))
\end{verbatim}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 0\tabcolsep) * \real{0.5972}}@{}}
\toprule()
\endhead
\#\#\# Tossing dice and coins \\
Let us toss many dice or coins to find out: - the average value of a
six-faced die - the variation around the mean when averaging - the
probability of various ``common hands'' in the game
\href{https://en.wikipedia.org/wiki/Liar's_dice}{Liar's Dice}: * Full
house: e.g., 66111 * Three of a kind: e.g., 44432 * Two pair: e.g.,
22551 * Pair: e.g., 66532 \\
Some real world problems: 1. \textbf{Overbooking flights}: airlines 2.
\textbf{Home Office} days: planning office capacities and minimizing
social isolation \\
 \\
\texttt{\{=html\}\ \textless{}!-\/-\ quarto-file-metadata:\ eyJyZXNvdXJjZURpciI6Ii4iLCJib29rSXRlbVR5cGUiOiJjaGFwdGVyIiwiYm9va0l0ZW1OdW1iZXIiOjUsImJvb2tJdGVtRmlsZSI6IkludHJvQ29kaW5nX0xlY3R1cmU0LmlweW5iIiwiYm9va0l0ZW1EZXB0aCI6MH0=\ -\/-\textgreater{}} \\
\# Intro to pandas \\
In this \protect\hyperlink{Introduction-to-pandas}{Introduction to
pandas} we will get to know and become experts in: \\
1. Data Frames 2. Slicing 3. Counting and Summary Statistics 4.
\protect\hyperlink{Handling-Files}{Handling Files} \\
Relevant DataCamp lessons: \\
*
\href{https://campus.datacamp.com/courses/data-manipulation-with-pandas}{Data
manipulation with pandas}, Chaps 1-4 *
\href{https://campus.datacamp.com/courses/intermediate-python}{Matplotlib},
Chap 1 \\
::: \{.cell\} \\
\#\# Introduction to pandas \\
While numpy offers a lot of powerful numerical capabilities it lacks
some of the necessary convenience and natural of handling data as we
encounter them. For example, we would typically like to - mix data types
(strings, numbers, categories, Boolean, \ldots) - refer to columns and
rows by names - summarize and visualize data in efficient pivot style
manners \\
All of the above (and more) can be achieved easily by extending the
concept of an array (or a matrix) to a so called \textbf{dataframe}. \\
There are many ways to construct a DataFrame, though one of the most
common is from a dictionary of equal-length lists or NumPy arrays: \\
::: \{.cell
executionInfo=`\{``elapsed'':427,``status'':``ok'',``timestamp'':1683024188638,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`4a41f859-ce49-46a1-e308-5228df560828'\} \\
::: \{.cell-output .cell-output-display execution\_count=5\} \\
```\{=html\} \\
 \\
.dataframe tbody tr th \{ vertical-align: top; \} \\
.dataframe thead th \{ text-align: right; \} \\
 \\
 \\
.colab-df-convert \{ background-color: \#E8F0FE; border: none;
border-radius: 50\%; cursor: pointer; display: none; fill: \#1967D2;
height: 32px; padding: 0 0 0 0; width: 32px; \} \\
.colab-df-convert:hover \{ background-color: \#E2EBFA; box-shadow: 0px
1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
fill: \#174EA6; \} \\
{[}theme=dark{]} .colab-df-convert \{ background-color: \#3B4455; fill:
\#D2E3FC; \} \\
{[}theme=dark{]} .colab-df-convert:hover \{ background-color: \#434B5C;
box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px
1px 2px rgba(0, 0, 0, 0.3)); fill: \#FFFFFF; \} \\
 \\
async function convertToInteractive(key) \{ const element =
document.querySelector(`\#df-7efc9943-a94c-4abe-8400-c307ef90ad39');
const dataTable = await
google.colab.kernel.invokeFunction(`convertToInteractive', {[}key{]},
\{\}); if (!dataTable) return; \\
const docLinkHtml = `Like what you see? Visit the' + `data table
notebook' + ' to learn more about interactive tables.';
element.innerHTML = '`; dataTable{[}'output\_type'{]} = 'display\_data';
await google.colab.output.renderOutput(dataTable, element); const
docLink = document.createElement(`div'); docLink.innerHTML =
docLinkHtml; element.appendChild(docLink); \}  \\
``` \\
::: ::: \\
::: \{.cell\} \\
::: \{.cell
executionInfo=`\{``elapsed'':314,``status'':``ok'',``timestamp'':1683024441955,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`ada23816-c0ea-4474-d85f-22305d57758c'\} \\
::: \{.cell-output .cell-output-display execution\_count=9\} \\
\#\#\# Subsetting/Slicing \\
We first need to understand the attributes \textbf{index} (=rownames)
and \textbf{columns} (= column names): \\
::: \{.cell outputId=`eadfec57-e302-45bd-c74a-b93be6fd4c2a'\} \\
::: \{.cell-output .cell-output-display execution\_count=23\} \\
::: \{.cell
executionInfo=`\{``elapsed'':308,``status'':``ok'',``timestamp'':1683024590608,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`4714ea11-ad8c-421e-96e7-e44d7403e4ac'\} \\
::: \{.cell-output .cell-output-stdout\} \\
::: \{.cell
executionInfo=`\{``elapsed'':323,``status'':``ok'',``timestamp'':1683024115682,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`a3942774-31f5-4631-e5ff-d8d2b06e4452'\} \\
::: \{.cell-output .cell-output-display execution\_count=3\} \\
::: \{.cell
executionInfo=`\{``elapsed'':610,``status'':``ok'',``timestamp'':1683024927760,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`6c362666-e5f7-4b69-d705-e77380be158d'\} \\
::: \{.cell-output .cell-output-display execution\_count=18\} \\
\#\#\# Asking for rows \\
Unfortunately, we cannot use the simple \texttt{{[}row,col{]}} notation
that we are used to from numpy arrays. (Try asking for
\texttt{frame{[}0,1{]}}) \\
Instead, row subsetting can be achieved with either the \texttt{.loc()}
or the \texttt{.iloc()} methods. The latter takes integers, the former
indices: \\
::: \{.cell
executionInfo=`\{``elapsed'':398,``status'':``ok'',``timestamp'':1683024602187,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`fac5a1e4-297c-437a-d37a-e32a58881c03'\} \\
::: \{.cell-output .cell-output-display execution\_count=12\} \\
```\{=html\} \\
 \\
.dataframe tbody tr th \{ vertical-align: top; \} \\
.dataframe thead th \{ text-align: right; \} \\
 \\
 \\
.colab-df-convert \{ background-color: \#E8F0FE; border: none;
border-radius: 50\%; cursor: pointer; display: none; fill: \#1967D2;
height: 32px; padding: 0 0 0 0; width: 32px; \} \\
.colab-df-convert:hover \{ background-color: \#E2EBFA; box-shadow: 0px
1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
fill: \#174EA6; \} \\
{[}theme=dark{]} .colab-df-convert \{ background-color: \#3B4455; fill:
\#D2E3FC; \} \\
{[}theme=dark{]} .colab-df-convert:hover \{ background-color: \#434B5C;
box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px
1px 2px rgba(0, 0, 0, 0.3)); fill: \#FFFFFF; \} \\
 \\
async function convertToInteractive(key) \{ const element =
document.querySelector(`\#df-a1942c22-8fb1-498e-95ae-0dd27211201f');
const dataTable = await
google.colab.kernel.invokeFunction(`convertToInteractive', {[}key{]},
\{\}); if (!dataTable) return; \\
const docLinkHtml = `Like what you see? Visit the' + `data table
notebook' + ' to learn more about interactive tables.';
element.innerHTML = '`; dataTable{[}'output\_type'{]} = 'display\_data';
await google.colab.output.renderOutput(dataTable, element); const
docLink = document.createElement(`div'); docLink.innerHTML =
docLinkHtml; element.appendChild(docLink); \}  \\
``` \\
::: ::: \\
::: \{.cell outputId=`ec6b2a53-1bf7-4385-84ae-72602fc85c2a'\} \\
::: \{.cell-output .cell-output-display execution\_count=30\} \\
::: \{.cell
executionInfo=`\{``elapsed'':380,``status'':``ok'',``timestamp'':1683010563095,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`aeea9b53-c0ba-4210-84b7-dd521975a80b'\} \\
::: \{.cell-output .cell-output-display execution\_count=10\} \\
::: \{.cell
executionInfo=`\{``elapsed'':320,``status'':``ok'',``timestamp'':1683024851857,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`999dffbd-f606-4533-8d7c-8385a2e83f82'\} \\
::: \{.cell-output .cell-output-stdout\} \\
::: \{.cell
executionInfo=`\{``elapsed'':373,``status'':``ok'',``timestamp'':1683024804583,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`c67188a7-c6db-48a8-9de5-bada14c2c063'\} \\
::: \{.cell-output .cell-output-display execution\_count=16\} \\
```\{=html\} \\
 \\
.dataframe tbody tr th \{ vertical-align: top; \} \\
.dataframe thead th \{ text-align: right; \} \\
 \\
 \\
.colab-df-convert \{ background-color: \#E8F0FE; border: none;
border-radius: 50\%; cursor: pointer; display: none; fill: \#1967D2;
height: 32px; padding: 0 0 0 0; width: 32px; \} \\
.colab-df-convert:hover \{ background-color: \#E2EBFA; box-shadow: 0px
1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
fill: \#174EA6; \} \\
{[}theme=dark{]} .colab-df-convert \{ background-color: \#3B4455; fill:
\#D2E3FC; \} \\
{[}theme=dark{]} .colab-df-convert:hover \{ background-color: \#434B5C;
box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px
1px 2px rgba(0, 0, 0, 0.3)); fill: \#FFFFFF; \} \\
 \\
async function convertToInteractive(key) \{ const element =
document.querySelector(`\#df-c88550dd-144b-455c-895e-e1d55f1d4036');
const dataTable = await
google.colab.kernel.invokeFunction(`convertToInteractive', {[}key{]},
\{\}); if (!dataTable) return; \\
const docLinkHtml = `Like what you see? Visit the' + `data table
notebook' + ' to learn more about interactive tables.';
element.innerHTML = '`; dataTable{[}'output\_type'{]} = 'display\_data';
await google.colab.output.renderOutput(dataTable, element); const
docLink = document.createElement(`div'); docLink.innerHTML =
docLinkHtml; element.appendChild(docLink); \}  \\
``` \\
::: ::: \\
::: \{.cell\} \\
::: \{.cell outputId=`a0402acf-d77b-4e18-9ff8-4e90d3356b27'\} \\
::: \{.cell-output .cell-output-display execution\_count=39\} \\
\#\#\# Asking for columns \\
::: \{.cell
executionInfo=`\{``elapsed'':521,``status'':``ok'',``timestamp'':1683010794284,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`371e5e00-b584-4258-8d5e-36fa37333d86'\} \\
::: \{.cell-output .cell-output-display execution\_count=12\} \\
A column in a DataFrame can be retrieved MUCH easier: as a Series either
by dictionary-like notation or by using the dot attribute notation: \\
::: \{.cell outputId=`732d0d02-3a0a-493f-d8d8-ff905c29cd87'\} \\
::: \{.cell-output .cell-output-display execution\_count=17\} \\
::: \{.cell outputId=`7ca7fea4-8e89-4925-9c05-59713798f068'\} \\
::: \{.cell-output .cell-output-display execution\_count=18\} \\
\#\#\# Summary Stats \\
Just like in numpy you can compute sums, means, counts and many other
summaries along rows and columns, by specifying the \texttt{axis}
argument: \\
::: \{.cell
executionInfo=`\{``elapsed'':4,``status'':``ok'',``timestamp'':1683025066198,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`212a4691-0400-40a2-c35a-a163be5ba112'\} ``` \{.python
.cell-code\} height = np.array({[}1.79, 1.85, 1.95, 1.55{]}) weight =
np.array({[}70, 80, 85, 65{]}) hw = np.array({[}height,
weight{]}).transpose() \\
hw ``` \\
::: \{.cell-output .cell-output-display execution\_count=20\} \\
::: \{.cell
executionInfo=`\{``elapsed'':361,``status'':``ok'',``timestamp'':1683025151434,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`50c15fbb-8402-4db8-d3af-c0b5f2469cfe'\} \\
::: \{.cell-output .cell-output-stdout\} \\
::: \{.cell
executionInfo=`\{``elapsed'':313,``status'':``ok'',``timestamp'':1683025183810,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`20e33e59-4133-42d5-e36a-b68f66780832'\} \\
::: \{.cell-output .cell-output-stdout\} \\
Can you extract: \\
0. All weights 1. Peter's height 2. Bee's full info 3. the average
height 4. get all persons with height greater than 180cm \\
::: \{.cell\} \\
::: \{.cell
executionInfo=`\{``elapsed'':377,``status'':``ok'',``timestamp'':1683012383970,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`1c45ebe6-37d5-430d-fe59-87f9fca7e65b'\} \\
::: \{.cell-output .cell-output-stdout\} \\
Some methods are neither reductions nor accumulations. \texttt{describe}
is one such example, producing multiple summary statistics in one
shot: \\
::: \{.cell outputId=`9e73eec1-32d2-486b-8671-0d631653595a'\} \\
::: \{.cell-output .cell-output-display execution\_count=43\} \\
```\{=html\} \\
.dataframe tbody tr th \{ vertical-align: top; \} \\
.dataframe thead th \{ text-align: right; \} \\
::: ::: \\
\#\# Built in data sets \\
\#\# Gapminder Data \\
https://www.gapminder.org/fw/world-health-chart/ \\
https://www.ted.com/talks/hans\_rosling\_the\_best\_stats\_you\_ve\_ever\_seen\#t-241405 \\
\textgreater{} You've never seen data presented like this. With the
drama and urgency of a sportscaster, statistics guru Hans Rosling
debunks myths about the so-called ``developing world.'' \\
::: \{.cell\} \\
::: \{.cell
executionInfo=`\{``elapsed'':8,``status'':``ok'',``timestamp'':1683013486502,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`974adae1-7cde-43ff-b4bc-1d4c28403147'\} \\
::: \{.cell-output .cell-output-display execution\_count=37\} \\
```\{=html\} \\
 \\
.dataframe tbody tr th \{ vertical-align: top; \} \\
.dataframe thead th \{ text-align: right; \} \\
 \\
 \\
.colab-df-convert \{ background-color: \#E8F0FE; border: none;
border-radius: 50\%; cursor: pointer; display: none; fill: \#1967D2;
height: 32px; padding: 0 0 0 0; width: 32px; \} \\
.colab-df-convert:hover \{ background-color: \#E2EBFA; box-shadow: 0px
1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
fill: \#174EA6; \} \\
{[}theme=dark{]} .colab-df-convert \{ background-color: \#3B4455; fill:
\#D2E3FC; \} \\
{[}theme=dark{]} .colab-df-convert:hover \{ background-color: \#434B5C;
box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px
1px 2px rgba(0, 0, 0, 0.3)); fill: \#FFFFFF; \} \\
 \\
async function convertToInteractive(key) \{ const element =
document.querySelector(`\#df-acc59b03-c8fa-4715-afe8-4eb0bbcb7128');
const dataTable = await
google.colab.kernel.invokeFunction(`convertToInteractive', {[}key{]},
\{\}); if (!dataTable) return; \\
const docLinkHtml = `Like what you see? Visit the' + `data table
notebook' + ' to learn more about interactive tables.';
element.innerHTML = '`; dataTable{[}'output\_type'{]} = 'display\_data';
await google.colab.output.renderOutput(dataTable, element); const
docLink = document.createElement(`div'); docLink.innerHTML =
docLinkHtml; element.appendChild(docLink); \}  \\
``` \\
::: ::: \\
::: \{.cell
executionInfo=`\{``elapsed'':774,``status'':``ok'',``timestamp'':1683013794713,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`0c83252b-2176-4c75-b0d5-81cf7dfe9d1b'\} ``` \{.python
.cell-code\} \#find the unique years \\
\#get the years: gapminder{[}``year''{]} np.unique(gapminder.year)
``` \\
::: \{.cell-output .cell-output-display execution\_count=41\} \\
::: \{.cell
executionInfo=`\{``elapsed'':1099,``status'':``ok'',``timestamp'':1683014342194,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`2e6fc695-c0a6-4fa3-d60a-240b8eba9324'\} ``` \{.python
.cell-code\} \#get all rows with year 1952: \#Hint: \#either use Boolean
subsetting gapminder{[}``year''{]} == 1952
gapminder{[}gapminder{[}``year''{]} == 1952{]} \#or use an index !! \\
``` \\
::: \{.cell-output .cell-output-display execution\_count=43\} \\
```\{=html\} \\
 \\
.dataframe tbody tr th \{ vertical-align: top; \} \\
.dataframe thead th \{ text-align: right; \} \\
 \\
 \\
.colab-df-convert \{ background-color: \#E8F0FE; border: none;
border-radius: 50\%; cursor: pointer; display: none; fill: \#1967D2;
height: 32px; padding: 0 0 0 0; width: 32px; \} \\
.colab-df-convert:hover \{ background-color: \#E2EBFA; box-shadow: 0px
1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
fill: \#174EA6; \} \\
{[}theme=dark{]} .colab-df-convert \{ background-color: \#3B4455; fill:
\#D2E3FC; \} \\
{[}theme=dark{]} .colab-df-convert:hover \{ background-color: \#434B5C;
box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px
1px 2px rgba(0, 0, 0, 0.3)); fill: \#FFFFFF; \} \\
 \\
async function convertToInteractive(key) \{ const element =
document.querySelector(`\#df-4e372233-0124-4621-8cee-92d859b78962');
const dataTable = await
google.colab.kernel.invokeFunction(`convertToInteractive', {[}key{]},
\{\}); if (!dataTable) return; \\
const docLinkHtml = `Like what you see? Visit the' + `data table
notebook' + ' to learn more about interactive tables.';
element.innerHTML = '`; dataTable{[}'output\_type'{]} = 'display\_data';
await google.colab.output.renderOutput(dataTable, element); const
docLink = document.createElement(`div'); docLink.innerHTML =
docLinkHtml; element.appendChild(docLink); \}  \\
``` \\
::: ::: \\
\#\# Handling Files \\
Get to know your friends \\
* \texttt{pd.read\_csv} * \texttt{pd.read\_table} *
\texttt{pd.read\_excel} \\
::: \{.cell
executionInfo=`\{``elapsed'':1235,``status'':``ok'',``timestamp'':1682343627220,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`34256d0e-6290-43ef-c208-e5e1178a3a01'\} ``` \{.python
.cell-code\} '`'url =
``https://drive.google.com/file/d/1oIvCdN15UEwt4dCyjkArekHnTrivN43v/view?usp=share\_link''
url='https://drive.google.com/uc?id=' + url.split(`/'){[}-2{]} gapminder
= pd.read\_csv(url, index\_col=0) gapminder.head()'\,'\,' \\
``` \\
::: \{.cell-output .cell-output-display execution\_count=2\} \\
```\{=html\} \\
 \\
.dataframe tbody tr th \{ vertical-align: top; \} \\
.dataframe thead th \{ text-align: right; \} \\
 \\
 \\
.colab-df-convert \{ background-color: \#E8F0FE; border: none;
border-radius: 50\%; cursor: pointer; display: none; fill: \#1967D2;
height: 32px; padding: 0 0 0 0; width: 32px; \} \\
.colab-df-convert:hover \{ background-color: \#E2EBFA; box-shadow: 0px
1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
fill: \#174EA6; \} \\
{[}theme=dark{]} .colab-df-convert \{ background-color: \#3B4455; fill:
\#D2E3FC; \} \\
{[}theme=dark{]} .colab-df-convert:hover \{ background-color: \#434B5C;
box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px
1px 2px rgba(0, 0, 0, 0.3)); fill: \#FFFFFF; \} \\
 \\
async function convertToInteractive(key) \{ const element =
document.querySelector(`\#df-42dc89b0-0da4-4c66-a3f0-42fcc92029a5');
const dataTable = await
google.colab.kernel.invokeFunction(`convertToInteractive', {[}key{]},
\{\}); if (!dataTable) return; \\
const docLinkHtml = `Like what you see? Visit the' + `data table
notebook' + ' to learn more about interactive tables.';
element.innerHTML = '`; dataTable{[}'output\_type'{]} = 'display\_data';
await google.colab.output.renderOutput(dataTable, element); const
docLink = document.createElement(`div'); docLink.innerHTML =
docLinkHtml; element.appendChild(docLink); \}  \\
``` \\
::: ::: \\
::: \{.cell outputId=`0c07ed68-f605-407f-b3fd-78dd9c63dc48'\} \\
::: \{.cell-output .cell-output-display execution\_count=71\} \\
```\{=html\} \\
.dataframe tbody tr th \{ vertical-align: top; \} \\
.dataframe thead th \{ text-align: right; \} \\
::: ::: \\
::: \{.cell outputId=`6379a0f5-d837-4c9c-9253-926496fc87bc'\} \\
::: \{.cell-output .cell-output-display execution\_count=63\} \\
::: \{.cell outputId=`2f46c987-3383-4731-b1ae-fc5249a15d85'\} \\
::: \{.cell-output .cell-output-display execution\_count=75\} \\
::: \{.cell outputId=`def2732a-7153-4a42-a486-5c7194958f4c'\} \\
::: \{.cell-output .cell-output-stdout\} \\
::: \{.cell-output .cell-output-stderr\} \\
::: \{.cell-output .cell-output-display execution\_count=54\} \\
```\{=html\} \\
.dataframe tbody tr th \{ vertical-align: top; \} \\
.dataframe thead th \{ text-align: right; \} \\
::: ::: \\
\textbf{Sort the index before you slice!!} \\
Choose a time range and specific countries \\
::: \{.cell outputId=`614b674e-df99-42e7-c9e4-a68105d86180'\} \\
::: \{.cell-output .cell-output-display execution\_count=91\} \\
```\{=html\} \\
.dataframe tbody tr th \{ vertical-align: top; \} \\
.dataframe thead th \{ text-align: right; \} \\
::: ::: \\
::: \{.cell outputId=`8a594a1d-ea6c-439a-8de9-f356414dc8cd'\} \\
::: \{.cell-output .cell-output-display execution\_count=93\} \\
 \\
\texttt{\{=html\}\ \textless{}!-\/-\ quarto-file-metadata:\ eyJyZXNvdXJjZURpciI6Ii4iLCJib29rSXRlbVR5cGUiOiJjaGFwdGVyIiwiYm9va0l0ZW1OdW1iZXIiOjYsImJvb2tJdGVtRmlsZSI6IkludHJvQ29kaW5nX0xlY3R1cmU1LmlweW5iIiwiYm9va0l0ZW1EZXB0aCI6MH0=\ -\/-\textgreater{}} \\
\# Data Summaries \\
In this lecture we will get to know and become experts in: 1.
\protect\hyperlink{Data-Manipulation-with-pandas}{Data Manipulation with
pandas} * \protect\hyperlink{Handling-Files}{Handling Files} * Counting
and Summary Statistics * \protect\hyperlink{Grouped-Operations}{Grouped
Operations} 2. \protect\hyperlink{Plotting}{Plotting} * matplotlib *
pandas \\
And if you want to delve deeper, look at the
\protect\hyperlink{Advanced-topics}{Advanced topics} \\
Relevant DataCamp lessons: \\
*
\href{https://campus.datacamp.com/courses/data-manipulation-with-pandas}{Data
manipulation with pandas}, Chaps 2 and 4 *
\href{https://campus.datacamp.com/courses/intermediate-python}{Matplotlib},
Chap 1 \\
::: \{.cell\} \\
\#\# Data Manipulation with pandas \\
While we have seen panda's ability to (i) mix data types (strings,
numbers, categories, Boolean, \ldots) and (ii) refer to columns and rows
by names, this library offers a lot more powerful tools for efficiently
gaining insights from data, e.g. \\
- summarize/aggregate data in efficient pivot style manners - handling
missing values - visualize/plot data \\
::: \{.cell\} \\
\#\# Handling Files \\
Get to know your friends \\
* \texttt{pd.read\_csv} * \texttt{pd.read\_table} *
\texttt{pd.read\_excel} \\
But before that we need to connect to our Google drives ! (more
instructions can be found
\href{https://gist.github.com/kelly-sovacool/c33b64b70164be37d75bd40c9b56b498}{here}) \\
::: \{.cell
executionInfo=`\{``elapsed'':4,``status'':``ok'',``timestamp'':1683555854441,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`2c49553e-941c-4125-c2c3-52e6b4c730c2'\} \\
::: \{.cell-output .cell-output-display execution\_count=5\} \\
Counting and Summary Statistics \\
::: \{.cell outputId=`af1ef667-f29d-4c92-86b8-2fd1c6ffe159'\} \\
::: \{.cell-output .cell-output-display execution\_count=71\} \\
```\{=html\} \\
.dataframe tbody tr th \{ vertical-align: top; \} \\
.dataframe thead th \{ text-align: right; \} \\
::: ::: \\
::: \{.cell
executionInfo=`\{``elapsed'':284,``status'':``ok'',``timestamp'':1683556772006,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`2a36faee-6c99-4bc8-bc6a-0bb8785462e3'\} \\
::: \{.cell-output .cell-output-display execution\_count=10\} \\
\#\# Grouped Operations \\
The gapminder data is a good example for wanting to apply functions to
subsets to data that correspond to categories, e.g. * by year * by
country * by continent \\
The powerful pandas \texttt{.groupby()} method enables exactly this goal
rather elegantly and efficiently. \\
First, think how you could possibly compute the average GDP seprataley
for each continent. The \texttt{numpy.mean(...,\ axis=...)} will not
help you. \\
Instead you will have to manually find all continents and then use
Boolean logic: \\
::: \{.cell
executionInfo=`\{``elapsed'':212,``status'':``ok'',``timestamp'':1683439733094,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`15481d0e-0491-467c-9721-db0329d72684'\} \\
::: \{.cell-output .cell-output-display execution\_count=10\} \\
::: \{.cell\} ``` \{.python .cell-code\} AfricaRows =
gapminder{[}``continent''{]}==``Africa''
gapminder{[}AfricaRows{]}{[}``gdpPercap''{]}.mean() \\
``` ::: \\
::: \{.cell
executionInfo=`\{``elapsed'':223,``status'':``ok'',``timestamp'':1683556339119,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`bc685147-f274-4a07-d91d-8db44044b594'\} \\
::: \{.cell-output .cell-output-display execution\_count=7\} \\
Instead, we should embrace the concept of \textbf{grouping by a
variable} \\
::: \{.cell
executionInfo=`\{``elapsed'':4,``status'':``ok'',``timestamp'':1683556453453,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`e401809a-8936-441b-e045-f08629f010e2'\} \\
::: \{.cell-output .cell-output-stderr\} \\
::: \{.cell-output .cell-output-display execution\_count=8\} \\
::: \{.cell
executionInfo=`\{``elapsed'':8,``status'':``ok'',``timestamp'':1683556842613,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`18a97f92-9784-4722-c2be-3c52c2f3913e'\} \\
::: \{.cell-output .cell-output-stderr\} \\
::: \{.cell-output .cell-output-display execution\_count=12\} \\
```\{=html\} \\
 \\
.dataframe tbody tr th \{ vertical-align: top; \} \\
.dataframe thead th \{ text-align: right; \} \\
 \\
 \\
.colab-df-convert \{ background-color: \#E8F0FE; border: none;
border-radius: 50\%; cursor: pointer; display: none; fill: \#1967D2;
height: 32px; padding: 0 0 0 0; width: 32px; \} \\
.colab-df-convert:hover \{ background-color: \#E2EBFA; box-shadow: 0px
1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
fill: \#174EA6; \} \\
{[}theme=dark{]} .colab-df-convert \{ background-color: \#3B4455; fill:
\#D2E3FC; \} \\
{[}theme=dark{]} .colab-df-convert:hover \{ background-color: \#434B5C;
box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px
1px 2px rgba(0, 0, 0, 0.3)); fill: \#FFFFFF; \} \\
 \\
async function convertToInteractive(key) \{ const element =
document.querySelector(`\#df-9e17e362-14cc-4068-a4ec-3ebcc0a6afcc');
const dataTable = await
google.colab.kernel.invokeFunction(`convertToInteractive', {[}key{]},
\{\}); if (!dataTable) return; \\
const docLinkHtml = `Like what you see? Visit the' + `data table
notebook' + ' to learn more about interactive tables.';
element.innerHTML = '`; dataTable{[}'output\_type'{]} = 'display\_data';
await google.colab.output.renderOutput(dataTable, element); const
docLink = document.createElement(`div'); docLink.innerHTML =
docLinkHtml; element.appendChild(docLink); \}  \\
``` \\
::: ::: \\
::: \{.cell
executionInfo=`\{``elapsed'':4,``status'':``ok'',``timestamp'':1683556960873,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`1cc02fd3-3a03-4576-8f37-393660f66c95'\} \\
::: \{.cell-output .cell-output-display execution\_count=16\} \\
::: \{.cell
executionInfo=`\{``elapsed'':947,``status'':``ok'',``timestamp'':1683557058870,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`bd3688a0-cbb4-4529-af96-d9fd89761ad9'\} \\
::: \{.cell-output .cell-output-display execution\_count=17\} \\
```\{=html\} \\
 \\
.dataframe tbody tr th \{ vertical-align: top; \} \\
.dataframe thead th \{ text-align: right; \} \\
 \\
 \\
.colab-df-convert \{ background-color: \#E8F0FE; border: none;
border-radius: 50\%; cursor: pointer; display: none; fill: \#1967D2;
height: 32px; padding: 0 0 0 0; width: 32px; \} \\
.colab-df-convert:hover \{ background-color: \#E2EBFA; box-shadow: 0px
1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
fill: \#174EA6; \} \\
{[}theme=dark{]} .colab-df-convert \{ background-color: \#3B4455; fill:
\#D2E3FC; \} \\
{[}theme=dark{]} .colab-df-convert:hover \{ background-color: \#434B5C;
box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px
1px 2px rgba(0, 0, 0, 0.3)); fill: \#FFFFFF; \} \\
 \\
async function convertToInteractive(key) \{ const element =
document.querySelector(`\#df-faa90c88-6b88-43cc-9e80-c69e3f47eec9');
const dataTable = await
google.colab.kernel.invokeFunction(`convertToInteractive', {[}key{]},
\{\}); if (!dataTable) return; \\
const docLinkHtml = `Like what you see? Visit the' + `data table
notebook' + ' to learn more about interactive tables.';
element.innerHTML = '`; dataTable{[}'output\_type'{]} = 'display\_data';
await google.colab.output.renderOutput(dataTable, element); const
docLink = document.createElement(`div'); docLink.innerHTML =
docLinkHtml; element.appendChild(docLink); \}  \\
``` \\
::: ::: \\
::: \{.cell
executionInfo=`\{``elapsed'':320,``status'':``ok'',``timestamp'':1683442764530,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`1b15b924-c0f4-46ce-f5cc-884c324dfa60'\} \\
::: \{.cell-output .cell-output-display execution\_count=18\} \\
```\{=html\} \\
 \\
.dataframe tbody tr th \{ vertical-align: top; \} \\
.dataframe thead th \{ text-align: right; \} \\
 \\
 \\
.colab-df-convert \{ background-color: \#E8F0FE; border: none;
border-radius: 50\%; cursor: pointer; display: none; fill: \#1967D2;
height: 32px; padding: 0 0 0 0; width: 32px; \} \\
.colab-df-convert:hover \{ background-color: \#E2EBFA; box-shadow: 0px
1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
fill: \#174EA6; \} \\
{[}theme=dark{]} .colab-df-convert \{ background-color: \#3B4455; fill:
\#D2E3FC; \} \\
{[}theme=dark{]} .colab-df-convert:hover \{ background-color: \#434B5C;
box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15); filter: drop-shadow(0px
1px 2px rgba(0, 0, 0, 0.3)); fill: \#FFFFFF; \} \\
 \\
async function convertToInteractive(key) \{ const element =
document.querySelector(`\#df-2c5b6b6a-c411-4919-aa90-4a512fa77b69');
const dataTable = await
google.colab.kernel.invokeFunction(`convertToInteractive', {[}key{]},
\{\}); if (!dataTable) return; \\
const docLinkHtml = `Like what you see? Visit the' + `data table
notebook' + ' to learn more about interactive tables.';
element.innerHTML = '`; dataTable{[}'output\_type'{]} = 'display\_data';
await google.colab.output.renderOutput(dataTable, element); const
docLink = document.createElement(`div'); docLink.innerHTML =
docLinkHtml; element.appendChild(docLink); \}  \\
``` \\
::: ::: \\
::: \{.cell\} \\
::: \{.cell
executionInfo=`\{``elapsed'':211,``status'':``ok'',``timestamp'':1683443085773,``user'':\{``displayName'':``Markus
Loecher'',``userId'':``02488037228155275753''\},``user\_tz'':-120\}'
outputId=`e3ea1161-2b2e-481e-eb94-77195c4ae250'\} \\
::: \{.cell-output .cell-output-display execution\_count=24\} \\
\bottomrule()
\end{longtable}

\hypertarget{titanic-data}{%
\subsection{Titanic data}\label{titanic-data}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Since pandas does not have any built in data, I am going to "cheat" and }
\CommentTok{\# make use of the \textasciigrave{}seaborn\textasciigrave{} library}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns }

\NormalTok{titanic }\OperatorTok{=}\NormalTok{ sns. load\_dataset(}\StringTok{\textquotesingle{}titanic\textquotesingle{}}\NormalTok{)}
\NormalTok{titanic[}\StringTok{"3rdClass"}\NormalTok{] }\OperatorTok{=}\NormalTok{ titanic[}\StringTok{"pclass"}\NormalTok{]}\OperatorTok{==}\DecValTok{3}
\NormalTok{titanic[}\StringTok{"male"}\NormalTok{] }\OperatorTok{=}\NormalTok{ titanic[}\StringTok{"sex"}\NormalTok{]}\OperatorTok{==}\StringTok{"male"}

\NormalTok{titanic}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llllllllllllllllll@{}}
\toprule()
& survived & pclass & sex & age & sibsp & parch & fare & embarked &
class & who & adult\_male & deck & embark\_town & alive & alone &
3rdClass & male \\
\midrule()
\endhead
0 & 0 & 3 & male & 22.0 & 1 & 0 & 7.2500 & S & Third & man & True & NaN
& Southampton & no & False & True & True \\
1 & 1 & 1 & female & 38.0 & 1 & 0 & 71.2833 & C & First & woman & False
& C & Cherbourg & yes & False & False & False \\
2 & 1 & 3 & female & 26.0 & 0 & 0 & 7.9250 & S & Third & woman & False &
NaN & Southampton & yes & True & True & False \\
3 & 1 & 1 & female & 35.0 & 1 & 0 & 53.1000 & S & First & woman & False
& C & Southampton & yes & False & False & False \\
4 & 0 & 3 & male & 35.0 & 0 & 0 & 8.0500 & S & Third & man & True & NaN
& Southampton & no & True & True & True \\
... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... &
... & ... & ... & ... & ... & ... \\
886 & 0 & 2 & male & 27.0 & 0 & 0 & 13.0000 & S & Second & man & True &
NaN & Southampton & no & True & False & True \\
887 & 1 & 1 & female & 19.0 & 0 & 0 & 30.0000 & S & First & woman &
False & B & Southampton & yes & True & False & False \\
888 & 0 & 3 & female & NaN & 1 & 2 & 23.4500 & S & Third & woman & False
& NaN & Southampton & no & False & True & False \\
889 & 1 & 1 & male & 26.0 & 0 & 0 & 30.0000 & C & First & man & True & C
& Cherbourg & yes & True & False & True \\
890 & 0 & 3 & male & 32.0 & 0 & 0 & 7.7500 & Q & Third & man & True &
NaN & Queenstown & no & True & True & True \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#overall survival rate}
\NormalTok{titanic.survived.mean()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.3838383838383838
\end{verbatim}

Tasks:

\begin{itemize}
\tightlist
\item
  compute the proportion of survived separately for

  \begin{itemize}
  \tightlist
  \item
    male/female
  \item
    the three classes
  \item
    Pclass and sex
  \end{itemize}
\item
  compute the mean age separately for male/female
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#I would like to compute the mean survical seprately for each group}
\NormalTok{bySex }\OperatorTok{=}\NormalTok{ titanic.groupby(}\StringTok{"sex"}\NormalTok{)}
\CommentTok{\#here I am specifically asking for the mean}
\NormalTok{bySex[}\StringTok{"survived"}\NormalTok{].mean()}
\CommentTok{\#if you want multiple summaries, you can list them all inside the agg():}
\NormalTok{bySex[}\StringTok{"survived"}\NormalTok{].agg([}\BuiltInTok{min}\NormalTok{, }\BuiltInTok{max}\NormalTok{, np.mean ])}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule()
& min & max & mean \\
sex & & & \\
\midrule()
\endhead
female & 0 & 1 & 0.742038 \\
male & 0 & 1 & 0.188908 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#I would like to compute the mean survical seprately for each group}
\NormalTok{bySexPclass }\OperatorTok{=}\NormalTok{ titanic.groupby([}\StringTok{"pclass"}\NormalTok{, }\StringTok{"sex"}\NormalTok{])}
\CommentTok{\#here I am specifically asking for the mean}
\NormalTok{bySexPclass[}\StringTok{"survived"}\NormalTok{].mean()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
pclass  sex   
1       female    0.968085
        male      0.368852
2       female    0.921053
        male      0.157407
3       female    0.500000
        male      0.135447
Name: survived, dtype: float64
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bySex }\OperatorTok{=}\NormalTok{ titanic.groupby(}\StringTok{"sex"}\NormalTok{)}
\CommentTok{\#here I am specifically asking for the mean}
\NormalTok{bySex[}\StringTok{"survived"}\NormalTok{].mean()}
\end{Highlighting}
\end{Shaded}

\hypertarget{plotting}{%
\section{Plotting}\label{plotting}}

We will not spend much time with basic plots in matplotlib but instead
move quickly toward the pandas versions of these functions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\%matplotlib inline}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\CommentTok{\#plt.rcParams[\textquotesingle{}figure.dpi\textquotesingle{}] = 800}
\NormalTok{year }\OperatorTok{=}\NormalTok{ [}\DecValTok{1950}\NormalTok{, }\DecValTok{1970}\NormalTok{, }\DecValTok{1990}\NormalTok{, }\DecValTok{2010}\NormalTok{]}
\NormalTok{pop }\OperatorTok{=}\NormalTok{ [}\FloatTok{2.519}\NormalTok{, }\FloatTok{3.692}\NormalTok{, }\FloatTok{5.263}\NormalTok{, }\FloatTok{6.972}\NormalTok{]}
\NormalTok{plt.plot(year, pop)}
\CommentTok{\#plt.bar(year, pop)}
\CommentTok{\#plt.scatter(year, pop)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Year\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Population\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{\textquotesingle{}World Population\textquotesingle{}}\NormalTok{)}
\NormalTok{x }\OperatorTok{=} \DecValTok{1}
\CommentTok{\#plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./IntroCoding_Lecture5_files/figure-pdf/cell-22-output-1.png}

}

\end{figure}

pandas offers plots directly from its objects

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{titanic.age.hist()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./IntroCoding_Lecture5_files/figure-pdf/cell-23-output-1.png}

}

\end{figure}

And often the axis labels are taken care of

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#titanic.groupby("pclass").survived.mean().plot.bar()}
\NormalTok{SurvByPclass }\OperatorTok{=}\NormalTok{ titanic.groupby(}\StringTok{"pclass"}\NormalTok{).survived.mean()}

\NormalTok{SurvByPclass.plot(kind}\OperatorTok{=}\StringTok{"bar"}\NormalTok{, title }\OperatorTok{=} \StringTok{"Mean Survival"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<Axes: title={'center': 'Mean Survival'}, xlabel='pclass'>
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{./IntroCoding_Lecture5_files/figure-pdf/cell-24-output-2.png}

}

\end{figure}

But you can customize each plot as you wish:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SurvByPclass.plot(kind}\OperatorTok{=}\StringTok{"bar"}\NormalTok{, x }\OperatorTok{=} \StringTok{"Passenger Class"}\NormalTok{, y }\OperatorTok{=} \StringTok{"Survived"}\NormalTok{, title }\OperatorTok{=} \StringTok{"Mean Survival"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<Axes: title={'center': 'Mean Survival'}, xlabel='pclass'>
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{./IntroCoding_Lecture5_files/figure-pdf/cell-25-output-2.png}

}

\end{figure}

Tasks:

\begin{itemize}
\tightlist
\item
  Compute the avg. life expectancy in the gapminder data for each year
\item
  Plot this as a line plot and give meaningful x and y labels and a
  title
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lifeExpbyYear }\OperatorTok{=}\NormalTok{ gapminder.groupby(}\StringTok{"year"}\NormalTok{)[}\StringTok{"lifeExp"}\NormalTok{].mean()}

\NormalTok{lifeExpbyYear.plot(y}\OperatorTok{=} \StringTok{"avg. life Exp"}\NormalTok{, title }\OperatorTok{=} \StringTok{"Average life Expectancvy per year"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<Axes: title={'center': 'Average life Expectancvy per year'}, xlabel='year'>
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{./IntroCoding_Lecture5_files/figure-pdf/cell-26-output-2.png}

}

\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{advanced-topics}{%
\section{Advanced topics}\label{advanced-topics}}

\hypertarget{creating-dataframes}{%
\subsection{Creating Dataframes}\label{creating-dataframes}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Zip
\item
  From list of dicts
\end{enumerate}

\hypertarget{indexing}{%
\subsection{Indexing:}\label{indexing}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  multilevel indexes
\item
  sorting
\item
  asking for ranges
\end{enumerate}

\hypertarget{types-of-columns}{%
\section{Types of columns}\label{types-of-columns}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  categorical
\item
  dates
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Creating Dataframes}
\CommentTok{\#using zip}
\CommentTok{\# List1}
\NormalTok{Name }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}tom\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}krish\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}nick\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}juli\textquotesingle{}}\NormalTok{]}
  
\CommentTok{\# List2}
\NormalTok{Age }\OperatorTok{=}\NormalTok{ [}\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{26}\NormalTok{, }\DecValTok{22}\NormalTok{]}
  
\CommentTok{\# get the list of tuples from two lists.}
\CommentTok{\# and merge them by using zip().}
\NormalTok{list\_of\_tuples }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{zip}\NormalTok{(Name, Age))}
\NormalTok{list\_of\_tuples }\OperatorTok{=} \BuiltInTok{zip}\NormalTok{(Name, Age)}
\CommentTok{\# Assign data to tuples.}
\CommentTok{\#print(list\_of\_tuples)}
  
  
\CommentTok{\# Converting lists of tuples into}
\CommentTok{\# pandas Dataframe.}
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(list\_of\_tuples,}
\NormalTok{                  columns}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}Name\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Age\textquotesingle{}}\NormalTok{])}
  
\CommentTok{\# Print data.}
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule()
& Name & Age \\
\midrule()
\endhead
0 & tom & 25 \\
1 & krish & 30 \\
2 & nick & 26 \\
3 & juli & 22 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#from list of dicts}
\NormalTok{data }\OperatorTok{=}\NormalTok{ [\{}\StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{: }\DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{: }\DecValTok{3}\NormalTok{\},}
\NormalTok{        \{}\StringTok{\textquotesingle{}a\textquotesingle{}}\NormalTok{: }\DecValTok{10}\NormalTok{, }\StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{: }\DecValTok{20}\NormalTok{, }\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{: }\DecValTok{30}\NormalTok{\}]}
  
\CommentTok{\# Creates DataFrame.}
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(data)}
  
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule()
& a & b & c \\
\midrule()
\endhead
0 & 1 & 2 & 3 \\
1 & 10 & 20 & 30 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Indexing:}

\NormalTok{advLesson }\OperatorTok{=} \VariableTok{True}
\ControlFlowTok{if}\NormalTok{ advLesson:}
\NormalTok{    frame2 }\OperatorTok{=}\NormalTok{ frame.set\_index([}\StringTok{"year"}\NormalTok{, }\StringTok{"state"}\NormalTok{])}
    \BuiltInTok{print}\NormalTok{(frame2)}
\NormalTok{    frame3 }\OperatorTok{=}\NormalTok{ frame2.sort\_index()}
    \BuiltInTok{print}\NormalTok{(frame3)}
    \BuiltInTok{print}\NormalTok{(frame.loc[:,}\StringTok{"state"}\NormalTok{:}\StringTok{"year"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             pop
year state      
2000 Ohio    1.5
2001 Ohio    1.7
2002 Ohio    3.6
2001 Nevada  2.4
2002 Nevada  2.9
2003 Nevada  3.2
             pop
year state      
2000 Ohio    1.5
2001 Nevada  2.4
     Ohio    1.7
2002 Nevada  2.9
     Ohio    3.6
2003 Nevada  3.2
    state  year
0    Ohio  2000
1    Ohio  2001
2    Ohio  2002
3  Nevada  2001
4  Nevada  2002
5  Nevada  2003
\end{verbatim}

\hypertarget{inplace}{%
\subsection{Inplace}\label{inplace}}

Note that I reassigned the objects in the code above. That is because
most operations, such as \texttt{set\_index}, \texttt{sort\_index},
\texttt{drop}, etc. do not operate \textbf{inplace} unless specified!

\bookmarksetup{startatroot}

\hypertarget{missing-valuesduplicates}{%
\chapter{Missing Values/Duplicates}\label{missing-valuesduplicates}}

In this lecture we will continue our journey of Data Manipulation with
pandas after reviewing some fundamental aspects of the syntax

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Review

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{Review-of-ux5cux2522bracketsux5cux2522}{Review of
    ``brackets''}
  \end{itemize}
\item
  Pandas

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{Missing-Values}{Dealing with Missing Values}
  \item
    \protect\hyperlink{Duplicate-Values}{Dealing with Duplicates}
  \end{itemize}
\item
  \protect\hyperlink{Plotting}{Plot of the Day}

  \begin{itemize}
  \tightlist
  \item
    boxplot
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

\hypertarget{review-of-brackets}{%
\subsection{Review of ``brackets''}\label{review-of-brackets}}

In Python, there are several types of brackets used for different
purposes. Here's a brief review of the most commonly used brackets:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Parentheses \texttt{(\ )}: Parentheses are used for grouping
  expressions, defining function parameters, and invoking functions.
  They are also used in mathematical expressions to indicate order of
  operations.
\item
  Square brackets \texttt{{[}\ {]}}: Square brackets are primarily used
  for indexing and slicing operations on lists, tuples, and strings.
  They allow you to access individual elements or extract subsequences
  from these data types.
\item
  Curly brackets or braces \texttt{\{\ \}}: Curly brackets are used to
  define dictionaries, which are key-value pairs. Dictionaries store
  data in an unordered manner, and you can access or manipulate values
  by referencing their corresponding keys within the curly brackets.
\end{enumerate}

It's important to note that the usage of these brackets may vary
depending on the specific context or programming paradigm you're working
with. Nonetheless, understanding their general purpose will help you
navigate Python code effectively.

\hypertarget{examples-1}{%
\paragraph{Examples}\label{examples-1}}

Here are a few examples of how each type of bracket is used in Python:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Parentheses \texttt{(\ )}:

  \begin{itemize}
  \item
    Grouping expressions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\OperatorTok{=}\NormalTok{ (}\DecValTok{2} \OperatorTok{+} \DecValTok{3}\NormalTok{) }\OperatorTok{*} \DecValTok{4}  
\CommentTok{\# Output: 20}
\end{Highlighting}
\end{Shaded}
  \item
    Defining function parameters:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ greet(name):}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"Hello, "} \OperatorTok{+}\NormalTok{ name }\OperatorTok{+} \StringTok{"!"}\NormalTok{)}

\NormalTok{greet(}\StringTok{"Alice"}\NormalTok{)  }
\CommentTok{\# Output: Hello, Alice!}
\end{Highlighting}
\end{Shaded}
  \item
    Invoking functions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{)  }
\CommentTok{\# Output: 10}
\end{Highlighting}
\end{Shaded}
  \end{itemize}
\item
  Square brackets \texttt{{[}\ {]}}:

  \begin{itemize}
  \item
    Indexing and slicing:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_list }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(my\_list[}\DecValTok{0}\NormalTok{])      }
\CommentTok{\# Output: 1}
\BuiltInTok{print}\NormalTok{(my\_list[}\DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{])    }
\CommentTok{\# Output: [2, 3]}
\end{Highlighting}
\end{Shaded}
  \item
    Modifying list elements:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_list }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{]}
\NormalTok{my\_list[}\DecValTok{1}\NormalTok{] }\OperatorTok{=} \DecValTok{10}
\BuiltInTok{print}\NormalTok{(my\_list)         }
\CommentTok{\# Output: [1, 10, 3]}
\end{Highlighting}
\end{Shaded}
  \end{itemize}
\item
  Curly brackets or braces \texttt{\{\ \}}:

  \begin{itemize}
  \item
    Defining dictionaries:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_dict }\OperatorTok{=}\NormalTok{ \{}\StringTok{"name"}\NormalTok{: }\StringTok{"Alice"}\NormalTok{, }\StringTok{"age"}\NormalTok{: }\DecValTok{25}\NormalTok{, }\StringTok{"city"}\NormalTok{: }\StringTok{"London"}\NormalTok{\}}
\BuiltInTok{print}\NormalTok{(my\_dict[}\StringTok{"name"}\NormalTok{])    }
\CommentTok{\# Output: Alice}
\end{Highlighting}
\end{Shaded}
  \item
    Modifying dictionary values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_dict }\OperatorTok{=}\NormalTok{ \{}\StringTok{"name"}\NormalTok{: }\StringTok{"Alice"}\NormalTok{, }\StringTok{"age"}\NormalTok{: }\DecValTok{25}\NormalTok{\}}
\NormalTok{my\_dict[}\StringTok{"age"}\NormalTok{] }\OperatorTok{=} \DecValTok{30}
\BuiltInTok{print}\NormalTok{(my\_dict)            }
\CommentTok{\# Output: \{\textquotesingle{}name\textquotesingle{}: \textquotesingle{}Alice\textquotesingle{}, \textquotesingle{}age\textquotesingle{}: 30\}}
\end{Highlighting}
\end{Shaded}
  \end{itemize}
\end{enumerate}

Remember that the usage of brackets can vary depending on the specific
programming context, but these examples provide a general understanding
of their usage in Python.

\hypertarget{tasks-2}{%
\subsection{\texorpdfstring{\href{Lecture6_exercises.ipynb}{Tasks}}{Tasks}}\label{tasks-2}}

\hypertarget{data-manipulation-with-pandas}{%
\subsection{Data Manipulation with
pandas}\label{data-manipulation-with-pandas}}

While we have seen panda's ability to (i) mix data types (strings,
numbers, categories, Boolean, \ldots) and (ii) refer to columns and rows
by names, this library offers a lot more powerful tools for efficiently
gaining insights from data, e.g.

\begin{itemize}
\tightlist
\item
  deal with missing values
\end{itemize}

\hypertarget{missing-values}{%
\subsection{Missing Values}\label{missing-values}}

Missing data occurs commonly in many data analysis applications. Most
often they are a consequence of

\begin{itemize}
\tightlist
\item
  data entry errors, or
\item
  unknown numbers, or
\item
  \texttt{groupby} operations, or
\item
  wrong mathematical operations (\texttt{1/0},
  \(\sqrt{-1}\),\(\log(0)\), \ldots)
\item
  ``not applicable'' questions, or
\item
  \ldots.
\end{itemize}

For data with float type, pandas uses the floating-point value
\texttt{NaN} (Not a Number) to represent missing data.

Pandas refers to missing data as \texttt{NA}, which stands for \emph{not
available}.

The built-in Python \texttt{None} value is also treated as NA:

Recall constructing a DataFrame from a dictionary of equal-length lists
or NumPy arrays (Lecture 4). What if there were data entry errors or
just unknown numbers

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{=}\NormalTok{ \{}\StringTok{"state"}\NormalTok{: [}\StringTok{"Ohio"}\NormalTok{, }\StringTok{"Ohio"}\NormalTok{, }\StringTok{"Ohio"}\NormalTok{, }\StringTok{"Nevada"}\NormalTok{, }\StringTok{"Nevada"}\NormalTok{, }\StringTok{"Nevada"}\NormalTok{],}
        \StringTok{"year"}\NormalTok{: [}\DecValTok{2000}\NormalTok{, }\DecValTok{2001}\NormalTok{, }\VariableTok{None}\NormalTok{, }\DecValTok{2001}\NormalTok{, }\DecValTok{2002}\NormalTok{, }\DecValTok{2003}\NormalTok{],}
        \StringTok{"gdp"}\NormalTok{: [}\FloatTok{1.5}\NormalTok{, }\FloatTok{1.7}\NormalTok{, }\FloatTok{3.6}\NormalTok{, }\FloatTok{2.4}\NormalTok{, np.nan, }\FloatTok{3.2}\NormalTok{]\}}
\NormalTok{frame }\OperatorTok{=}\NormalTok{ pd.DataFrame(data)}\CommentTok{\# creates a dataframe out of the data given!}
\NormalTok{df }\OperatorTok{=}\NormalTok{ frame}
\NormalTok{frame}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule()
& state & year & gdp \\
\midrule()
\endhead
0 & Ohio & 2000.0 & 1.5 \\
1 & Ohio & 2001.0 & 1.7 \\
2 & Ohio & NaN & 3.6 \\
3 & Nevada & 2001.0 & 2.4 \\
4 & Nevada & 2002.0 & NaN \\
5 & Nevada & 2003.0 & 3.2 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{])}
\NormalTok{x}\OperatorTok{==}\DecValTok{2}
\CommentTok{\#remove those values of x that are equal to 2}
\CommentTok{\#x[[0,2,3]]}
\NormalTok{x[x}\OperatorTok{!=}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([1, 3, 4])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frame.columns[frame.isna().}\BuiltInTok{any}\NormalTok{()]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Index(['year', 'gdp'], dtype='object')
\end{verbatim}

(Note the annoying conversion of integers to float, a solution discussed
\href{https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html}{here})

\hypertarget{filtering-out-missing-data}{%
\subsubsection{Filtering Out Missing
Data}\label{filtering-out-missing-data}}

There are a few ways to filter out missing data. While you always have
the option to do it by hand using \texttt{pandas.isna} and Boolean
indexing, \texttt{dropna} can be helpful.

With DataFrame objects, there are different ways to remove missing data.
You may want to drop rows or columns that are all \texttt{NA}, or only
those rows or columns containing any \texttt{NA}s at all.
\texttt{dropna} by default drops any row containing a missing value:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frame.dropna()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule()
& state & year & gdp \\
\midrule()
\endhead
0 & Ohio & 2000.0 & 1.5 \\
1 & Ohio & 2001.0 & 1.7 \\
3 & Nevada & 2001.0 & 2.4 \\
5 & Nevada & 2003.0 & 3.2 \\
\bottomrule()
\end{longtable}

Passing \texttt{how="all"} will drop only rows that are all NA:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frame.dropna(how}\OperatorTok{=}\StringTok{"all"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule()
& state & year & gdp \\
\midrule()
\endhead
0 & Ohio & 2000.0 & 1.5 \\
1 & Ohio & 2001.0 & 1.7 \\
2 & Ohio & NaN & 3.6 \\
3 & Nevada & 2001.0 & 2.4 \\
4 & Nevada & 2002.0 & NaN \\
5 & Nevada & 2003.0 & 3.2 \\
\bottomrule()
\end{longtable}

To drop columns in the same way, pass \texttt{axis="columns"}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frame.dropna(axis}\OperatorTok{=}\StringTok{"columns"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}ll@{}}
\toprule()
& state \\
\midrule()
\endhead
0 & Ohio \\
1 & Ohio \\
2 & Ohio \\
3 & Nevada \\
4 & Nevada \\
5 & Nevada \\
\bottomrule()
\end{longtable}

Suppose you want to keep only rows containing at most a certain number
of missing observations. You can indicate this with the \texttt{thresh}
argument:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frame.iloc[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OperatorTok{=}\NormalTok{ np.nan}
\NormalTok{frame}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule()
& state & year & gdp \\
\midrule()
\endhead
0 & Ohio & 2000.0 & 1.5 \\
1 & Ohio & 2001.0 & 1.7 \\
2 & Ohio & NaN & NaN \\
3 & Nevada & 2001.0 & 2.4 \\
4 & Nevada & 2002.0 & NaN \\
5 & Nevada & 2003.0 & 3.2 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frame.dropna(thresh}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule()
& state & year & gdp \\
\midrule()
\endhead
0 & Ohio & 2000.0 & 1.5 \\
1 & Ohio & 2001.0 & 1.7 \\
3 & Nevada & 2001.0 & 2.4 \\
4 & Nevada & 2002.0 & NaN \\
5 & Nevada & 2003.0 & 3.2 \\
\bottomrule()
\end{longtable}

\hypertarget{filling-in-missing-data}{%
\subsubsection{Filling In Missing Data}\label{filling-in-missing-data}}

Rather than filtering out missing data (and potentially discarding other
data along with it), you may want to fill in the ``holes'' in any number
of ways. For most purposes, the fillna method is the workhorse function
to use. Calling \texttt{fillna} with a constant replaces missing values
with that value:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frame2 }\OperatorTok{=}\NormalTok{ frame}
\NormalTok{frame2.iloc[}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{] }\OperatorTok{=}\NormalTok{ np.nan}\CommentTok{\#None}
\NormalTok{frame2.iloc[}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OperatorTok{=}\NormalTok{ np.nan}
\NormalTok{frame2}
\CommentTok{\#frame2.fillna(0)}
\CommentTok{\#type(frame2["state"])}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule()
& state & year & gdp \\
\midrule()
\endhead
0 & NaN & 2000.0 & NaN \\
1 & Ohio & 2001.0 & 1.70 \\
2 & Ohio & NaN & 3.60 \\
3 & Nevada & 2001.0 & 2.40 \\
4 & Nevada & 2002.0 & 2.48 \\
5 & Nevada & 2003.0 & 3.20 \\
\bottomrule()
\end{longtable}

Calling \texttt{fillna} with a dictionary, you can use a different fill
value for each column:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frame2.fillna(\{}\StringTok{"state"}\NormalTok{ : }\StringTok{"Neverland"}\NormalTok{, }\StringTok{"year"}\NormalTok{: }\OperatorTok{{-}}\DecValTok{999}\NormalTok{, }\StringTok{"gdp"}\NormalTok{: }\DecValTok{0}\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule()
& state & year & gdp \\
\midrule()
\endhead
0 & Neverland & 2000.0 & 0.00 \\
1 & Ohio & 2001.0 & 1.70 \\
2 & Ohio & -999.0 & 3.60 \\
3 & Nevada & 2001.0 & 2.40 \\
4 & Nevada & 2002.0 & 2.48 \\
5 & Nevada & 2003.0 & 3.20 \\
\bottomrule()
\end{longtable}

With \texttt{fillna} you can do lots of other things such as simple data
imputation using the median or mean statistics, at least for purely
numeric data types:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frame[}\StringTok{\textquotesingle{}gdp\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ frame[}\StringTok{\textquotesingle{}gdp\textquotesingle{}}\NormalTok{].fillna(frame[}\StringTok{\textquotesingle{}gdp\textquotesingle{}}\NormalTok{].mean())}
\NormalTok{frame}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule()
& state & year & gdp \\
\midrule()
\endhead
0 & NaN & 2000.0 & 1.50 \\
1 & Ohio & 2001.0 & 1.70 \\
2 & Ohio & NaN & 3.60 \\
3 & Nevada & 2001.0 & 2.40 \\
4 & Nevada & 2002.0 & 2.48 \\
5 & Nevada & 2003.0 & 3.20 \\
\bottomrule()
\end{longtable}

Different values for each column

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.fillna(df.mean())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.
  df.fillna(df.mean())
\end{verbatim}

\begin{longtable}[]{@{}llll@{}}
\toprule()
& state & year & gdp \\
\midrule()
\endhead
0 & Ohio & 2000.0 & 1.50 \\
1 & Ohio & 2001.0 & 1.70 \\
2 & Ohio & 2001.4 & 3.60 \\
3 & Nevada & 2001.0 & 2.40 \\
4 & Nevada & 2002.0 & 2.48 \\
5 & Nevada & 2003.0 & 3.20 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fillna(frame[}\StringTok{\textquotesingle{}gdp\textquotesingle{}}\NormalTok{].mean())}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{duplicate-values}{%
\subsection{Duplicate Values}\label{duplicate-values}}

Duplicate rows may be found in a DataFrame for any number of reasons.

Removing duplicates in pandas can be useful in various scenarios,
particularly when working with large datasets or performing data
analysis. Here's a convincing example to illustrate its usefulness:

Let's say you have a dataset containing sales transactions from an
online store. Each transaction record consists of multiple columns,
including customer ID, product ID, purchase date, and purchase amount.
Due to various reasons such as system glitches or human errors,
duplicate entries might exist in the dataset, meaning that multiple
identical transaction records are present.

In such a scenario, removing duplicates becomes beneficial for several
reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Accurate Analysis: Duplicate entries can skew your analysis and lead
  to incorrect conclusions. By removing duplicates, you ensure that each
  transaction is represented only once, providing more accurate insights
  and preventing inflated or biased results.
\item
  Data Integrity: Duplicate entries consume unnecessary storage space
  and can make data management more challenging. By eliminating
  duplicates, you maintain data integrity and ensure a clean and
  organized dataset.
\item
  Efficiency: When dealing with large datasets, duplicate records can
  significantly impact computational efficiency. Removing duplicates
  allows you to streamline your data processing operations, leading to
  faster analysis and improved performance.
\item
  Unique Identifiers: Removing duplicates becomes crucial when working
  with columns that should contain unique values, such as customer IDs
  or product IDs. By eliminating duplicates, you ensure the integrity of
  these unique identifiers and prevent issues when performing joins or
  merging dataframes.
\end{enumerate}

To remove duplicates in pandas, you can use the
\texttt{drop\_duplicates()} function. It identifies and removes
duplicate rows based on specified columns or all columns in the
dataframe, depending on your requirements.

Overall, removing duplicates in pandas is essential for maintaining data
accuracy, integrity, and efficiency, allowing you to derive meaningful
insights and make informed decisions based on reliable data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#super simple example:}
\NormalTok{frame.iloc[}\DecValTok{5}\NormalTok{] }\OperatorTok{=}\NormalTok{ frame.iloc[}\DecValTok{4}\NormalTok{]}\CommentTok{\#this line makes lines 5 and 6 equal }
\NormalTok{frame}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule()
& state & year & gdp \\
\midrule()
\endhead
0 & NaN & 2000.0 & NaN \\
1 & Ohio & 2001.0 & 1.70 \\
2 & Ohio & NaN & 3.60 \\
3 & Nevada & 2001.0 & 2.40 \\
4 & Nevada & 2002.0 & 2.48 \\
5 & Nevada & 2002.0 & 2.48 \\
\bottomrule()
\end{longtable}

The DataFrame method \texttt{duplicated} returns a Boolean Series
indicating whether each row is a duplicate

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frame.duplicated()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0    False
1    False
2    False
3    False
4    False
5     True
dtype: bool
\end{verbatim}

Relatedly, \texttt{drop\_duplicates} returns a DataFrame with rows where
the \texttt{duplicated} array is \texttt{False} filtered out:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frame.drop\_duplicates()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule()
& state & year & gdp \\
\midrule()
\endhead
0 & NaN & 2000.0 & NaN \\
1 & Ohio & 2001.0 & 1.70 \\
2 & Ohio & NaN & 3.60 \\
3 & Nevada & 2001.0 & 2.40 \\
4 & Nevada & 2002.0 & 2.48 \\
\bottomrule()
\end{longtable}

Both methods by default consider all of the columns; alternatively, you
can specify any \texttt{subset} of them to detect duplicates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frame.drop\_duplicates(subset}\OperatorTok{=}\NormalTok{[}\StringTok{"year"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule()
& state & year & gdp \\
\midrule()
\endhead
0 & NaN & 2000.0 & NaN \\
1 & Ohio & 2001.0 & 1.70 \\
2 & Ohio & NaN & 3.60 \\
4 & Nevada & 2002.0 & 2.48 \\
\bottomrule()
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{titanic-data-1}{%
\paragraph{Titanic data}\label{titanic-data-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Since pandas does not have any built in data, I am going to "cheat" and }
\CommentTok{\# make use of the \textasciigrave{}seaborn\textasciigrave{} library}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns }

\NormalTok{titanic }\OperatorTok{=}\NormalTok{ sns. load\_dataset(}\StringTok{\textquotesingle{}titanic\textquotesingle{}}\NormalTok{)}
\NormalTok{titanic[}\StringTok{"3rdClass"}\NormalTok{] }\OperatorTok{=}\NormalTok{ titanic[}\StringTok{"pclass"}\NormalTok{]}\OperatorTok{==}\DecValTok{3}
\NormalTok{titanic[}\StringTok{"male"}\NormalTok{] }\OperatorTok{=}\NormalTok{ titanic[}\StringTok{"sex"}\NormalTok{]}\OperatorTok{==}\StringTok{"male"}

\NormalTok{titanic}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llllllllllllllllll@{}}
\toprule()
& survived & pclass & sex & age & sibsp & parch & fare & embarked &
class & who & adult\_male & deck & embark\_town & alive & alone &
3rdClass & male \\
\midrule()
\endhead
0 & 0 & 3 & male & 22.0 & 1 & 0 & 7.2500 & S & Third & man & True & NaN
& Southampton & no & False & True & True \\
1 & 1 & 1 & female & 38.0 & 1 & 0 & 71.2833 & C & First & woman & False
& C & Cherbourg & yes & False & False & False \\
2 & 1 & 3 & female & 26.0 & 0 & 0 & 7.9250 & S & Third & woman & False &
NaN & Southampton & yes & True & True & False \\
3 & 1 & 1 & female & 35.0 & 1 & 0 & 53.1000 & S & First & woman & False
& C & Southampton & yes & False & False & False \\
4 & 0 & 3 & male & 35.0 & 0 & 0 & 8.0500 & S & Third & man & True & NaN
& Southampton & no & True & True & True \\
... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... &
... & ... & ... & ... & ... & ... \\
886 & 0 & 2 & male & 27.0 & 0 & 0 & 13.0000 & S & Second & man & True &
NaN & Southampton & no & True & False & True \\
887 & 1 & 1 & female & 19.0 & 0 & 0 & 30.0000 & S & First & woman &
False & B & Southampton & yes & True & False & False \\
888 & 0 & 3 & female & NaN & 1 & 2 & 23.4500 & S & Third & woman & False
& NaN & Southampton & no & False & True & False \\
889 & 1 & 1 & male & 26.0 & 0 & 0 & 30.0000 & C & First & man & True & C
& Cherbourg & yes & True & False & True \\
890 & 0 & 3 & male & 32.0 & 0 & 0 & 7.7500 & Q & Third & man & True &
NaN & Queenstown & no & True & True & True \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#how many missing values in age ?}
\NormalTok{np.}\BuiltInTok{sum}\NormalTok{(titanic[}\StringTok{"age"}\NormalTok{].isna())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
177
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{titanic.describe()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllllll@{}}
\toprule()
& survived & pclass & age & sibsp & parch & fare \\
\midrule()
\endhead
count & 891.000000 & 891.000000 & 714.000000 & 891.000000 & 891.000000 &
891.000000 \\
mean & 0.383838 & 2.308642 & 29.699118 & 0.523008 & 0.381594 &
32.204208 \\
std & 0.486592 & 0.836071 & 14.526497 & 1.102743 & 0.806057 &
49.693429 \\
min & 0.000000 & 1.000000 & 0.420000 & 0.000000 & 0.000000 & 0.000000 \\
25\% & 0.000000 & 2.000000 & 20.125000 & 0.000000 & 0.000000 &
7.910400 \\
50\% & 0.000000 & 3.000000 & 28.000000 & 0.000000 & 0.000000 &
14.454200 \\
75\% & 1.000000 & 3.000000 & 38.000000 & 1.000000 & 0.000000 &
31.000000 \\
max & 1.000000 & 3.000000 & 80.000000 & 8.000000 & 6.000000 &
512.329200 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.mean(titanic[}\StringTok{"age"}\NormalTok{])}

\NormalTok{np.}\BuiltInTok{sum}\NormalTok{(titanic[}\StringTok{"age"}\NormalTok{])}\OperatorTok{/}\DecValTok{714}

\NormalTok{titanic[}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ titanic[}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{].fillna(titanic[}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{].mean())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
29.69911764705882
\end{verbatim}

Notice that the age columns contains missing values, which is a big
topic by itself in data science.

How should an aggregating react to and handle missing values? The
default often is to ignore and exclude them from the computation, e.g.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#the following shoudl be equal but is not due to missing values}
\NormalTok{meanAge }\OperatorTok{=}\NormalTok{np.mean(titanic.age)}
\BuiltInTok{print}\NormalTok{(meanAge)}
\BuiltInTok{print}\NormalTok{(np.}\BuiltInTok{sum}\NormalTok{(titanic.age)}\OperatorTok{/}\BuiltInTok{len}\NormalTok{(titanic.age))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
29.69911764705882
23.79929292929293
\end{verbatim}

In general, it is a good idea to diagnose how many missing values there
are in each column. We can use some handy built-in support for this
task:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{titanic.isna().}\BuiltInTok{sum}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
survived         0
pclass           0
sex              0
age            177
sibsp            0
parch            0
fare             0
embarked         2
class            0
who              0
adult_male       0
deck           688
embark_town      2
alive            0
alone            0
3rdClass         0
male             0
dtype: int64
\end{verbatim}

\hypertarget{tasks-3}{%
\subsection{\texorpdfstring{\href{Lecture6_exercises.ipynb}{Tasks}}{Tasks}}\label{tasks-3}}

Dropping or replacing NAs:

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{plotting-1}{%
\section{Plotting}\label{plotting-1}}

The ``plot type of the day'' is one of the most popular ones used to
display data distributions, the \textbf{boxplot}.

Boxplots, also known as \textbf{box-and-whisker plots}, are a
statistical visualization tool that provides a concise summary of a
dataset's distribution. They display key descriptive statistics and
provide insights into the central tendency, variability, and skewness of
the data. Here's a brief introduction and motivation for using boxplots:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Structure of Boxplots: Boxplots consist of a box and whiskers that
  represent different statistical measures of the data:

  \begin{itemize}
  \tightlist
  \item
    The box represents the interquartile range (IQR), which spans from
    the lower quartile (25th percentile) to the upper quartile (75th
    percentile). The width of the box indicates the spread of the middle
    50\% of the data.
  \item
    A line (whisker) extends from each end of the box to show the
    minimum and maximum values within a certain range (often defined as
    1.5 times the IQR).
  \item
    Points beyond the whiskers are considered outliers and plotted
    individually.
  \end{itemize}
\item
  Motivation for Using Boxplots: Boxplots offer several benefits and are
  commonly used for the following reasons:

  \begin{itemize}
  \tightlist
  \item
    Visualizing Data Distribution: Boxplots provide a concise overview
    of the distribution of a dataset. They show the skewness, symmetry,
    and presence of outliers, allowing for quick identification of key
    features.
  \item
    Comparing Groups: Boxplots enable easy visual comparison of multiple
    groups or categories. By placing side-by-side boxplots, you can
    assess differences in central tendency and variability between
    groups.
  \item
    Outlier Detection: Boxplots explicitly mark outliers, aiding in the
    identification of extreme values or data points that deviate
    significantly from the overall pattern.
  \item
    Data Summary: Boxplots summarize key statistics, including the
    median, quartiles, and range, providing a quick understanding of the
    dataset without the need for detailed calculations.
  \item
    Robustness: Boxplots are relatively robust to skewed or asymmetric
    data and can effectively handle datasets with outliers.
  \end{itemize}
\end{enumerate}

Boxplots are widely used in various fields, including data analysis,
exploratory data visualization, and statistical reporting. They offer a
clear and concise representation of data distribution, making them a
valuable tool for understanding and communicating the characteristics of
a dataset.

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{!}\NormalTok{pip install gapminder}
\ImportTok{from}\NormalTok{ gapminder }\ImportTok{import}\NormalTok{ gapminder}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllllll@{}}
\toprule()
& country & continent & year & lifeExp & pop & gdpPercap \\
\midrule()
\endhead
0 & Afghanistan & Asia & 1952 & 28.801 & 8425333 & 779.445314 \\
1 & Afghanistan & Asia & 1957 & 30.332 & 9240934 & 820.853030 \\
2 & Afghanistan & Asia & 1962 & 31.997 & 10267083 & 853.100710 \\
3 & Afghanistan & Asia & 1967 & 34.020 & 11537966 & 836.197138 \\
4 & Afghanistan & Asia & 1972 & 36.088 & 13079460 & 739.981106 \\
... & ... & ... & ... & ... & ... & ... \\
1699 & Zimbabwe & Africa & 1987 & 62.351 & 9216418 & 706.157306 \\
1700 & Zimbabwe & Africa & 1992 & 60.377 & 10704340 & 693.420786 \\
1701 & Zimbabwe & Africa & 1997 & 46.809 & 11404948 & 792.449960 \\
1702 & Zimbabwe & Africa & 2002 & 39.989 & 11926563 & 672.038623 \\
1703 & Zimbabwe & Africa & 2007 & 43.487 & 12311143 & 469.709298 \\
\bottomrule()
\end{longtable}

The pandas way

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder.boxplot(column }\OperatorTok{=} \StringTok{"lifeExp"}\NormalTok{, by}\OperatorTok{=}\StringTok{"continent"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./IntroCoding_Lecture6_files/figure-pdf/cell-30-output-1.png}

}

\end{figure}

The matplotlib way

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.boxplot(gapminder[}\StringTok{"continent"}\NormalTok{], gapminder[}\StringTok{"lifeExp"}\NormalTok{])}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\hypertarget{task}{%
\subsection{Task}\label{task}}

\begin{itemize}
\item
  Create a boxplot for \texttt{gdpPercap} instead. What do you notice ?
  Are you happy with how the plot looks? Any ``trick'' you can think to
  make this more readable?
\item
  Advanced: can you create boxplots for \texttt{gdpPerCap} and
  \texttt{lifeExp} in one command?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder.boxplot(column }\OperatorTok{=} \StringTok{"gdpPercap"}\NormalTok{, by}\OperatorTok{=}\StringTok{"continent"}\NormalTok{)}\OperatorTok{;}
\NormalTok{plt.yscale(}\StringTok{"log"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./IntroCoding_Lecture6_files/figure-pdf/cell-32-output-1.png}

}

\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Further Reading:

\begin{itemize}
\tightlist
\item
  \href{https://realpython.com/python-matplotlib-guide/\#:~:text=A\%20Figure\%20object\%20is\%20the,\%E2\%80\%9D\%20as\%20we\%20might\%20expect}{Python
  Plotting With Matplotlib Tutorial}.)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ entropy}

\NormalTok{p }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\OperatorTok{/}\DecValTok{100}\NormalTok{, }\DecValTok{99}\OperatorTok{/}\DecValTok{100}\NormalTok{])}
\NormalTok{n}\OperatorTok{=}\DecValTok{2}
\CommentTok{\#p = np.array(np.ones)/n}
\NormalTok{H }\OperatorTok{=}\NormalTok{ entropy(p, base}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{H}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.08079313589591118
\end{verbatim}

\bookmarksetup{startatroot}

\hypertarget{intro-to-models}{%
\chapter{Intro to Models}\label{intro-to-models}}

In this lecture we will learn about \textbf{modeling} data for the first
time. After this lesson, you should know what we generally mean by a
``model'', what linear regression is and how to interpret the output.
But first we need to introduce a new data type: \emph{categorical
variables}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \protect\hyperlink{Categorical-variables}{Categorical variables}
\item
  \protect\hyperlink{Models}{Models}

  \begin{itemize}
  \tightlist
  \item
    \protect\hyperlink{Tables-as-models}{Tables as models}
  \item
    \protect\hyperlink{Modeling-Missing-Values}{Modeling Missing Values}
  \item
    \protect\hyperlink{Linear-Regression}{Linear Regression}
  \end{itemize}
\end{enumerate}

Online Resources:

\href{https://wesmckinney.com/book/data-cleaning.html\#pandas-categorical}{Chapter
7.5} of our textbook introduces categorical variables.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\ImportTok{from}\NormalTok{ numpy.random }\ImportTok{import}\NormalTok{ default\_rng}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ statsmodels.api }\ImportTok{as}\NormalTok{ sm}
\ImportTok{import}\NormalTok{ statsmodels.formula.api }\ImportTok{as}\NormalTok{ smf}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#!pip install gapminder}
\ImportTok{from}\NormalTok{ gapminder }\ImportTok{import}\NormalTok{ gapminder}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder.head()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllllll@{}}
\toprule()
& country & continent & year & lifeExp & pop & gdpPercap \\
\midrule()
\endhead
0 & Afghanistan & Asia & 1952 & 28.801 & 8425333 & 779.445314 \\
1 & Afghanistan & Asia & 1957 & 30.332 & 9240934 & 820.853030 \\
2 & Afghanistan & Asia & 1962 & 31.997 & 10267083 & 853.100710 \\
3 & Afghanistan & Asia & 1967 & 34.020 & 11537966 & 836.197138 \\
4 & Afghanistan & Asia & 1972 & 36.088 & 13079460 & 739.981106 \\
\bottomrule()
\end{longtable}

\hypertarget{categorical-variables}{%
\section{Categorical variables}\label{categorical-variables}}

As a motivation, take another look at the gapminder data which contains
variables of a \textbf{mixed type}: numeric columns along with string
type columns which contain repeated instances of a smaller set of
distinct or \textbf{discrete} values which

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  are not numeric (but could be represented as numbers)
\item
  cannot really be ordered
\item
  typically take on a finite set of values, or \emph{categories}.
\end{enumerate}

We refer to these data types as \textbf{categorical}.

We have already seen functions like \texttt{unique} and
\texttt{value\_counts}, which enable us to extract the distinct values
from an array and compute their frequencies.

Boxplots and grouping operations typically use a categorical variable to
compute summaries of a numerical variables for each category separately,
e.g.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder.boxplot(column }\OperatorTok{=} \StringTok{"lifeExp"}\NormalTok{, by}\OperatorTok{=}\StringTok{"continent"}\NormalTok{,figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{2}\NormalTok{))}\OperatorTok{;}
\NormalTok{plt.title(}\StringTok{\textquotesingle{}Life expectancy by continent\textquotesingle{}}\NormalTok{)}
\CommentTok{\# Remove the default suptitle}
\NormalTok{plt.suptitle(}\StringTok{""}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./IntroCoding_Lecture7_files/figure-pdf/cell-6-output-1.png}

}

\end{figure}

pandas has a special \texttt{Categorical} extension type for holding
data that uses the integer-based categorical representation or encoding.
This is a popular data compression technique for data with many
occurrences of similar values and can provide significantly faster
performance with lower memory use, especially for string data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder.info()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1704 entries, 0 to 1703
Data columns (total 6 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   country    1704 non-null   object 
 1   continent  1704 non-null   object 
 2   year       1704 non-null   int64  
 3   lifeExp    1704 non-null   float64
 4   pop        1704 non-null   int64  
 5   gdpPercap  1704 non-null   float64
dtypes: float64(2), int64(2), object(2)
memory usage: 80.0+ KB
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder[}\StringTok{\textquotesingle{}country\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ gapminder[}\StringTok{\textquotesingle{}country\textquotesingle{}}\NormalTok{].astype(}\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{)}
\NormalTok{gapminder.info()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1704 entries, 0 to 1703
Data columns (total 6 columns):
 #   Column     Non-Null Count  Dtype   
---  ------     --------------  -----   
 0   country    1704 non-null   category
 1   continent  1704 non-null   object  
 2   year       1704 non-null   int64   
 3   lifeExp    1704 non-null   float64 
 4   pop        1704 non-null   int64   
 5   gdpPercap  1704 non-null   float64 
dtypes: category(1), float64(2), int64(2), object(1)
memory usage: 75.2+ KB
\end{verbatim}

We will come back to the usefulness of this later.

\hypertarget{tables-as-models}{%
\section{Tables as models}\label{tables-as-models}}

For now let us look at our first ``model'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{titanic }\OperatorTok{=}\NormalTok{ sns.load\_dataset(}\StringTok{\textquotesingle{}titanic\textquotesingle{}}\NormalTok{)}
\NormalTok{titanic[}\StringTok{"class3"}\NormalTok{] }\OperatorTok{=}\NormalTok{ (titanic[}\StringTok{"pclass"}\NormalTok{]}\OperatorTok{==}\DecValTok{3}\NormalTok{)}
\NormalTok{titanic[}\StringTok{"male"}\NormalTok{] }\OperatorTok{=}\NormalTok{ (titanic[}\StringTok{"sex"}\NormalTok{]}\OperatorTok{==}\StringTok{"male"}\NormalTok{)}
\NormalTok{titanic.head()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llllllllllllllllll@{}}
\toprule()
& survived & pclass & sex & age & sibsp & parch & fare & embarked &
class & who & adult\_male & deck & embark\_town & alive & alone & class3
& male \\
\midrule()
\endhead
0 & 0 & 3 & male & 22.0 & 1 & 0 & 7.2500 & S & Third & man & True & NaN
& Southampton & no & False & True & True \\
1 & 1 & 1 & female & 38.0 & 1 & 0 & 71.2833 & C & First & woman & False
& C & Cherbourg & yes & False & False & False \\
2 & 1 & 3 & female & 26.0 & 0 & 0 & 7.9250 & S & Third & woman & False &
NaN & Southampton & yes & True & True & False \\
3 & 1 & 1 & female & 35.0 & 1 & 0 & 53.1000 & S & First & woman & False
& C & Southampton & yes & False & False & False \\
4 & 0 & 3 & male & 35.0 & 0 & 0 & 8.0500 & S & Third & man & True & NaN
& Southampton & no & True & True & True \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vals1, cts1 }\OperatorTok{=}\NormalTok{ np.unique(titanic[}\StringTok{"class3"}\NormalTok{], return\_counts}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(cts1)}
\BuiltInTok{print}\NormalTok{(vals1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[400 491]
[False  True]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"The mean survival on the Titanic was"}\NormalTok{, np.mean(titanic.survived))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The mean survival on the Titanic was 0.3838383838383838
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ConTbl }\OperatorTok{=}\NormalTok{ pd.crosstab(titanic[}\StringTok{"sex"}\NormalTok{], titanic[}\StringTok{"survived"}\NormalTok{])}
\NormalTok{ConTbl}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule()
survived & 0 & 1 \\
sex & & \\
\midrule()
\endhead
female & 81 & 233 \\
male & 468 & 109 \\
\bottomrule()
\end{longtable}

What are the estimated survival probabilities?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#the good old groupby way:}
\NormalTok{bySex }\OperatorTok{=}\NormalTok{ titanic.groupby(}\StringTok{"sex"}\NormalTok{).survived}
\NormalTok{bySex.mean()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
sex
female    0.742038
male      0.188908
Name: survived, dtype: float64
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p3D }\OperatorTok{=}\NormalTok{ pd.crosstab([titanic[}\StringTok{"sex"}\NormalTok{], titanic[}\StringTok{"class3"}\NormalTok{]], titanic[}\StringTok{"survived"}\NormalTok{])}
\NormalTok{p3D}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule()
& survived & 0 & 1 \\
sex & class3 & & \\
\midrule()
\endhead
\multirow{2}{*}{female} & False & 9 & 161 \\
& True & 72 & 72 \\
\multirow{2}{*}{male} & False & 168 & 62 \\
& True & 300 & 47 \\
\bottomrule()
\end{longtable}

What are the estimated survival probabilities?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#the good old groupby way:}
\NormalTok{bySex }\OperatorTok{=}\NormalTok{ titanic.groupby([}\StringTok{"sex"}\NormalTok{, }\StringTok{"class3"}\NormalTok{]).survived}
\NormalTok{bySex.mean()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
sex     class3
female  False     0.947059
        True      0.500000
male    False     0.269565
        True      0.135447
Name: survived, dtype: float64
\end{verbatim}

The above table can be looked at as a \textbf{model}, which is defined
as a function which takes \emph{inputs} \textbf{x} and ``spits out'' a
\emph{prediction}:

\(y = f(\mathbf{x})\)

In our case, the inputs are \(x_1=\text{sex}\), \(x_2=\text{class3}\),
and the output is the estimated survival probability!

It is evident that we could keep adding more \emph{input} variables and
make finer and finer grained predictions.

\hypertarget{linear-models}{%
\subsection{Linear Models}\label{linear-models}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lsFit }\OperatorTok{=}\NormalTok{ smf.ols(}\StringTok{\textquotesingle{}survived \textasciitilde{} sex:class3{-}1\textquotesingle{}}\NormalTok{, titanic).fit()}
\NormalTok{lsFit.summary().tables[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllllll@{}}
\toprule()
\endhead
& coef & std err & t & P\textgreater\textbar t\textbar{} & {[}0.025 &
0.975{]} \\
sex{[}female{]}:class3{[}False{]} & 0.9471 & 0.029 & 32.200 & 0.000 &
0.889 & 1.005 \\
sex{[}male{]}:class3{[}False{]} & 0.2696 & 0.025 & 10.660 & 0.000 &
0.220 & 0.319 \\
sex{[}female{]}:class3{[}True{]} & 0.5000 & 0.032 & 15.646 & 0.000 &
0.437 & 0.563 \\
sex{[}male{]}:class3{[}True{]} & 0.1354 & 0.021 & 6.579 & 0.000 & 0.095
& 0.176 \\
\bottomrule()
\end{longtable}

\hypertarget{modeling-missing-values}{%
\subsection{Modeling Missing Values}\label{modeling-missing-values}}

We have already seen how to detect and how to replace missing values.
But the latter -until now- was rather crude: we often replaced all
values with a ``global'' average.

Clearly, we can do better than replacing all missing entries in the
\emph{survived} column with the average \(0.38\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rng }\OperatorTok{=}\NormalTok{ default\_rng()}

\NormalTok{missingRows }\OperatorTok{=}\NormalTok{ rng.integers(}\DecValTok{0}\NormalTok{,}\DecValTok{890}\NormalTok{,}\DecValTok{20}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(missingRows)}
\CommentTok{\#introduce missing values}
\NormalTok{titanic.iloc[missingRows,}\DecValTok{0}\NormalTok{] }\OperatorTok{=}\NormalTok{ np.nan}
\NormalTok{np.}\BuiltInTok{sum}\NormalTok{(titanic.survived.isna())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[864 299 857 182 808 817 802 295 255 644   1 685 452 463 303 551 517 502
 495 412]
\end{verbatim}

\begin{verbatim}
20
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predSurv }\OperatorTok{=}\NormalTok{ lsFit.predict()}
\BuiltInTok{print}\NormalTok{( }\BuiltInTok{len}\NormalTok{(predSurv))}
\NormalTok{predSurv[titanic.survived.isna()]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
891
\end{verbatim}

\begin{verbatim}
array([0.94705882, 0.13544669, 0.5       , 0.26956522, 0.94705882,
       0.94705882, 0.94705882, 0.26956522, 0.26956522, 0.13544669,
       0.5       , 0.13544669, 0.26956522, 0.5       , 0.26956522,
       0.26956522, 0.26956522, 0.26956522, 0.26956522, 0.26956522])
\end{verbatim}

\hypertarget{from-categorical-to-numerical-relations}{%
\subsubsection{From categorical to numerical
relations}\label{from-categorical-to-numerical-relations}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{url }\OperatorTok{=} \StringTok{"https://drive.google.com/file/d/1UbZy5Ecknpl1GXZBkbhJ\_K6GJcIA2Plq/view?usp=share\_link"} 
\NormalTok{url}\OperatorTok{=}\StringTok{\textquotesingle{}https://drive.google.com/uc?id=\textquotesingle{}} \OperatorTok{+}\NormalTok{ url.split(}\StringTok{\textquotesingle{}/\textquotesingle{}}\NormalTok{)[}\OperatorTok{{-}}\DecValTok{2}\NormalTok{]}
\NormalTok{auto }\OperatorTok{=}\NormalTok{ pd.read\_csv(url)}
\NormalTok{auto.head()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllllllllll@{}}
\toprule()
& mpg & cylinders & displacement & horsepower & weight & acceleration &
year & origin & name & Manufacturer \\
\midrule()
\endhead
0 & 18.0 & 8 & 307.0 & 130 & 3504 & 12.0 & 70 & 1 & chevrolet chevelle
malibu & chevrolet \\
1 & 15.0 & 8 & 350.0 & 165 & 3693 & 11.5 & 70 & 1 & buick skylark 320 &
buick \\
2 & 18.0 & 8 & 318.0 & 150 & 3436 & 11.0 & 70 & 1 & plymouth satellite &
plymouth \\
3 & 16.0 & 8 & 304.0 & 150 & 3433 & 12.0 & 70 & 1 & amc rebel sst &
amc \\
4 & 17.0 & 8 & 302.0 & 140 & 3449 & 10.5 & 70 & 1 & ford torino &
ford \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\NormalTok{plt.scatter(x}\OperatorTok{=}\NormalTok{auto[}\StringTok{"weight"}\NormalTok{], y}\OperatorTok{=}\NormalTok{auto[}\StringTok{"mpg"}\NormalTok{])}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./IntroCoding_Lecture7_files/figure-pdf/cell-20-output-1.png}

}

\end{figure}

\hypertarget{linear-regression}{%
\section{Linear Regression}\label{linear-regression}}

We can roughly estimate, i.e.~``model'' this relationship with a
straight line:

\[
y = \beta_0 + \beta_1 x
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\NormalTok{tmp}\OperatorTok{=}\NormalTok{sns.regplot(x}\OperatorTok{=}\NormalTok{auto[}\StringTok{"weight"}\NormalTok{], y}\OperatorTok{=}\NormalTok{auto[}\StringTok{"mpg"}\NormalTok{], order}\OperatorTok{=}\DecValTok{1}\NormalTok{, ci}\OperatorTok{=}\DecValTok{95}\NormalTok{, }
\NormalTok{                scatter\_kws}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}color\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}b\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}s\textquotesingle{}}\NormalTok{:}\DecValTok{9}\NormalTok{\}, line\_kws}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}color\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./IntroCoding_Lecture7_files/figure-pdf/cell-21-output-1.png}

}

\end{figure}

Remind yourself of the definition of the slope of a straight line

\[
\beta_1 = \frac{\Delta y}{\Delta x} =  \frac{y_2-y_1}{x_2-x_1}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est }\OperatorTok{=}\NormalTok{ smf.ols(}\StringTok{\textquotesingle{}mpg \textasciitilde{} weight\textquotesingle{}}\NormalTok{, auto).fit()}
\NormalTok{est.summary().tables[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllllll@{}}
\toprule()
\endhead
& coef & std err & t & P\textgreater\textbar t\textbar{} & {[}0.025 &
0.975{]} \\
Intercept & 46.2165 & 0.799 & 57.867 & 0.000 & 44.646 & 47.787 \\
weight & -0.0076 & 0.000 & -29.645 & 0.000 & -0.008 & -0.007 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.corrcoef(auto[}\StringTok{"weight"}\NormalTok{], auto[}\StringTok{"mpg"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[ 1.        , -0.83224421],
       [-0.83224421,  1.        ]])
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Further Reading:

\begin{itemize}
\tightlist
\item
  \href{}{}
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{sampling-distributions}{%
\chapter{Sampling Distributions}\label{sampling-distributions}}

\textbf{Overview}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Taking Samples

  \begin{itemize}
  \tightlist
  \item
    Variation of samples
  \item
    The \(1/\sqrt{n}\) law
  \end{itemize}
\item
  Distributions

  \begin{itemize}
  \tightlist
  \item
    densities
  \item
    ecdfs
  \end{itemize}
\item
  Parametric (``analytic'') versus nonparametric (``hacker statistics'')

  \begin{itemize}
  \tightlist
  \item
    Confidence Intervals
  \item
    Testing
  \end{itemize}
\item
  Resampling

  \begin{itemize}
  \tightlist
  \item
    Permutations
  \item
    Bootstrap
  \end{itemize}
\end{enumerate}

\textbf{Importing libraries}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{from}\NormalTok{ numpy }\ImportTok{import}\NormalTok{ random }
\NormalTok{random.seed(}\DecValTok{42}\NormalTok{) }\CommentTok{\#What is this for ???}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Importing the Birthweights Dataframe}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"https://raw.githubusercontent.com/markusloecher/DataScience{-}HWR/main/data/BirthWeights.csv"}\NormalTok{)}
\CommentTok{\#df = pd.read\_csv(\textquotesingle{}../data/BirthWeights.csv\textquotesingle{})[["sex", "dbirwt"]]}
\NormalTok{df }\OperatorTok{=}\NormalTok{ df[[}\StringTok{"sex"}\NormalTok{, }\StringTok{"dbirwt"}\NormalTok{]]}
\NormalTok{df.head()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule()
& sex & dbirwt \\
\midrule()
\endhead
0 & male & 2551 \\
1 & male & 2778 \\
2 & female & 2976 \\
3 & female & 3345 \\
4 & female & 3175 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.groupby(}\StringTok{"sex"}\NormalTok{).mean()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}ll@{}}
\toprule()
& dbirwt \\
sex & \\
\midrule()
\endhead
female & 3419.186742 \\
male & 3507.089865 \\
\bottomrule()
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{operator-overloading}{%
\section{Operator Overloading}\label{operator-overloading}}

The {[}{]} operator is overloaded. This means, that depending on the
inputs, pandas will do something completely different. Here are the
rules for the different objects you pass to just the indexing operator.

\begin{itemize}
\tightlist
\item
  string --- return a column as a Series
\item
  list of strings --- return all those columns as a DataFrame
\item
  a slice --- select rows (can do both label and integer location ---
  confusing!)
\item
  a sequence of booleans --- select all rows where True
\end{itemize}

In summary, primarily just the indexing operator selects columns, but if
you pass it a sequence of booleans it will select all rows that are
\texttt{True}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OperatorTok{=}\NormalTok{ df[(df[ }\StringTok{"dbirwt"}\NormalTok{] }\OperatorTok{\textless{}} \DecValTok{6000}\NormalTok{) }\OperatorTok{\&}\NormalTok{ (df[ }\StringTok{"dbirwt"}\NormalTok{] }\OperatorTok{\textgreater{}} \DecValTok{2000}\NormalTok{)] }\CommentTok{\# 2000 \textless{} birthweight \textless{} 6000}
\NormalTok{df}\CommentTok{\#.head()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule()
& sex & dbirwt \\
\midrule()
\endhead
0 & male & 2551 \\
1 & male & 2778 \\
2 & female & 2976 \\
3 & female & 3345 \\
4 & female & 3175 \\
... & ... & ... \\
4995 & male & 4405 \\
4996 & male & 2764 \\
4997 & female & 2776 \\
4998 & female & 3615 \\
4999 & male & 3379 \\
\bottomrule()
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Boxplot of weight vs.~sex}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.describe()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}ll@{}}
\toprule()
& dbirwt \\
\midrule()
\endhead
count & 4909.000000 \\
mean & 3480.557344 \\
std & 529.280103 \\
min & 2012.000000 \\
25\% & 3146.000000 \\
50\% & 3486.000000 \\
75\% & 3827.000000 \\
max & 5981.000000 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tmp}\OperatorTok{=}\NormalTok{df.boxplot( }\StringTok{"dbirwt"}\NormalTok{,}\StringTok{"sex"}\NormalTok{)}
\NormalTok{plt.tight\_layout()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture2_files/figure-pdf/cell-7-output-1.png}

}

\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{We notice a small difference in the average weight, which is
more clearly visible when we plot overlaying densities for male/female}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bwghtBySex }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(df[[}\StringTok{"dbirwt"}\NormalTok{,}\StringTok{"sex"}\NormalTok{]].groupby(}\StringTok{"sex"}\NormalTok{)[[}\StringTok{"dbirwt"}\NormalTok{]].mean())}

\BuiltInTok{print}\NormalTok{(bwghtBySex, }\StringTok{\textquotesingle{}}\CharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{\textquotesingle{}mean: \textquotesingle{}}\NormalTok{,bwghtBySex.mean())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        dbirwt
sex           
female  3427.0
male    3533.0 

mean:  dbirwt    3480.0
dtype: float64
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tmp}\OperatorTok{=}\NormalTok{df[[}\StringTok{"dbirwt"}\NormalTok{,}\StringTok{"sex"}\NormalTok{]].groupby(}\StringTok{"sex"}\NormalTok{)[}\StringTok{"dbirwt"}\NormalTok{].plot(kind}\OperatorTok{=}\StringTok{\textquotesingle{}density\textquotesingle{}}\NormalTok{, legend}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture2_files/figure-pdf/cell-9-output-1.png}

}

\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{ab-testing}{%
\section{A/B Testing}\label{ab-testing}}

Let us hypothesize that one wanted to classify babies into male/female
solely based on their weight. What would its accuracy be if we applied
the following simple rule:

if dbirwt \textgreater{} 3480 y = male else y = female

This would be the equivalent of testing for global warming by measuring
the temperature on \textbf{one} day. We all know that it took a long
time (= many samples) to reliably detect a small difference like 0.5
degrees buried in the noise. Let us apply the same idea here. Maybe we
can build a high-accuracy classifier if we weighed enough babies
separately for each sex.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Confusion Matrix for simple classifier}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df[}\StringTok{"predMale"}\NormalTok{] }\OperatorTok{=}\NormalTok{ (df[}\StringTok{"dbirwt"}\NormalTok{] }\OperatorTok{\textgreater{}} \DecValTok{3480}\NormalTok{)}
\NormalTok{ConfMat }\OperatorTok{=}\NormalTok{ pd.crosstab(df[}\StringTok{"predMale"}\NormalTok{], df[}\StringTok{"sex"}\NormalTok{])}
\NormalTok{ConfMat}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule()
sex & female & male \\
predMale & & \\
\midrule()
\endhead
False & 1331 & 1105 \\
True & 1100 & 1373 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{N }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(ConfMat.values)}
\NormalTok{acc1 }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{( (ConfMat.values[}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{]}\OperatorTok{+}\NormalTok{ConfMat.values[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]) }\OperatorTok{/}\NormalTok{ N, }\DecValTok{3}\NormalTok{)}

\CommentTok{\#Acc0 = (1331+1373)/5000}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Accuracy of lame classifier:"}\NormalTok{, acc1)}
\CommentTok{\#Think about the baseline accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Accuracy of lame classifier: 0.551
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{distributions}{%
\section{Distributions}\label{distributions}}

\hypertarget{mean-density-comparison-function}{%
\subsection{Mean Density Comparison
Function}\label{mean-density-comparison-function}}

\textbf{Write a function which:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  draws repeated (e.g.~M=500) random samples of size n (e.g.~40, 640)
  from each sex from the data
\item
  Computes the stdevs for the sample means of each sex separately
\item
  Repeats the above density plot for the sample mean distributions
\item
  computes the confusion matrix/accuracy of a classifier that applies
  the rule \(\bar{x} > 3480\).
\end{enumerate}

\texttt{Hint:\ np.random.choice(df{[}"dbirwt"{]},2)}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ mean\_density\_comparison(df\_cleaned, M}\OperatorTok{=}\DecValTok{500}\NormalTok{, n}\OperatorTok{=}\DecValTok{10}\NormalTok{):}
    
    \CommentTok{\#Generate a sex iteration array}
\NormalTok{    sex\_iter }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}male\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}female\textquotesingle{}}\NormalTok{]}
    
    \CommentTok{\#Create an empty DataFrame with \textquotesingle{}sex\textquotesingle{} and \textquotesingle{}dbirwt\textquotesingle{} column}
\NormalTok{    columns }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}sex\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}dbirwt\textquotesingle{}}\NormalTok{]}
\NormalTok{    df\_new }\OperatorTok{=}\NormalTok{ pd.DataFrame(columns}\OperatorTok{=}\NormalTok{columns)}
    
    \CommentTok{\#Create an empty array to store the standard deviation of the differnt sex \textquotesingle{}male\textquotesingle{} = std\_dev[0], \textquotesingle{}female\textquotesingle{} = std\_dev[1]}
\NormalTok{    std\_dev }\OperatorTok{=}\NormalTok{ np.empty(}\DecValTok{2}\NormalTok{)}
    
    \CommentTok{\#Iterate over sex and create a specific data subset}
    \ControlFlowTok{for}\NormalTok{ ind,v }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(sex\_iter):}
\NormalTok{        subset }\OperatorTok{=}\NormalTok{ df\_cleaned[df\_cleaned.sex }\OperatorTok{==}\NormalTok{ v]}
        
        \CommentTok{\#create M random sample means of n samples and add it to df\_new}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(M):}
\NormalTok{            rand\_samples }\OperatorTok{=}\NormalTok{ np.random.choice(subset.dbirwt, n)}
\NormalTok{            x }\OperatorTok{=}\NormalTok{ np.mean(rand\_samples)}\CommentTok{\#sample mean per sex}
\NormalTok{            df\_new.loc[}\BuiltInTok{len}\NormalTok{(df\_new)}\OperatorTok{+}\DecValTok{1}\NormalTok{] }\OperatorTok{=}\NormalTok{ [v, x]}
        
        \CommentTok{\#plot male and female data and calculate the standard deviation of the data}
\NormalTok{        plot\_data }\OperatorTok{=}\NormalTok{ df\_new[df\_new.sex }\OperatorTok{==}\NormalTok{ v]}
\NormalTok{        std\_dev[ind] }\OperatorTok{=}\NormalTok{ np.std(plot\_data[}\StringTok{\textquotesingle{}dbirwt\textquotesingle{}}\NormalTok{])  }
        
\NormalTok{        plot\_data.dbirwt.plot.density()}
\NormalTok{        plt.xlabel(}\StringTok{\textquotesingle{}dbirwt\textquotesingle{}}\NormalTok{)}
\NormalTok{        plt.legend(sex\_iter)}
        \CommentTok{\#plt.grid()}
        \CommentTok{\#plt.title("n=" + str(n))}
        
    \CommentTok{\#return the sample mean data}
    \ControlFlowTok{return}\NormalTok{ df\_new}
 
        
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Testing the Function}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SM10 }\OperatorTok{=}\NormalTok{ mean\_density\_comparison(df, M}\OperatorTok{=}\DecValTok{500}\NormalTok{, n}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{SM640 }\OperatorTok{=}\NormalTok{ mean\_density\_comparison(df, M}\OperatorTok{=}\DecValTok{500}\NormalTok{, n}\OperatorTok{=}\DecValTok{640}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture2_files/figure-pdf/cell-13-output-1.png}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SM10[}\StringTok{"predMale"}\NormalTok{] }\OperatorTok{=}\NormalTok{ (SM10[}\StringTok{"dbirwt"}\NormalTok{] }\OperatorTok{\textgreater{}} \DecValTok{3480}\NormalTok{)}
\NormalTok{ConfMat10 }\OperatorTok{=}\NormalTok{ pd.crosstab(SM10[}\StringTok{"predMale"}\NormalTok{], SM10[}\StringTok{"sex"}\NormalTok{])}
\NormalTok{ConfMat10}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule()
sex & female & male \\
predMale & & \\
\midrule()
\endhead
False & 320 & 182 \\
True & 180 & 318 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SM640[}\StringTok{"predMale"}\NormalTok{] }\OperatorTok{=}\NormalTok{ (SM640[}\StringTok{"dbirwt"}\NormalTok{] }\OperatorTok{\textgreater{}} \DecValTok{3480}\NormalTok{)}
\NormalTok{ConfMat640 }\OperatorTok{=}\NormalTok{ pd.crosstab(SM640[}\StringTok{"predMale"}\NormalTok{], SM640[}\StringTok{"sex"}\NormalTok{])}
\NormalTok{ConfMat640}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule()
sex & female & male \\
predMale & & \\
\midrule()
\endhead
False & 498 & 0 \\
True & 2 & 500 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grouped640 }\OperatorTok{=}\NormalTok{ SM640[}\StringTok{"dbirwt"}\NormalTok{].groupby(SM640[}\StringTok{"sex"}\NormalTok{])}

\BuiltInTok{print}\NormalTok{(}\StringTok{"n=640, means:"}\NormalTok{, grouped640.mean())}
\BuiltInTok{print}\NormalTok{()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"n=640, SESMs:"}\NormalTok{, grouped640.std())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
n=640, means: sex
female    3427.276397
male      3533.081803
Name: dbirwt, dtype: float64

n=640, SESMs: sex
female    20.937106
male      19.819436
Name: dbirwt, dtype: float64
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SM40 }\OperatorTok{=}\NormalTok{ mean\_density\_comparison(df, M}\OperatorTok{=}\DecValTok{500}\NormalTok{, n}\OperatorTok{=}\DecValTok{40}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture2_files/figure-pdf/cell-17-output-1.png}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grouped40 }\OperatorTok{=}\NormalTok{ SM40[}\StringTok{"dbirwt"}\NormalTok{].groupby(SM40[}\StringTok{"sex"}\NormalTok{])}

\BuiltInTok{print}\NormalTok{(}\StringTok{"n=40, means:"}\NormalTok{, grouped40.mean())}
\BuiltInTok{print}\NormalTok{()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"n=40, SESMs:"}\NormalTok{, grouped40.std())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
n=40, means: sex
female    3423.30740
male      3527.34015
Name: dbirwt, dtype: float64

n=40, SESMs: sex
female    82.787145
male      84.921927
Name: dbirwt, dtype: float64
\end{verbatim}

How much smaller is \(\sigma_{\bar{x},640}\) than
\(\sigma_{\bar{x},40}\) ? Compare that factor to the ratio of the sample
sizes \(640/40 = 16\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{empirical-cumulative-distribution-function}{%
\subsection{Empirical Cumulative Distribution
Function}\label{empirical-cumulative-distribution-function}}

The density -like a histogram- has a few complications that include the
arbitrary choice of bin width (kernel width for density) and the loss of
information. Welcome to the \emph{empirical cumulative distribution
function} \textbf{ecdf}

\textbf{ECDF Function}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ ecdf(data):}
    \CommentTok{"""Compute ECDF for a one{-}dimensional array of measurements."""}

    \CommentTok{\# Number of data points: n}
\NormalTok{    n }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(data)}

    \CommentTok{\# x{-}data for the ECDF: x}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ np.sort(data)}

    \CommentTok{\# y{-}data for the ECDF: y}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ np.arange(}\DecValTok{1}\NormalTok{, n}\OperatorTok{+}\DecValTok{1}\NormalTok{) }\OperatorTok{/}\NormalTok{ n}

    \ControlFlowTok{return}\NormalTok{ x, y}
\end{Highlighting}
\end{Shaded}

\textbf{ECDF Plot}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute ECDF for sample size 40: m\_40, f\_40}
\NormalTok{male40 }\OperatorTok{=}\NormalTok{ SM40[SM40.sex }\OperatorTok{==} \StringTok{"male"}\NormalTok{][}\StringTok{"dbirwt"}\NormalTok{]}
\NormalTok{female40 }\OperatorTok{=}\NormalTok{ SM40[SM40.sex }\OperatorTok{==} \StringTok{"female"}\NormalTok{][}\StringTok{"dbirwt"}\NormalTok{]}

\NormalTok{mx\_40, my\_40 }\OperatorTok{=}\NormalTok{ ecdf(male40)}
\NormalTok{fx\_40, fy\_40 }\OperatorTok{=}\NormalTok{ ecdf(female40)}

\CommentTok{\# Plot all ECDFs on the same plot}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ ax.plot(mx\_40, my\_40, marker }\OperatorTok{=} \StringTok{\textquotesingle{}.\textquotesingle{}}\NormalTok{, linestyle }\OperatorTok{=} \StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ ax.plot(fx\_40, fy\_40, marker }\OperatorTok{=} \StringTok{\textquotesingle{}.\textquotesingle{}}\NormalTok{, linestyle }\OperatorTok{=} \StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Make nice margins}
\NormalTok{plt.margins(}\FloatTok{0.02}\NormalTok{)}

\CommentTok{\# Annotate the plot}
\NormalTok{plt.legend((}\StringTok{\textquotesingle{}male\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}female\textquotesingle{}}\NormalTok{), loc}\OperatorTok{=}\StringTok{\textquotesingle{}lower right\textquotesingle{}}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.xlabel(}\StringTok{\textquotesingle{}birth weight(g)\textquotesingle{}}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.ylabel(}\StringTok{\textquotesingle{}ECDF\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Display the plot}
\NormalTok{plt.grid()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture2_files/figure-pdf/cell-20-output-1.png}

}

\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{itemize}
\tightlist
\item
  What is the relationship to quantiles/percentiles ?
\item
  Find the IQR !
\item
  Sketch the densities just from the ecdf.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{checking-normality-of-sample-mean-distribution}{%
\subsection{Checking Normality of sample mean
distribution}\label{checking-normality-of-sample-mean-distribution}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute mean and standard deviation: mu, sigma}
\NormalTok{mu }\OperatorTok{=}\NormalTok{ np.mean(male40)}
\NormalTok{sigma }\OperatorTok{=}\NormalTok{ np.std(male40)}

\CommentTok{\# Sample out of a normal distribution with this mu and sigma: samples}
\NormalTok{samples }\OperatorTok{=}\NormalTok{ np.random.normal(mu, sigma, }\DecValTok{10000}\NormalTok{)}

\CommentTok{\# Get the CDF of the samples and of the data}
\NormalTok{x\_theor, y\_theor }\OperatorTok{=}\NormalTok{ ecdf(samples)}

\CommentTok{\# Plot the CDFs and show the plot}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.plot(mx\_40, my\_40, marker}\OperatorTok{=}\StringTok{\textquotesingle{}.\textquotesingle{}}\NormalTok{, linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.plot(x\_theor, y\_theor)}

\NormalTok{plt.margins(}\FloatTok{0.02}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.xlabel(}\StringTok{\textquotesingle{}birth weight (g)\textquotesingle{}}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.ylabel(}\StringTok{\textquotesingle{}CDF\textquotesingle{}}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.title(}\StringTok{\textquotesingle{}CDF of Birthweight\textquotesingle{}}\NormalTok{)}

\NormalTok{plt.grid()}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture2_files/figure-pdf/cell-21-output-1.png}

}

\end{figure}

\textbf{Tasks}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find the ``5\% tails'' which are just the (0.05, 0.95) quantiles
\item
  Read up on theoretical quantiles:
  https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html\#scipy.stats.norm
\item
  stone age: get the ``5\% tails'' from a normal table.
\item
  How many stdevs do you need to cover the 90\% sample interval ?
\item
  Can you replace the ``empirical theoretical cdf'' from above with the
  exact line without sampling 10000 random numbers from a normal
  distribution ?
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Let us recap what we observed when sampling from a ``population'': The
\emph{sample mean distribution} gets narrower with increasing sample
size n, SESM =\(\sigma_{\bar{x}} = \sigma/\sqrt{n}\). Take a look at
this
\href{http://onlinestatbook.com/stat_sim/sampling_dist/}{interactive
applet} for further understanding.

How is this useful ? And how is it relevant because in reality we would
only have \textbf{one sample}, not hundreds !

\textbf{Small Tasks}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose one random sample of size n=40 from the male babies and compute
  \(\bar{x}\), \(\hat{\sigma}\). Assume all that is known to you, are
  these two \emph{summary statistics}. In particular, we do \textbf{not
  know} the true mean \(\mu\)!
\item
  Argue intuitively with the ecdf plot about plausible values of
  \(\mu\).
\item
  More precisely: what interval around \(\bar{x}\) would contain \(\mu\)
  with 90\% probability ?
\end{enumerate}

\hypertarget{hacker-statistic}{%
\section{Hacker Statistic}\label{hacker-statistic}}

The ability to draw new samples from a population with a known mean is a
luxury that we usually do not have. Is there any way to ``fake'' new
samples using just the one ``lousy'' sample we have at hand ? This might
sound like an impossible feat analogously to ``pulling yourself up by
your own \textbf{bootstraps}''!

\href{https://github.com/markusloecher/DataScience-HWR/blob/main/figures/DC_bootstrap_animated.gif}{Bootstrap
Ilustration}

But that is exactly what we will try now:

\textbf{Tasks}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Look up the help for
  \href{https://docs.scipy.org/doc/numpy15.0/reference/generated/numpy.random.choice.html}{np.random.choice()}
\item
  Draw repeated samples of size n=40 from the sample above.
\item
  Compute the mean of each sample and store it an array.
\item
  Plot the histogram
\item
  Compute the stdev of this distribution and compare to the SEM.
\item
  Write a function that computes \emph{bootstrap replicates} of the mean
  from a sample.
\item
  Generalize this function to accept any summary statistic, not just the
  mean.
\end{enumerate}

\bookmarksetup{startatroot}

\hypertarget{hypothesis-tests}{%
\chapter{Hypothesis Tests}\label{hypothesis-tests}}

\textbf{Overview}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Hypothesis Tests

  \begin{itemize}
  \tightlist
  \item
    1-sample
  \item
    2-sample
  \end{itemize}
\item
  Permutation Tests
\end{enumerate}

\hypertarget{warmupreview-exercises}{%
\section{WarmUp/Review Exercises}\label{warmupreview-exercises}}

Load the \emph{Auto.csv} data (into a dataframe \emph{cars}) for the
following tasks and create a scatter plot
\texttt{mpg\ \textasciitilde{}\ weight}. Take a look at row with index
\(25\) (i.e.~row 26); we will refer to that data point as \(x_{25}\)
from now on.

\begin{itemize}
\tightlist
\item
  Compute the standard score for

  \begin{itemize}
  \tightlist
  \item
    \(x_{25}\){[}``mpg''{]}
  \item
    \(x_{25}\){[}``weight''{]}
  \end{itemize}
\item
  Compute the product of these standard scores (call it \(p_{25}\)).
\item
  If you repeated this process for all rows of the \emph{cars} dataframe
  and averaged all products \(p_{i}\), what would the resulting number
  tell you ? (What is it called?)
\item
  Take a bootstrap sample from cars.mpg and compute the mean
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Importing Standard Libraries}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\CommentTok{\#pd.options.mode.chained\_assignment = None \# disable chained assignment warning}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\end{Highlighting}
\end{Shaded}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Loading our Functions}

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{\%}\NormalTok{run .}\OperatorTok{/}\NormalTok{ourFunctions.py}
\OperatorTok{\%}\NormalTok{precision }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'%.3f'
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Read in the Cars DF}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#cars = pd.read\_csv(\textquotesingle{}../data/Auto.csv\textquotesingle{})}
\NormalTok{cars }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"https://raw.githubusercontent.com/markusloecher/DataScience{-}HWR/main/data/Auto.csv"}\NormalTok{)}
\CommentTok{\#d}
\NormalTok{cars.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(392, 10)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars.head()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllllllllll@{}}
\toprule()
& mpg & cylinders & displacement & horsepower & weight & acceleration &
year & origin & name & Manufacturer \\
\midrule()
\endhead
0 & 18.0 & 8 & 307.0 & 130 & 3504 & 12.0 & 70 & 1 & chevrolet chevelle
malibu & chevrolet \\
1 & 15.0 & 8 & 350.0 & 165 & 3693 & 11.5 & 70 & 1 & buick skylark 320 &
buick \\
2 & 18.0 & 8 & 318.0 & 150 & 3436 & 11.0 & 70 & 1 & plymouth satellite &
plymouth \\
3 & 16.0 & 8 & 304.0 & 150 & 3433 & 12.0 & 70 & 1 & amc rebel sst &
amc \\
4 & 17.0 & 8 & 302.0 & 140 & 3449 & 10.5 & 70 & 1 & ford torino &
ford \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(cars.iloc[}\DecValTok{25}\NormalTok{,])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
mpg                  10.0
cylinders               8
displacement        360.0
horsepower            215
weight               4615
acceleration         14.0
year                   70
origin                  1
name            ford f250
Manufacturer         ford
Name: 25, dtype: object
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.scatter(}\StringTok{"weight"}\NormalTok{, }\StringTok{"mpg"}\NormalTok{,data}\OperatorTok{=}\NormalTok{cars)}
\NormalTok{plt.xlabel(}\StringTok{"weight"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"mpg"}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture3_files/figure-pdf/cell-7-output-1.png}

}

\end{figure}

\hypertarget{hypothesis-tests-1}{%
\section{Hypothesis Tests}\label{hypothesis-tests-1}}

We have learned about the \textbf{bootstrap} as a slick way of
resampling your data to obtain sampling distributions of various
measures of interest, Without having to learn highly specific
distributions (such as the \(\chi^2\), Poisson, Binomial or
F-distribution) the bootstrap enables us to

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  get \textbf{confidence intervals}
\item
  perform \textbf{one-sample tests}
\item
  perform \textbf{two-sample tests}
\end{enumerate}

Imagine the EPA requiring the average mpg for 4-cylinder cars to be at
least \(\mu_0 = 30\) and needs to decide -based on this sample only-
whether the manufacturers need to implement some improvements. In
statistical jargon: can the EPA \textbf{reject the claim} that the
\textbf{true mean} is at least 30 ?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cars4}\OperatorTok{=}\NormalTok{cars[cars[}\StringTok{"cylinders"}\NormalTok{]}\OperatorTok{==}\DecValTok{4}\NormalTok{]}
\NormalTok{empirical\_mean }\OperatorTok{=}\NormalTok{ np.mean(cars4.mpg)}
\NormalTok{empirical\_mean}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
29.28391959798995
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{empirical\_sd }\OperatorTok{=}\NormalTok{ np.std(cars4.mpg)}
\NormalTok{empirical\_sd}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
5.656280603443601
\end{verbatim}

Compute a confidence interval of the true mpg of 4-cyl cars via the
bootstrap ! (Is there just ``THE CI'' ??)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mpg\_bs }\OperatorTok{=}\NormalTok{ draw\_bs\_reps(cars4.mpg, func }\OperatorTok{=}\NormalTok{ np.mean, size}\OperatorTok{=}\DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.percentile(mpg\_bs, [}\FloatTok{2.5}\NormalTok{, }\FloatTok{97.5}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([28.498, 30.093])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm}
\NormalTok{norm.ppf(}\FloatTok{0.975}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
1.959963984540054
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{There, we have it, the empirical mean is in fact below 30.} Is
this sufficient evidence for the EPA to demand improvements ? Does
\(\bar{x} < \mu_0\) ``prove'' that \(\mu < \mu_0\) ??

Think about an experiment which tries to establish whether a pair of
dice in a casino is biased (``gezinkt'' in German). You toss the dice
\(50\) times and observe an average number of pips of \(\bar{x} = 3.8\)
which is clearly ``way above'' the required value of \(\mu_0 = 3.5\) for
a clean pair of dice. Should you go to the authorities and file a
complaint ? Since no one really knows the true value of \(\mu\) for
these dice, how would a court decide whether they are unbiased ?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Innocent until proven guilty}: Always assume the opposite of
  what you want to illustrate ! That way you can gather evidence against
  this \textbf{Null hypothesis}, e.g.~\(H_0: \mu = 3.5 (\equiv \mu_0)\).
\item
  Define a \textbf{test statistic} which is pertinent to the question
  you are seeking to answer (computed entirely from your sample). In
  this case, your test statistics could be e.g.~the (scaled?) difference
  between the observed sample mean and the claim: \(\bar{x} - \mu_0\).
\item
  We define the \textbf{p-value} as the probability of observing a test
  statistic equally or more extreme than the one observed, given that
  \(H_0\) is true.
\item
  If the observed value of your test statistic seems very unlikely under
  \(H_0\) we would favor the \textbf{alternative hypothesis} which is
  usually complementary, e.g.~\(H_A: \mu \neq 3.5\)
\end{enumerate}

Notice that we used vague expressions such as \emph{unlikely} in the
procedure above. In a legal context, we would want to make statements
such as ``beyond a reasonable doubt''. Well, in statistics we ry to
quantifiy that notion by limiting the type-I error probability
\(\alpha\) to be at most e.g.~\(0.01\) or \(0.05\). So none our
decisions are ever ``proofs'' or free of error.

It also means that you, the person conducting the hypothesis test, need
to specify a value of \(\alpha\), which clearly introduces a certain
amount of subjectivity. Later on, we will discuss the inherent tradeoffs
between type-I and type-II errors: only then will one fully understand
the non-arbitrary choice of \(\alpha\).

\hypertarget{parametric-tests}{%
\subsection{Parametric Tests}\label{parametric-tests}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  From the sample, compute \(\bar{x}\) and \(\hat{\sigma}\).
\item
  Compute the test statistic
\end{enumerate}

\[t = \frac{\bar{x} - \mu_0}{\hat{\sigma}/\sqrt{n}}\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Reject \(H_0\)

  \begin{itemize}
  \tightlist
  \item
    Two-sided test: if \(|t| > t_{n-1, 1-\alpha/2}\)
  \item
    Left-sided test: if \(t < t_{n-1, \alpha}\)
  \item
    Right-sided test: if \(t > t_{n-1, 1-\alpha}\)
  \item
    If \(n>50\) one may replace the t distribution with the normal
  \end{itemize}
\item
  Define the p-value as twice the tail area of a t distribution
\item
  Alternatively one rejects the Null if p-val \(< \alpha\).
\end{enumerate}

Back to the casino example, assume the following empirical data:

\(\bar{x} = 3.8, n = 100, \hat{\sigma} = 1.7\)

\(H_0: \mu = 3.5\), \(H_A: \mu \neq 3.5\), \(\alpha = 0.01\)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm}
\CommentTok{\# that is the critical value that we compare our test statistic with}
\BuiltInTok{print}\NormalTok{(}\StringTok{\textquotesingle{}two{-}sided cv: \textquotesingle{}}\NormalTok{,norm.ppf(}\DecValTok{1}\OperatorTok{{-}}\FloatTok{0.01}\OperatorTok{/}\DecValTok{2}\NormalTok{)) }\CommentTok{\# returns the norm score}
\BuiltInTok{print}\NormalTok{(}\StringTok{\textquotesingle{}one{-}sided cv: \textquotesingle{}}\NormalTok{,norm.ppf(}\DecValTok{1}\OperatorTok{{-}}\FloatTok{0.01}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
two-sided cv:  2.5758293035489004
one-sided cv:  2.3263478740408408
\end{verbatim}

\hypertarget{two-sided-test}{%
\subsubsection{Two sided test}\label{two-sided-test}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We care about both sides since we do not know how the casino is using the biased dice.}
\NormalTok{a}\OperatorTok{=}\FloatTok{0.01}
\NormalTok{n}\OperatorTok{=}\DecValTok{200}

\NormalTok{critValue }\OperatorTok{=}\NormalTok{ norm.ppf(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{a}\OperatorTok{/}\DecValTok{2}\NormalTok{)}
\NormalTok{z }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{abs}\NormalTok{((}\FloatTok{3.8}\OperatorTok{{-}}\FloatTok{3.5}\NormalTok{)}\OperatorTok{/}\NormalTok{(}\FloatTok{1.7}\OperatorTok{/}\NormalTok{np.sqrt(n))) }

\ControlFlowTok{if}\NormalTok{ (z }\OperatorTok{\textless{}}\NormalTok{ critValue):}
\NormalTok{    s }\OperatorTok{=} \StringTok{"not"}
\NormalTok{    s2 }\OperatorTok{=} \StringTok{"fail to"}
\ControlFlowTok{else}\NormalTok{:}
\NormalTok{    s }\OperatorTok{=} \StringTok{""}   
\NormalTok{    s2 }\OperatorTok{=} \StringTok{""}
    
\BuiltInTok{print}\NormalTok{(}\StringTok{"The test statistic t ="}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{(z,}\DecValTok{3}\NormalTok{), }\StringTok{"is"}\NormalTok{, s ,}\StringTok{"larger than the critical value"}\NormalTok{,np.}\BuiltInTok{round}\NormalTok{(critValue,}\DecValTok{3}\NormalTok{), }\StringTok{"so we"}\NormalTok{,s2,}\StringTok{"reject the Null"}\NormalTok{ ) }
\NormalTok{norm.cdf(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{a}\OperatorTok{/}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The test statistic t = 2.496 is not larger than the critical value 2.576 so we fail to reject the Null
\end{verbatim}

\begin{verbatim}
0.840131867824506
\end{verbatim}

\hypertarget{one-sided-test}{%
\subsubsection{One sided test!}\label{one-sided-test}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We care only about one side since we do know that the casino makes }
\CommentTok{\# money off upward biased dice.}

\NormalTok{critValue }\OperatorTok{=}\NormalTok{ norm.ppf(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{a)}
\NormalTok{z }\OperatorTok{=}\NormalTok{ (}\FloatTok{3.8}\OperatorTok{{-}}\FloatTok{3.5}\NormalTok{)}\OperatorTok{/}\NormalTok{(}\FloatTok{1.7}\OperatorTok{/}\NormalTok{np.sqrt(n))}
\ControlFlowTok{if}\NormalTok{ (z }\OperatorTok{\textless{}}\NormalTok{ critValue):}
\NormalTok{    s }\OperatorTok{=} \StringTok{"not"}
\NormalTok{    s2 }\OperatorTok{=} \StringTok{"fail to"}
\ControlFlowTok{else}\NormalTok{:}
\NormalTok{    s }\OperatorTok{=} \StringTok{""}   
\NormalTok{    s2 }\OperatorTok{=} \StringTok{""}
    
\BuiltInTok{print}\NormalTok{(}\StringTok{"The test statistic t ="}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{(z,}\DecValTok{3}\NormalTok{), }\StringTok{"is"}\NormalTok{, s ,}\StringTok{"larger than the critical value"}\NormalTok{,np.}\BuiltInTok{round}\NormalTok{(critValue,}\DecValTok{3}\NormalTok{), }\StringTok{"so we"}\NormalTok{,s2,}\StringTok{"reject the Null"}\NormalTok{ ) }
\CommentTok{\#norm.cdf(1{-}a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The test statistic t = 2.496 is  larger than the critical value 2.326 so we  reject the Null
\end{verbatim}

\hypertarget{p-value}{%
\subsubsection{p-value}\label{p-value}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#p{-}value is the right tail probability of values equal or larger than your test statistic}
\NormalTok{pVal }\OperatorTok{=} \DecValTok{1}\OperatorTok{{-}}\NormalTok{norm.cdf(z)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"The p value "}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{(pVal,}\DecValTok{3}\NormalTok{), }\StringTok{"is less than alpha"}\NormalTok{, a, }\StringTok{"so we reject the Null"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The p value  0.006 is less than alpha 0.01 so we reject the Null
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{non-parametric-tests}{%
\subsection{Non parametric Tests}\label{non-parametric-tests}}

Parametric Tests require many assumptions, that may not always be true,
and are also a bit abstract. It often helps (i) robustness and (ii) the
understanding of the testing process to use simulations instead. We now
learn about two different ways of such simulations, each following the
basic ``pipeline'':

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Clearly define the Null hypothesis.
\item
  Define the test statistic.
\item
  Generate many sets of simulated data (bootstrap replicates)
  \textbf{assuming the null hypothesis is true}
\item
  Compute the test statistic for each simulated data set
\item
  The p-value is the fraction of your simulated data sets for which the
  test statistic is at least as extreme as for the real data
\end{enumerate}

\hypertarget{bootstrap-hypothesis-tests}{%
\subsubsection{Bootstrap Hypothesis
Tests}\label{bootstrap-hypothesis-tests}}

Besides computing confidence intervals we can also use the bootstrap to
perform hypothesis test. We need to ``fake'' the process of drawing new
samples again and again under the Null hypothesis, which might appear
impossible since our one sample likely will have a sample mean not equal
to \(\mu_0\).

The trick is to ``shift'' our data such that \(\bar{x} = \mu_0\)! As a
test statistic we use the difference of the mean of the bootstrap value
minus \(\mu_0\). Notice that there is no need to scale this difference!

We illustrate this with our casino example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.random.choice(}\DecValTok{6}\NormalTok{,}\DecValTok{20}\NormalTok{)}\OperatorTok{+}\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([5, 6, 4, 5, 2, 2, 3, 2, 5, 6, 4, 1, 3, 4, 5, 5, 6, 5, 5, 4])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.random.seed(}\DecValTok{123}\NormalTok{)}
\NormalTok{n}\OperatorTok{=}\DecValTok{400}
\CommentTok{\#x = np.random.choice(6,n,p=[0.14,0.14,0.15,0.18,0.19,0.20])+1 \#add a bias}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.random.choice(}\DecValTok{6}\NormalTok{,n)}\OperatorTok{+}\DecValTok{1} 
\CommentTok{\#cheating:}
\NormalTok{x }\OperatorTok{=}\NormalTok{ x }\OperatorTok{+} \FloatTok{0.2} \CommentTok{\#add a bias}
\NormalTok{a}\OperatorTok{=}\FloatTok{0.01}

\NormalTok{mu0}\OperatorTok{=}\FloatTok{3.5}
\NormalTok{xBar }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(np.mean(x),}\DecValTok{2}\NormalTok{)}
\NormalTok{obsDiff }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{round}\NormalTok{(mu0}\OperatorTok{{-}}\NormalTok{xBar,}\DecValTok{2}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"The sample mean is"}\NormalTok{, xBar, }\StringTok{", so we shift our data by"}\NormalTok{, obsDiff)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The sample mean is 3.65 , so we shift our data by -0.15
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Alternatively, if we had not shifted our data}
\CommentTok{\#TS = bs\_mean\_dice{-}xBar}

\CommentTok{\#The test statistic}
\NormalTok{bs\_mean\_dice }\OperatorTok{=}\NormalTok{ draw\_bs\_reps(x}\OperatorTok{+}\NormalTok{obsDiff,np.mean,}\DecValTok{10000}\NormalTok{)}
\NormalTok{TS }\OperatorTok{=}\NormalTok{ bs\_mean\_dice}\OperatorTok{{-}}\NormalTok{mu0}
\NormalTok{plt.hist(TS)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture3_files/figure-pdf/cell-19-output-1.png}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#two sided p value}
\NormalTok{pVal }\OperatorTok{=}\NormalTok{ np.mean( }\BuiltInTok{abs}\NormalTok{(TS) }\OperatorTok{\textgreater{}} \BuiltInTok{abs}\NormalTok{(obsDiff))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"The two sided p value of"}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{(pVal,}\DecValTok{3}\NormalTok{), }\StringTok{"is not smaller than alpha="}\NormalTok{,a,}\StringTok{"so we fail to reject the Null."}\NormalTok{ ) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The two sided p value of 0.012 is not smaller than alpha= 0.01 so we fail to reject the Null.
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{tasks-4}{%
\subsubsection{Tasks}\label{tasks-4}}

\begin{itemize}
\tightlist
\item
  Test \(H_0: \mu \geq 30, H_A: \mu < 30\) for the mean mpg of
  4-cylinder cars

  \begin{itemize}
  \tightlist
  \item
    using bootstrap replicates
  \item
    via standard testing theory.
  \end{itemize}
\item
  Compute the corresponding \textbf{p-values}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu0}\OperatorTok{=}\DecValTok{30}
\NormalTok{empirical\_mean }\OperatorTok{=}\NormalTok{ np.mean(cars4.mpg)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#assume the claim is true !!}
\NormalTok{shift }\OperatorTok{=}\NormalTok{ mu0}\OperatorTok{{-}}\NormalTok{empirical\_mean}
\NormalTok{cars4Shifted}\OperatorTok{=}\NormalTok{ cars4.mpg}\OperatorTok{+}\NormalTok{shift}
\NormalTok{bs\_mean\_mpg }\OperatorTok{=}\NormalTok{ draw\_bs\_reps(cars4Shifted,np.mean,}\DecValTok{10000}\NormalTok{)}

\CommentTok{\#The test statistic}
\NormalTok{TS }\OperatorTok{=}\NormalTok{ bs\_mean\_mpg}\OperatorTok{{-}}\NormalTok{mu0}
\NormalTok{pVal }\OperatorTok{=}\NormalTok{ np.mean( TS }\OperatorTok{\textless{}} \OperatorTok{{-}}\NormalTok{shift)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#assume the claim is true !!}
\NormalTok{shift }\OperatorTok{=}\NormalTok{ mu0}\OperatorTok{{-}}\NormalTok{empirical\_mean}
\NormalTok{cars4Shifted}\OperatorTok{=}\NormalTok{ cars4.mpg}\OperatorTok{+}\NormalTok{shift}
\NormalTok{bs\_mean\_mpg }\OperatorTok{=}\NormalTok{ draw\_bs\_reps(cars4Shifted,np.mean,}\DecValTok{10000}\NormalTok{)}

\CommentTok{\#The test statistic}
\NormalTok{TS }\OperatorTok{=}\NormalTok{ bs\_mean\_mpg}\OperatorTok{{-}}\NormalTok{mu0}
\NormalTok{pVal }\OperatorTok{=}\NormalTok{ np.mean( TS }\OperatorTok{\textless{}} \OperatorTok{{-}}\NormalTok{shift)}

\NormalTok{plt.hist(TS)}\OperatorTok{;}

\CommentTok{\#np.percentile(bs\_mean\_mpg,5)}
\CommentTok{\#p value is simply the left tail beyond xBar}
\CommentTok{\#np.mean(bs\_mean\_mpg \textless{} empirical\_mean)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture3_files/figure-pdf/cell-23-output-1.png}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{\textquotesingle{}shift =\textquotesingle{}}\NormalTok{,shift)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"pValue ="}\NormalTok{, pVal)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
shift = 0.7160804020100429
pValue = 0.0375
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{from-one-sample-to-2-samples}{%
\subsection{From one sample to 2
samples}\label{from-one-sample-to-2-samples}}

From Auto to birth weights

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{preg }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}data/pregNSFG.csv.gz\textquotesingle{}}\NormalTok{, compression}\OperatorTok{=}\StringTok{\textquotesingle{}gzip\textquotesingle{}}\NormalTok{)}

\CommentTok{\#only look at live births}
\NormalTok{firsts }\OperatorTok{=}\NormalTok{ preg[(preg.outcome }\OperatorTok{==} \DecValTok{1}\NormalTok{) }\OperatorTok{\&}\NormalTok{ (preg.birthord }\OperatorTok{==} \DecValTok{1}\NormalTok{)]}
\CommentTok{\#live[live.babysex == 1].babysex = "male"}

\CommentTok{\#we reduce the sample size further by conditioning on }
\CommentTok{\#the mother\textquotesingle{}s age at the end of pregnancy}
\NormalTok{firsts }\OperatorTok{=}\NormalTok{ firsts[(firsts.agepreg }\OperatorTok{\textless{}} \DecValTok{30}\NormalTok{) }\OperatorTok{\&}\NormalTok{ (firsts.prglngth }\OperatorTok{\textgreater{}=} \DecValTok{30}\NormalTok{)]}
\NormalTok{bwt }\OperatorTok{=}\NormalTok{ firsts[[}\StringTok{"babysex"}\NormalTok{,}\StringTok{"totalwgt\_lb"}\NormalTok{]] }

\NormalTok{bwt.babysex.replace([}\FloatTok{1.0}\NormalTok{],}\StringTok{"male"}\NormalTok{,inplace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{bwt.babysex.replace([}\FloatTok{2.0}\NormalTok{],}\StringTok{"female"}\NormalTok{,inplace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{bwt }\OperatorTok{=}\NormalTok{ bwt.dropna()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{\textquotesingle{}shape:\textquotesingle{}}\NormalTok{,bwt.shape)}
\NormalTok{bwt.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
shape: (3758, 2)
\end{verbatim}

\begin{longtable}[]{@{}lll@{}}
\toprule()
& babysex & totalwgt\_lb \\
\midrule()
\endhead
2 & male & 9.1250 \\
5 & male & 8.5625 \\
8 & male & 7.5625 \\
10 & male & 7.8125 \\
11 & female & 7.0000 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grouped }\OperatorTok{=}\NormalTok{ bwt[}\StringTok{"totalwgt\_lb"}\NormalTok{].groupby(bwt[}\StringTok{"babysex"}\NormalTok{])}
\NormalTok{grouped.mean()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
babysex
female    7.103830
male      7.378682
Name: totalwgt_lb, dtype: float64
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tmp}\OperatorTok{=}\NormalTok{grouped.plot(kind}\OperatorTok{=}\StringTok{\textquotesingle{}density\textquotesingle{}}\NormalTok{, legend}\OperatorTok{=}\VariableTok{True}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture3_files/figure-pdf/cell-28-output-1.png}

}

\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{a-two-sample-bootstrap-hypothesis-test-for-difference-of-means}{%
\subsubsection{A two-sample bootstrap hypothesis test for difference of
means}\label{a-two-sample-bootstrap-hypothesis-test-for-difference-of-means}}

A one sample test compares a data set to one fixed number !

We now want to compare two sets of data, both of which are samples! In
particular test the hypothesis that male and female babies have the same
biological weight (but not necessarily the same distribution).

\(H_0: \mu_m = \mu_f, H_A: \mu_m \neq \mu_f\)

To do the two-sample bootstrap test, we shift both arrays to have the
same mean, since we are simulating the hypothesis that their means are,
in fact, equal (\textbf{equal to what value ??}). We then draw bootstrap
samples out of the shifted arrays and compute the difference in means.
This constitutes a bootstrap replicate, and we generate many of them.
The p-value is the fraction of replicates with a difference in means
greater than or equal to what was observed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{meanNull }\OperatorTok{=}\NormalTok{ np.mean(bwt.totalwgt\_lb)}\CommentTok{\# pooled mean}
\NormalTok{w\_m }\OperatorTok{=}\NormalTok{ bwt[bwt[}\StringTok{"babysex"}\NormalTok{]}\OperatorTok{==}\StringTok{"male"}\NormalTok{].totalwgt\_lb}
\NormalTok{w\_f }\OperatorTok{=}\NormalTok{ bwt[bwt[}\StringTok{"babysex"}\NormalTok{]}\OperatorTok{==}\StringTok{"female"}\NormalTok{].totalwgt\_lb}
\NormalTok{empirical\_diff\_means }\OperatorTok{=}\NormalTok{ np.mean(w\_m)}\OperatorTok{{-}}\NormalTok{np.mean(w\_f) }
\CommentTok{\#shift:}

\NormalTok{w\_m\_shifted }\OperatorTok{=}\NormalTok{ w\_m }\OperatorTok{{-}}\NormalTok{ np.mean(w\_m) }\OperatorTok{+}\NormalTok{ meanNull}
\NormalTok{w\_f\_shifted }\OperatorTok{=}\NormalTok{ w\_f }\OperatorTok{{-}}\NormalTok{ np.mean(w\_f) }\OperatorTok{+}\NormalTok{ meanNull}


\CommentTok{\# Compute 10,000 bootstrap replicates from shifted arrays}
\NormalTok{M}\OperatorTok{=}\DecValTok{1000}
\NormalTok{bs\_replicates\_m }\OperatorTok{=}\NormalTok{ draw\_bs\_reps(w\_m\_shifted, np.mean, M)}
\NormalTok{bs\_replicates\_f }\OperatorTok{=}\NormalTok{ draw\_bs\_reps(w\_f\_shifted, np.mean, M)}

\CommentTok{\# Get replicates of difference of means: bs\_replicates}
\NormalTok{bs\_replicates }\OperatorTok{=}\NormalTok{ bs\_replicates\_m }\OperatorTok{{-}}\NormalTok{ bs\_replicates\_f}

\NormalTok{tmp}\OperatorTok{=}\NormalTok{plt.hist(bs\_replicates)}
\CommentTok{\# Compute and print p{-}value: p}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture3_files/figure-pdf/cell-29-output-1.png}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#debugging:}
\ControlFlowTok{if} \DecValTok{0}\NormalTok{:}
\NormalTok{    meanNull}
    \CommentTok{\#plt.hist(w\_f\_shifted)}
    \CommentTok{\#plt.hist(bs\_replicates\_m)}
    \CommentTok{\#bs\_replicates\_m}
\NormalTok{    w\_m\_bs }\OperatorTok{=}\NormalTok{ np.random.choice(w\_m, size}\OperatorTok{=}\BuiltInTok{len}\NormalTok{(w\_m))}
\NormalTok{    np.nanmean(w\_m\_bs)}
\NormalTok{    np.argwhere(np.isnan(w\_m\_bs))}
\NormalTok{    np.argwhere(np.isnan(w\_m))}

\CommentTok{\#p{-}value (one{-}sided):}
\NormalTok{pVal }\OperatorTok{=}\NormalTok{ np.mean(bs\_replicates}\OperatorTok{\textgreater{}}\NormalTok{ empirical\_diff\_means)}
\CommentTok{\#cutoff right tail}
\CommentTok{\#np.percentile(bs\_replicates, 95)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"The one sided p value of"}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{(pVal,}\DecValTok{3}\NormalTok{), }\StringTok{"is much smaller than alpha="}\NormalTok{,a,}\StringTok{", so we fail to reject the Null"}\NormalTok{ ) }

\BuiltInTok{print}\NormalTok{(}\StringTok{"{-}\textgreater{} The observed difference of "}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{(empirical\_diff\_means,}\DecValTok{5}\NormalTok{), }\StringTok{"is exremely unlikely to have occurred by chance alone"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The one sided p value of 0.0 is much smaller than alpha= 0.01 , so we fail to reject the Null
-> The observed difference of  0.275 is exremely unlikely to have occurred by chance alone
\end{verbatim}

\bookmarksetup{startatroot}

\hypertarget{ab-testing-1}{%
\chapter{AB Testing}\label{ab-testing-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Permutation Tests
\item
  Binary Processes

  \begin{itemize}
  \tightlist
  \item
    AB Tests
  \item
    Binomial Distribution
  \end{itemize}
\end{enumerate}

\textbf{Importing Our Functions}

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{\%}\NormalTok{run .}\OperatorTok{/}\NormalTok{ourFunctions.py}
\OperatorTok{\%}\NormalTok{precision }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'%.3f'
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# \%load ../ourFunctions.py}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib }\ImportTok{as}\NormalTok{ matplt}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm}
\ImportTok{from}\NormalTok{ scipy }\ImportTok{import}\NormalTok{ stats}

\ImportTok{from}\NormalTok{ numpy }\ImportTok{import}\NormalTok{ random }
\CommentTok{\#random.seed(42)}
\end{Highlighting}
\end{Shaded}

\hypertarget{permutation-2-sample-test}{%
\section{Permutation 2-sample test}\label{permutation-2-sample-test}}

We have used the bootstrap to compare two sets of data, both of which
are samples. In particular, we can test two-sample hypotheses such as

\(H_0: \mu_m = \mu_f, H_A: \mu_m \neq \mu_f\)

or the one-sided versions:

\(H_0: \mu_m = \mu_f, H_A: \mu_m > \mu_f\)

\(H_0: \mu_m = \mu_f, H_A: \mu_m < \mu_f\)

Another way to compare 2 distributions (in some ways much more
straightforward than the bootstrap) is \textbf{permutation sampling}. It
directly simulates the hypothesis that two variables have identical
probability distributions.

A permutation sample of two arrays having respectively \(n_1\) and
\(n_2\) entries is constructed by concatenating the arrays together,
scrambling the contents of the concatenated array, and then taking the
first \(n_1\) entries as the permutation sample of the first array and
the last \(n_2\) entries as the permutation sample of the second array.

At DataCamp the first example offers a nice visualization of this
process:

Take a look at the code in \emph{ourFunctions.py} to run a permutation
test

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Let us apply our first permutation sampling on the Titanic data.
(First, we explore the data a bit)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{titanic }\OperatorTok{=}\NormalTok{ sns.load\_dataset(}\StringTok{\textquotesingle{}titanic\textquotesingle{}}\NormalTok{)}
\NormalTok{titanic.head()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llllllllllllllll@{}}
\toprule()
& survived & pclass & sex & age & sibsp & parch & fare & embarked &
class & who & adult\_male & deck & embark\_town & alive & alone \\
\midrule()
\endhead
0 & 0 & 3 & male & 22.0 & 1 & 0 & 7.2500 & S & Third & man & True & NaN
& Southampton & no & False \\
1 & 1 & 1 & female & 38.0 & 1 & 0 & 71.2833 & C & First & woman & False
& C & Cherbourg & yes & False \\
2 & 1 & 3 & female & 26.0 & 0 & 0 & 7.9250 & S & Third & woman & False &
NaN & Southampton & yes & True \\
3 & 1 & 1 & female & 35.0 & 1 & 0 & 53.1000 & S & First & woman & False
& C & Southampton & yes & False \\
4 & 0 & 3 & male & 35.0 & 0 & 0 & 8.0500 & S & Third & man & True & NaN
& Southampton & no & True \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PclassSurv }\OperatorTok{=}\NormalTok{ titanic.groupby([}\StringTok{\textquotesingle{}pclass\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}survived\textquotesingle{}}\NormalTok{])}
\NormalTok{PclassSurv.size()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
pclass  survived
1       0            80
        1           136
2       0            97
        1            87
3       0           372
        1           119
dtype: int64
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pd.crosstab(titanic.pclass, titanic.survived,margins}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule()
survived & 0 & 1 & All \\
pclass & & & \\
\midrule()
\endhead
1 & 80 & 136 & 216 \\
2 & 97 & 87 & 184 \\
3 & 372 & 119 & 491 \\
All & 549 & 342 & 891 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{WomenOnly }\OperatorTok{=}\NormalTok{ titanic[titanic[}\StringTok{"sex"}\NormalTok{]}\OperatorTok{==}\StringTok{"female"}\NormalTok{]}
\NormalTok{pd.crosstab(WomenOnly.pclass, WomenOnly.survived,margins}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llll@{}}
\toprule()
survived & 0 & 1 & All \\
pclass & & & \\
\midrule()
\endhead
1 & 3 & 91 & 94 \\
2 & 6 & 70 & 76 \\
3 & 72 & 72 & 144 \\
All & 81 & 233 & 314 \\
\bottomrule()
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Test the claim that the survival chances of women in 1st and 2nd
class were pretty much the same.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write down the Null hypothesis and test statistic
\item
  Write code that generates permutation samples from two data sets
\item
  Generate many \textbf{permutation replicates} for the relevant Titanic
  subset
\item
  Compute a p-value
\end{enumerate}

We could choose alpha = 0.05, but keep in mind the following - would you
step into a plane that has a 5\% crash probability ? - Would you buy a
drug that has a 5\% chance of severe side effects ?

\textbf{What is the difference between these two methods (bootstrap,
permutation) ?}

Testing the hypothesis that two samples have the same distribution may
be done with a bootstrap test, but a permutation test is preferred
because it is more accurate (exact, in fact). But a permutation test is
not as versatile as the bootstrap.

We often want to test the hypothesis that population A and population B
have the same mean, but not necessarily the same distribution. This is
difficult with a permutation test as it assumes
\textbf{exchangeability}.

We will get back to this topic!

\href{https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20150001882.pdf}{More
info..}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sample-t-test}{%
\section{2-sample t test}\label{sample-t-test}}

Of course there is an equivalent fully parametric 2-sample test, the
t-test.

We first read in the
\href{https://greenteapress.com/thinkstats2/html/thinkstats2002.html\#sec7}{The
National Survey of Family Growth} data from the
\href{https://greenteapress.com/wp/think-stats-2e/}{think stats book}.

Look at section 1.7 for a description of the variables.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#preg = pd.read\_hdf(\textquotesingle{}data/pregNSFG.h5\textquotesingle{}, \textquotesingle{}df\textquotesingle{})}
\NormalTok{preg }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}data/pregNSFG.csv.gz\textquotesingle{}}\NormalTok{, compression}\OperatorTok{=}\StringTok{\textquotesingle{}gzip\textquotesingle{}}\NormalTok{)}
\CommentTok{\#only look at live births}
\NormalTok{live }\OperatorTok{=}\NormalTok{ preg[preg.outcome }\OperatorTok{==} \DecValTok{1}\NormalTok{]}

\CommentTok{\#define first babies}
\NormalTok{firsts }\OperatorTok{=}\NormalTok{ live[live.birthord }\OperatorTok{==} \DecValTok{1}\NormalTok{]}

\CommentTok{\#and all others}
\NormalTok{others }\OperatorTok{=}\NormalTok{ live[live.birthord }\OperatorTok{!=} \DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tRes }\OperatorTok{=}\NormalTok{ stats.ttest\_ind(firsts.prglngth.values, others.prglngth.values)}
\NormalTok{p }\OperatorTok{=}\NormalTok{ pd.Series([tRes.pvalue,tRes.statistic], index }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}p{-}value:\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}test statistic:\textquotesingle{}}\NormalTok{])}
\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
p-value:           0.167554
test statistic:    1.380215
dtype: float64
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#ttest\_ind often underestimates p for unequal variances:}
\NormalTok{tRes }\OperatorTok{=}\NormalTok{ stats.ttest\_ind(firsts.prglngth.values, others.prglngth.values, equal\_var }\OperatorTok{=} \VariableTok{False}\NormalTok{)}
\NormalTok{p }\OperatorTok{=}\NormalTok{ pd.Series([tRes.pvalue,tRes.statistic], index }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}p{-}value:\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}test statistic:\textquotesingle{}}\NormalTok{])}
\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
p-value:           0.168528
test statistic:    1.377059
dtype: float64
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Can you reproduce the first p-value from the
\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html}{test
statistic} ?}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 0\tabcolsep) * \real{0.1111}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\#\# Random Walks
\end{minipage} \\
\midrule()
\endhead
\#\#\# Simulating Many Random Walks at Once \\
If your goal was to simulate many random walks, say 5,000 of them, you
can generate all of the random walks with minor modifications to the
above code. The numpy.random functions if passed a 2-tuple will generate
a 2D array of draws, and we can compute the cumulative sum across the
rows to compute all 5,000 random walks in one shot \\
::: \{.cell execution\_count=15\} \\
::: \{.cell execution\_count=16\} \\
::: \{.cell-output .cell-output-display execution\_count=16\} \\
::: \{.cell execution\_count=17\} ``` \{.python .cell-code\} nwalks =
5000 nsteps = 10000 steps = random.choice({[}-1,1{]}, size=(nwalks,
nsteps), p=(0.9,0.1)) \# 0 or 1 \#steps = np.where(draws \textgreater{}
0, 1, 0) \#steps = np.where(np.random.rand() \textless= 0.1,1,-1) \\
walks = steps.cumsum(1) ``` ::: \\
::: \{.cell execution\_count=18\} \\
::: \{.cell-output .cell-output-display execution\_count=18\} \\
::: \{.cell execution\_count=16\} \\
::: \{.cell-output .cell-output-stdout\} \\
::: \{.cell execution\_count=26\} ``` \{.python .cell-code\} fig =
plt.figure() ax = fig.add\_subplot(1, 1, 1) \\
ax.plot(walks{[}1,:{]}, `k', label=`one', linewidth=0.25)
ax.plot(walks{[}2,:{]}, label=`two', linestyle =`--', linewidth=0.25)
ax.plot(walks{[}3,:{]}, label=`three', linestyle =`--', linewidth=0.25)
ax.plot(walks{[}4,:{]}, label=`four', linestyle =`--', linewidth=0.25)
ax.grid() \\
\#a very useless legend just because we can ax.legend(loc=`best');
``` \\
::: \{.cell-output .cell-output-display\}
\includegraphics{./DS_Lecture4_files/figure-pdf/cell-18-output-1.png}
::: ::: \\
\bottomrule()
\end{longtable}

\hypertarget{the-sqrtn-law-again}{%
\subsection{\texorpdfstring{The \(\sqrt{n}\) law again
!}{The \textbackslash sqrt\{n\} law again !}}\label{the-sqrtn-law-again}}

Compute the variance and/or standard deviation at times
\(1000, 4000, 9000\).

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{\textquotesingle{}std\_1000:\textquotesingle{}}\NormalTok{, np.std(walks[:,}\DecValTok{1000}\NormalTok{]))}
\BuiltInTok{print}\NormalTok{(}\StringTok{\textquotesingle{}std\_4000:\textquotesingle{}}\NormalTok{, np.std(walks[:,}\DecValTok{4000}\NormalTok{]))}
\BuiltInTok{print}\NormalTok{(}\StringTok{\textquotesingle{}std\_9000:\textquotesingle{}}\NormalTok{, np.std(walks[:,}\DecValTok{9000}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
std_1000: 31.985538009544253
std_4000: 63.65415401998522
std_9000: 95.417731496824
\end{verbatim}

\textbf{Extra Credit}

Out of these walks, let's compute the minimum crossing time to 30 or
-30. This is slightly tricky because not all 5,000 of them reach 30. We
can check this using the \emph{any} method.

We can then use this boolean array to select out the rows of walks that
actually cross the absolute 30 level and call argmax across axis 1 to
get the crossing times:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hits30 }\OperatorTok{=}\NormalTok{ (np.}\BuiltInTok{abs}\NormalTok{(walks) }\OperatorTok{\textgreater{}=} \DecValTok{30}\NormalTok{).}\BuiltInTok{any}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{hits30}
\NormalTok{hits30.}\BuiltInTok{sum}\NormalTok{() }\CommentTok{\# Number that hit 30 or {-}30}

\NormalTok{crossing\_times }\OperatorTok{=}\NormalTok{ (np.}\BuiltInTok{abs}\NormalTok{(walks[hits30]) }\OperatorTok{\textgreater{}=} \DecValTok{30}\NormalTok{).argmax(}\DecValTok{1}\NormalTok{)}
\NormalTok{crossing\_times.mean()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
892.4928
\end{verbatim}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 0\tabcolsep) * \real{0.1250}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\#\# Tasks
\end{minipage} \\
\midrule()
\endhead
\#\#\# Binomial Probability Distribution \\
1. Explore the \emph{binom} function from scipy.stats \\
2. Size matters: insurance company A insures 100 cars, company B 400
cars. The probability of a car being stolen is 10\%. Compute the
probabilities that more than 15\% of the respective fleets are
stolen. \\
4. Faced with a mutliple choice test containing 20 question with 4
choices each you decide in desparation to just guess all answers. What
is the probability that you will pass, i.e.~get at least 10 correct
answers? \\
5. Think about nonparametric versions of the above answers \\
::: \{.cell execution\_count=29\} \\
\bottomrule()
\end{longtable}

\hypertarget{ab-testing-2}{%
\subsection{A/B Testing}\label{ab-testing-2}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Perform a permutation test on the DataCamp example:
\end{enumerate}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 0\tabcolsep) * \real{0.2778}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{What does A/B testing have to do with random walks?}
\end{minipage} \\
\midrule()
\endhead
 \\
\textbf{Table 3.11} compares the coefficient estimates obtained from two
separate multiple regression models. The first is a regression of
balance on age and limit, and the second is a regression of balance on
rating and limit. In the first regression, both age and limit are highly
significant with very small pvalues. In the second, the collinearity
between limit and rating has caused the standard error for the limit
coefficient estimate to increase by a factor of 12 and the p-value to
increase to 0.701. In other words, the importance of the limit variable
has been masked due to the presence of collinearity. To avoid such a
situation, it is desirable to identify and address potential
collinearity problems while fitting the model. \\
\bottomrule()
\end{longtable}

\hypertarget{qualitativecategorical-variables}{%
\subsection{Qualitative/Categorical
Variables}\label{qualitativecategorical-variables}}

\begin{itemize}
\tightlist
\item
  Dummy Coding
\item
  Interpretation of coefficients
\end{itemize}

\textbf{Table 3.7}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est }\OperatorTok{=}\NormalTok{ smf.ols(}\StringTok{\textquotesingle{}Balance \textasciitilde{} Gender \textquotesingle{}}\NormalTok{, credit).fit()}
\NormalTok{est.summary().tables[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllllll@{}}
\toprule()
\endhead
& coef & std err & t & P\textgreater\textbar t\textbar{} & {[}0.025 &
0.975{]} \\
Intercept & 509.8031 & 33.128 & 15.389 & 0.000 & 444.675 & 574.931 \\
Gender{[}T.Female{]} & 19.7331 & 46.051 & 0.429 & 0.669 & -70.801 &
110.267 \\
\bottomrule()
\end{longtable}

\textbf{Table 3.8}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.unique(credit[}\StringTok{"Ethnicity"}\NormalTok{])}
\NormalTok{credit.groupby(}\StringTok{"Ethnicity"}\NormalTok{)[}\StringTok{"Balance"}\NormalTok{].mean()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Ethnicity
African American    531.000000
Asian               512.313725
Caucasian           518.497487
Name: Balance, dtype: float64
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est }\OperatorTok{=}\NormalTok{ smf.ols(}\StringTok{\textquotesingle{}Balance \textasciitilde{} Ethnicity\textquotesingle{}}\NormalTok{, credit).fit()}
\NormalTok{est.summary().tables[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllllll@{}}
\toprule()
\endhead
& coef & std err & t & P\textgreater\textbar t\textbar{} & {[}0.025 &
0.975{]} \\
Intercept & 531.0000 & 46.319 & 11.464 & 0.000 & 439.939 & 622.061 \\
Ethnicity{[}T.Asian{]} & -18.6863 & 65.021 & -0.287 & 0.774 & -146.515 &
109.142 \\
Ethnicity{[}T.Caucasian{]} & -12.5025 & 56.681 & -0.221 & 0.826 &
-123.935 & 98.930 \\
\bottomrule()
\end{longtable}

\hypertarget{interactions}{%
\subsection{Interactions}\label{interactions}}

\textbf{Table 3.9}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est }\OperatorTok{=}\NormalTok{ smf.ols(}\StringTok{\textquotesingle{}Sales \textasciitilde{} TV + Radio + TV*Radio\textquotesingle{}}\NormalTok{, advertising).fit()}
\NormalTok{est.summary().tables[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllllll@{}}
\toprule()
\endhead
& coef & std err & t & P\textgreater\textbar t\textbar{} & {[}0.025 &
0.975{]} \\
Intercept & 6.7502 & 0.248 & 27.233 & 0.000 & 6.261 & 7.239 \\
TV & 0.0191 & 0.002 & 12.699 & 0.000 & 0.016 & 0.022 \\
Radio & 0.0289 & 0.009 & 3.241 & 0.001 & 0.011 & 0.046 \\
TV:Radio & 0.0011 & 5.24e-05 & 20.727 & 0.000 & 0.001 & 0.001 \\
\bottomrule()
\end{longtable}

\hypertarget{interaction-between-qualitative-and-quantitative-variables}{%
\subsubsection{Interaction between qualitative and quantitative
variables}\label{interaction-between-qualitative-and-quantitative-variables}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{credit[}\StringTok{"Income2"}\NormalTok{] }\OperatorTok{=}\NormalTok{ credit[}\StringTok{"Income"}\NormalTok{] }\OperatorTok{+}\NormalTok{ scipy.stats.norm.rvs(}\DecValTok{400}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est1 }\OperatorTok{=}\NormalTok{ smf.ols(}\StringTok{\textquotesingle{}Balance \textasciitilde{} Income + Income2 + C(Student)\textquotesingle{}}\NormalTok{, credit).fit()}
\NormalTok{regr1 }\OperatorTok{=}\NormalTok{ est1.params}
\NormalTok{est2 }\OperatorTok{=}\NormalTok{ smf.ols(}\StringTok{\textquotesingle{}Balance \textasciitilde{} Income + Income*C(Student)\textquotesingle{}}\NormalTok{, credit).fit()}
\NormalTok{regr2 }\OperatorTok{=}\NormalTok{ est2.params}

\BuiltInTok{print}\NormalTok{(}\StringTok{\textquotesingle{}Regression 1 {-} without interaction term\textquotesingle{}}\NormalTok{)}
\NormalTok{est1.summary().tables[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Regression 1 - without interaction term
\end{verbatim}

\begin{longtable}[]{@{}lllllll@{}}
\toprule()
\endhead
& coef & std err & t & P\textgreater\textbar t\textbar{} & {[}0.025 &
0.975{]} \\
Intercept & -0.0123 & 0.002 & -7.143 & 0.000 & -0.016 & -0.009 \\
C(Student){[}T.Yes{]} & 382.6705 & 65.311 & 5.859 & 0.000 & 254.272 &
511.069 \\
Income & 5.4557 & 0.621 & 8.779 & 0.000 & 4.234 & 6.677 \\
Income2 & 0.5287 & 0.081 & 6.506 & 0.000 & 0.369 & 0.688 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.mean(credit[}\StringTok{"Income"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
45.218885000000036
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est2 }\OperatorTok{=}\NormalTok{ smf.ols(}\StringTok{\textquotesingle{}Balance \textasciitilde{} Income + Income*C(Ethnicity)*C(Student)\textquotesingle{}}\NormalTok{, credit).fit()}
\NormalTok{est2.summary().tables[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllllll@{}}
\toprule()
\endhead
& coef & std err & t & P\textgreater\textbar t\textbar{} & {[}0.025 &
0.975{]} \\
Intercept & 145.1346 & 64.922 & 2.236 & 0.026 & 17.492 & 272.777 \\
C(Ethnicity){[}T.Asian{]} & -15.5992 & 94.976 & -0.164 & 0.870 &
-202.332 & 171.133 \\
C(Ethnicity){[}T.Caucasian{]} & 122.8200 & 80.930 & 1.518 & 0.130 &
-36.297 & 281.937 \\
C(Student){[}T.Yes{]} & 427.5858 & 246.321 & 1.736 & 0.083 & -56.706 &
911.877 \\
C(Ethnicity){[}T.Asian{]}:C(Student){[}T.Yes{]} & 281.3665 & 297.683 &
0.945 & 0.345 & -303.906 & 866.639 \\
C(Ethnicity){[}T.Caucasian{]}:C(Student){[}T.Yes{]} & -109.7842 &
309.373 & -0.355 & 0.723 & -718.041 & 498.473 \\
Income & 7.1950 & 1.069 & 6.728 & 0.000 & 5.093 & 9.298 \\
Income:C(Ethnicity){[}T.Asian{]} & 0.0963 & 1.667 & 0.058 & 0.954 &
-3.181 & 3.374 \\
Income:C(Ethnicity){[}T.Caucasian{]} & -2.0995 & 1.371 & -1.531 & 0.127
& -4.796 & 0.597 \\
Income:C(Student){[}T.Yes{]} & -0.0693 & 3.716 & -0.019 & 0.985 & -7.375
& 7.236 \\
Income:C(Ethnicity){[}T.Asian{]}:C(Student){[}T.Yes{]} & -4.6311 & 4.475
& -1.035 & 0.301 & -13.429 & 4.167 \\
Income:C(Ethnicity){[}T.Caucasian{]}:C(Student){[}T.Yes{]} & -0.9514 &
5.431 & -0.175 & 0.861 & -11.630 & 9.727 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{\textquotesingle{}}\CharTok{\textbackslash{}n}\StringTok{Regression 2 {-} with interaction term\textquotesingle{}}\NormalTok{)}
\NormalTok{est2.summary().tables[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Regression 2 - with interaction term
\end{verbatim}

\begin{longtable}[]{@{}lllllll@{}}
\toprule()
\endhead
& coef & std err & t & P\textgreater\textbar t\textbar{} & {[}0.025 &
0.975{]} \\
Intercept & 145.1346 & 64.922 & 2.236 & 0.026 & 17.492 & 272.777 \\
C(Ethnicity){[}T.Asian{]} & -15.5992 & 94.976 & -0.164 & 0.870 &
-202.332 & 171.133 \\
C(Ethnicity){[}T.Caucasian{]} & 122.8200 & 80.930 & 1.518 & 0.130 &
-36.297 & 281.937 \\
C(Student){[}T.Yes{]} & 427.5858 & 246.321 & 1.736 & 0.083 & -56.706 &
911.877 \\
C(Ethnicity){[}T.Asian{]}:C(Student){[}T.Yes{]} & 281.3665 & 297.683 &
0.945 & 0.345 & -303.906 & 866.639 \\
C(Ethnicity){[}T.Caucasian{]}:C(Student){[}T.Yes{]} & -109.7842 &
309.373 & -0.355 & 0.723 & -718.041 & 498.473 \\
Income & 7.1950 & 1.069 & 6.728 & 0.000 & 5.093 & 9.298 \\
Income:C(Ethnicity){[}T.Asian{]} & 0.0963 & 1.667 & 0.058 & 0.954 &
-3.181 & 3.374 \\
Income:C(Ethnicity){[}T.Caucasian{]} & -2.0995 & 1.371 & -1.531 & 0.127
& -4.796 & 0.597 \\
Income:C(Student){[}T.Yes{]} & -0.0693 & 3.716 & -0.019 & 0.985 & -7.375
& 7.236 \\
Income:C(Ethnicity){[}T.Asian{]}:C(Student){[}T.Yes{]} & -4.6311 & 4.475
& -1.035 & 0.301 & -13.429 & 4.167 \\
Income:C(Ethnicity){[}T.Caucasian{]}:C(Student){[}T.Yes{]} & -0.9514 &
5.431 & -0.175 & 0.861 & -11.630 & 9.727 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Income (x{-}axis)}
\NormalTok{income }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{,}\DecValTok{150}\NormalTok{)}

\CommentTok{\# Balance without interaction term (y{-}axis)}
\NormalTok{student1 }\OperatorTok{=}\NormalTok{ np.linspace(regr1[}\StringTok{\textquotesingle{}Intercept\textquotesingle{}}\NormalTok{]}\OperatorTok{+}\NormalTok{regr1[}\StringTok{\textquotesingle{}C(Student)[T.Yes]\textquotesingle{}}\NormalTok{],}
\NormalTok{                       regr1[}\StringTok{\textquotesingle{}Intercept\textquotesingle{}}\NormalTok{]}\OperatorTok{+}\NormalTok{regr1[}\StringTok{\textquotesingle{}C(Student)[T.Yes]\textquotesingle{}}\NormalTok{]}\OperatorTok{+}\DecValTok{150}\OperatorTok{*}\NormalTok{regr1[}\StringTok{\textquotesingle{}Income\textquotesingle{}}\NormalTok{])}
\NormalTok{non\_student1 }\OperatorTok{=}\NormalTok{  np.linspace(regr1[}\StringTok{\textquotesingle{}Intercept\textquotesingle{}}\NormalTok{], regr1[}\StringTok{\textquotesingle{}Intercept\textquotesingle{}}\NormalTok{]}\OperatorTok{+}\DecValTok{150}\OperatorTok{*}\NormalTok{regr1[}\StringTok{\textquotesingle{}Income\textquotesingle{}}\NormalTok{])}

\CommentTok{\# Balance with iteraction term (y{-}axis)}
\NormalTok{student2 }\OperatorTok{=}\NormalTok{ np.linspace(regr2[}\StringTok{\textquotesingle{}Intercept\textquotesingle{}}\NormalTok{]}\OperatorTok{+}\NormalTok{regr2[}\StringTok{\textquotesingle{}C(Student)[T.Yes]\textquotesingle{}}\NormalTok{],}
\NormalTok{                       regr2[}\StringTok{\textquotesingle{}Intercept\textquotesingle{}}\NormalTok{]}\OperatorTok{+}\NormalTok{regr2[}\StringTok{\textquotesingle{}C(Student)[T.Yes]\textquotesingle{}}\NormalTok{]}\OperatorTok{+}
                       \DecValTok{150}\OperatorTok{*}\NormalTok{(regr2[}\StringTok{\textquotesingle{}Income\textquotesingle{}}\NormalTok{]}\OperatorTok{+}\NormalTok{regr2[}\StringTok{\textquotesingle{}Income:C(Student)[T.Yes]\textquotesingle{}}\NormalTok{]))}
\NormalTok{non\_student2 }\OperatorTok{=}\NormalTok{  np.linspace(regr2[}\StringTok{\textquotesingle{}Intercept\textquotesingle{}}\NormalTok{], regr2[}\StringTok{\textquotesingle{}Intercept\textquotesingle{}}\NormalTok{]}\OperatorTok{+}\DecValTok{150}\OperatorTok{*}\NormalTok{regr2[}\StringTok{\textquotesingle{}Income\textquotesingle{}}\NormalTok{])}

\CommentTok{\# Create plot}
\NormalTok{fig, (ax1,ax2) }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{12}\NormalTok{,}\DecValTok{5}\NormalTok{))}
\NormalTok{ax1.plot(income, student1, }\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{, income, non\_student1, }\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{)}

\NormalTok{ax2.plot(income, student2, }\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{, income, non\_student2, }\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ ax }\KeywordTok{in}\NormalTok{ fig.axes:}
\NormalTok{    ax.legend([}\StringTok{\textquotesingle{}student\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}non{-}student\textquotesingle{}}\NormalTok{], loc}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{    ax.set\_xlabel(}\StringTok{\textquotesingle{}Income\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax.set\_ylabel(}\StringTok{\textquotesingle{}Balance\textquotesingle{}}\NormalTok{)}
\NormalTok{    ax.set\_ylim(ymax}\OperatorTok{=}\DecValTok{1550}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture5_part2_files/figure-pdf/cell-30-output-1.png}

}

\end{figure}

\hypertarget{non-linear-relationships}{%
\subsection{Non-linear relationships}\label{non-linear-relationships}}

\textbf{Figure 3.8}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{auto }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}./data/Auto.csv\textquotesingle{}}\NormalTok{, na\_values}\OperatorTok{=}\StringTok{\textquotesingle{}?\textquotesingle{}}\NormalTok{).dropna()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.regplot(x}\OperatorTok{=}\StringTok{\textquotesingle{}weight\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}mpg\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{auto,fit\_reg}\OperatorTok{=}\VariableTok{True}\NormalTok{, order}\OperatorTok{=}\DecValTok{2}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<AxesSubplot:xlabel='weight', ylabel='mpg'>
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture5_part2_files/figure-pdf/cell-32-output-2.png}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# With Seaborn\textquotesingle{}s regplot() you can easily plot higher order polynomials.}
\NormalTok{plt.scatter(x}\OperatorTok{=}\NormalTok{auto.horsepower, y}\OperatorTok{=}\NormalTok{auto.mpg, facecolors}\OperatorTok{=}\StringTok{\textquotesingle{}None\textquotesingle{}}\NormalTok{, edgecolors}\OperatorTok{=}\StringTok{\textquotesingle{}k\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{.5}\NormalTok{) }
\NormalTok{sns.regplot(x}\OperatorTok{=}\NormalTok{auto.horsepower, y}\OperatorTok{=}\NormalTok{auto.mpg, ci}\OperatorTok{=}\VariableTok{None}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Linear\textquotesingle{}}\NormalTok{, scatter}\OperatorTok{=}\VariableTok{False}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}orange\textquotesingle{}}\NormalTok{)}
\NormalTok{sns.regplot(x}\OperatorTok{=}\NormalTok{auto.horsepower, y}\OperatorTok{=}\NormalTok{auto.mpg, ci}\OperatorTok{=}\VariableTok{None}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Degree 2\textquotesingle{}}\NormalTok{, order}\OperatorTok{=}\DecValTok{2}\NormalTok{, scatter}\OperatorTok{=}\VariableTok{False}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}lightblue\textquotesingle{}}\NormalTok{)}
\NormalTok{sns.regplot(x}\OperatorTok{=}\NormalTok{auto.horsepower, y}\OperatorTok{=}\NormalTok{auto.mpg, ci}\OperatorTok{=}\VariableTok{None}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}Degree 5\textquotesingle{}}\NormalTok{, order}\OperatorTok{=}\DecValTok{5}\NormalTok{, scatter}\OperatorTok{=}\VariableTok{False}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}g\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.legend()}
\NormalTok{plt.ylim(}\DecValTok{5}\NormalTok{,}\DecValTok{55}\NormalTok{)}
\NormalTok{plt.xlim(}\DecValTok{40}\NormalTok{,}\DecValTok{240}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture5_part2_files/figure-pdf/cell-33-output-1.png}

}

\end{figure}

\textbf{Table 3.10}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{auto[}\StringTok{\textquotesingle{}horsepower2\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ auto.horsepower}\OperatorTok{**}\DecValTok{2}
\NormalTok{auto.head(}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llllllllllll@{}}
\toprule()
& mpg & cylinders & displacement & horsepower & weight & acceleration &
year & origin & name & Manufacturer & horsepower2 \\
\midrule()
\endhead
0 & 18.0 & 8 & 307.0 & 130 & 3504 & 12.0 & 70 & 1 & chevrolet chevelle
malibu & chevrolet & 16900 \\
1 & 15.0 & 8 & 350.0 & 165 & 3693 & 11.5 & 70 & 1 & buick skylark 320 &
buick & 27225 \\
2 & 18.0 & 8 & 318.0 & 150 & 3436 & 11.0 & 70 & 1 & plymouth satellite &
plymouth & 22500 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est }\OperatorTok{=}\NormalTok{ smf.ols(}\StringTok{\textquotesingle{}mpg \textasciitilde{} horsepower + horsepower2\textquotesingle{}}\NormalTok{, auto).fit()}
\NormalTok{est.summary().tables[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllllll@{}}
\toprule()
\endhead
& coef & std err & t & P\textgreater\textbar t\textbar{} & {[}0.025 &
0.975{]} \\
Intercept & 56.9001 & 1.800 & 31.604 & 0.000 & 53.360 & 60.440 \\
horsepower & -0.4662 & 0.031 & -14.978 & 0.000 & -0.527 & -0.405 \\
horsepower2 & 0.0012 & 0.000 & 10.080 & 0.000 & 0.001 & 0.001 \\
\bottomrule()
\end{longtable}

\bookmarksetup{startatroot}

\hypertarget{sample-splitting}{%
\chapter{Sample Splitting}\label{sample-splitting}}

This challenge is based on the Boston data.

\begin{itemize}
\item
  Fit ``the biggest model'' (multiple linear regression) that you
  possibly can (including interactions and categorical variables).
\item
  Dependent Variable: \texttt{MEDV}
\item
  Whoever achieves the highest \(R^2\) wins :)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\ImportTok{import}\NormalTok{ sklearn}
\end{Highlighting}
\end{Shaded}

\hypertarget{find-the-best-fit}{%
\section{Find the best fit}\label{find-the-best-fit}}

Dataset Description:

The Boston house-price data of Harrison, D. and Rubinfeld, D.L. `Hedonic
prices and the demand for clean air'.

Features Description: - \textbf{CRIM}: per capita crime rate by town -
\textbf{ZN}: proportion of residential land zoned for lots over 25,000
sq.ft. - \textbf{INDUS}: proportion of non-retail business acres per
town - \textbf{CHAS}: Charles River dummy variable (= 1 if tract bounds
river; 0 otherwise) - \textbf{NOX}: nitric oxides concentration (parts
per 10 million) - \textbf{RM}: average number of rooms per dwelling -
\textbf{AGE}: proportion of owner-occupied units built prior to 1940 -
\textbf{DIS}: weighted distances to five Boston employment centres -
\textbf{RAD}: index of accessibility to radial highways - \textbf{TAX}:
full-value property-tax rate per 10,000 dollars - \textbf{PTRATIO}:
pupil-teacher ratio by town - \textbf{BLACK}: 1000(Bk - 0.63)\^{}2 where
Bk is the proportion of blacks by town - \textbf{LSTAT}: percentage
lower status of the population - \textbf{MEDV}: Median value of
owner-occupied homes in 1000's dollars

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}./data/Boston.csv\textquotesingle{}}\NormalTok{)}

\CommentTok{\# independent variables}
\NormalTok{X }\OperatorTok{=}\NormalTok{ boston.drop(}\StringTok{\textquotesingle{}medv\textquotesingle{}}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\CommentTok{\# dependent variable}
\NormalTok{y }\OperatorTok{=}\NormalTok{ boston[}\StringTok{\textquotesingle{}medv\textquotesingle{}}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(boston.shape)}
\NormalTok{boston.head()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllllllllllllll@{}}
\toprule()
& crim & zn & indus & chas & nox & rm & age & dis & rad & tax & ptratio
& black & lstat & medv \\
\midrule()
\endhead
0 & 0.00632 & 18.0 & 2.31 & 0 & 0.538 & 6.575 & 65.2 & 4.0900 & 1 & 296
& 15.3 & 396.90 & 4.98 & 24.0 \\
1 & 0.02731 & 0.0 & 7.07 & 0 & 0.469 & 6.421 & 78.9 & 4.9671 & 2 & 242 &
17.8 & 396.90 & 9.14 & 21.6 \\
2 & 0.02729 & 0.0 & 7.07 & 0 & 0.469 & 7.185 & 61.1 & 4.9671 & 2 & 242 &
17.8 & 392.83 & 4.03 & 34.7 \\
3 & 0.03237 & 0.0 & 2.18 & 0 & 0.458 & 6.998 & 45.8 & 6.0622 & 3 & 222 &
18.7 & 394.63 & 2.94 & 33.4 \\
4 & 0.06905 & 0.0 & 2.18 & 0 & 0.458 & 7.147 & 54.2 & 6.0622 & 3 & 222 &
18.7 & 396.90 & 5.33 & 36.2 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.describe()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllllllllllllll@{}}
\toprule()
& crim & zn & indus & chas & nox & rm & age & dis & rad & tax & ptratio
& black & lstat & medv \\
\midrule()
\endhead
count & 506.000000 & 506.000000 & 506.000000 & 506.000000 & 506.000000 &
506.000000 & 506.000000 & 506.000000 & 506.000000 & 506.000000 &
506.000000 & 506.000000 & 506.000000 & 506.000000 \\
mean & 3.613524 & 11.363636 & 11.136779 & 0.069170 & 0.554695 & 6.284634
& 68.574901 & 3.795043 & 9.549407 & 408.237154 & 18.455534 & 356.674032
& 12.653063 & 22.532806 \\
std & 8.601545 & 23.322453 & 6.860353 & 0.253994 & 0.115878 & 0.702617 &
28.148861 & 2.105710 & 8.707259 & 168.537116 & 2.164946 & 91.294864 &
7.141062 & 9.197104 \\
min & 0.006320 & 0.000000 & 0.460000 & 0.000000 & 0.385000 & 3.561000 &
2.900000 & 1.129600 & 1.000000 & 187.000000 & 12.600000 & 0.320000 &
1.730000 & 5.000000 \\
25\% & 0.082045 & 0.000000 & 5.190000 & 0.000000 & 0.449000 & 5.885500 &
45.025000 & 2.100175 & 4.000000 & 279.000000 & 17.400000 & 375.377500 &
6.950000 & 17.025000 \\
50\% & 0.256510 & 0.000000 & 9.690000 & 0.000000 & 0.538000 & 6.208500 &
77.500000 & 3.207450 & 5.000000 & 330.000000 & 19.050000 & 391.440000 &
11.360000 & 21.200000 \\
75\% & 3.677083 & 12.500000 & 18.100000 & 0.000000 & 0.624000 & 6.623500
& 94.075000 & 5.188425 & 24.000000 & 666.000000 & 20.200000 & 396.225000
& 16.955000 & 25.000000 \\
max & 88.976200 & 100.000000 & 27.740000 & 1.000000 & 0.871000 &
8.780000 & 100.000000 & 12.126500 & 24.000000 & 711.000000 & 22.000000 &
396.900000 & 37.970000 & 50.000000 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston.corr()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllllllllllllll@{}}
\toprule()
& crim & zn & indus & chas & nox & rm & age & dis & rad & tax & ptratio
& black & lstat & medv \\
\midrule()
\endhead
crim & 1.000000 & -0.200469 & 0.406583 & -0.055892 & 0.420972 &
-0.219247 & 0.352734 & -0.379670 & 0.625505 & 0.582764 & 0.289946 &
-0.385064 & 0.455621 & -0.388305 \\
zn & -0.200469 & 1.000000 & -0.533828 & -0.042697 & -0.516604 & 0.311991
& -0.569537 & 0.664408 & -0.311948 & -0.314563 & -0.391679 & 0.175520 &
-0.412995 & 0.360445 \\
indus & 0.406583 & -0.533828 & 1.000000 & 0.062938 & 0.763651 &
-0.391676 & 0.644779 & -0.708027 & 0.595129 & 0.720760 & 0.383248 &
-0.356977 & 0.603800 & -0.483725 \\
chas & -0.055892 & -0.042697 & 0.062938 & 1.000000 & 0.091203 & 0.091251
& 0.086518 & -0.099176 & -0.007368 & -0.035587 & -0.121515 & 0.048788 &
-0.053929 & 0.175260 \\
nox & 0.420972 & -0.516604 & 0.763651 & 0.091203 & 1.000000 & -0.302188
& 0.731470 & -0.769230 & 0.611441 & 0.668023 & 0.188933 & -0.380051 &
0.590879 & -0.427321 \\
rm & -0.219247 & 0.311991 & -0.391676 & 0.091251 & -0.302188 & 1.000000
& -0.240265 & 0.205246 & -0.209847 & -0.292048 & -0.355501 & 0.128069 &
-0.613808 & 0.695360 \\
age & 0.352734 & -0.569537 & 0.644779 & 0.086518 & 0.731470 & -0.240265
& 1.000000 & -0.747881 & 0.456022 & 0.506456 & 0.261515 & -0.273534 &
0.602339 & -0.376955 \\
dis & -0.379670 & 0.664408 & -0.708027 & -0.099176 & -0.769230 &
0.205246 & -0.747881 & 1.000000 & -0.494588 & -0.534432 & -0.232471 &
0.291512 & -0.496996 & 0.249929 \\
rad & 0.625505 & -0.311948 & 0.595129 & -0.007368 & 0.611441 & -0.209847
& 0.456022 & -0.494588 & 1.000000 & 0.910228 & 0.464741 & -0.444413 &
0.488676 & -0.381626 \\
tax & 0.582764 & -0.314563 & 0.720760 & -0.035587 & 0.668023 & -0.292048
& 0.506456 & -0.534432 & 0.910228 & 1.000000 & 0.460853 & -0.441808 &
0.543993 & -0.468536 \\
ptratio & 0.289946 & -0.391679 & 0.383248 & -0.121515 & 0.188933 &
-0.355501 & 0.261515 & -0.232471 & 0.464741 & 0.460853 & 1.000000 &
-0.177383 & 0.374044 & -0.507787 \\
black & -0.385064 & 0.175520 & -0.356977 & 0.048788 & -0.380051 &
0.128069 & -0.273534 & 0.291512 & -0.444413 & -0.441808 & -0.177383 &
1.000000 & -0.366087 & 0.333461 \\
lstat & 0.455621 & -0.412995 & 0.603800 & -0.053929 & 0.590879 &
-0.613808 & 0.602339 & -0.496996 & 0.488676 & 0.543993 & 0.374044 &
-0.366087 & 1.000000 & -0.737663 \\
medv & -0.388305 & 0.360445 & -0.483725 & 0.175260 & -0.427321 &
0.695360 & -0.376955 & 0.249929 & -0.381626 & -0.468536 & -0.507787 &
0.333461 & -0.737663 & 1.000000 \\
\bottomrule()
\end{longtable}

\hypertarget{additive-models-first}{%
\subsection{Additive Models first}\label{additive-models-first}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#import statsmodel and sklearn Library}
\ImportTok{import}\NormalTok{ sklearn.linear\_model }\ImportTok{as}\NormalTok{ skl\_lm}
\ImportTok{import}\NormalTok{ statsmodels.formula.api }\ImportTok{as}\NormalTok{ smf}

\CommentTok{\# Create a Baseline Multiple Linear Regression}
\NormalTok{est }\OperatorTok{=}\NormalTok{ smf.ols(}\StringTok{\textquotesingle{}medv \textasciitilde{} lstat + black + ptratio \textquotesingle{}}\NormalTok{, boston).fit()}
\NormalTok{est.summary().tables[}\DecValTok{1}\NormalTok{]}

\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ est.predict()}

\NormalTok{sklearn.metrics.r2\_score(y, y\_pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.6098453143448523
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a Multiple Linear Regression with all columns}
\NormalTok{est }\OperatorTok{=}\NormalTok{ smf.ols(}\StringTok{\textquotesingle{}medv \textasciitilde{} lstat + black + ptratio + tax + rad + dis + age + rm + nox +chas + indus + zn +crim \textquotesingle{}}\NormalTok{, boston).fit()}
\NormalTok{est.summary().tables[}\DecValTok{1}\NormalTok{]}

\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ est.predict()}

\NormalTok{sklearn.metrics.r2\_score(y, y\_pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.7406426641094095
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Try out with Variables which have a higher correlation with medv (\textgreater{} 0.3)}
\NormalTok{est }\OperatorTok{=}\NormalTok{ smf.ols(}\StringTok{\textquotesingle{}medv \textasciitilde{} lstat + black + ptratio + tax  + rad + age + rm + nox + indus + zn +crim \textquotesingle{}}\NormalTok{, boston).fit()}

\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ est.predict()}

\NormalTok{sklearn.metrics.r2\_score(y, y\_pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.7062733492875524
\end{verbatim}

\hypertarget{interactions-1}{%
\subsection{Interactions}\label{interactions-1}}

Find Variables which correlate and add their Interactions to the model

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# add Interactions between Variables}
\NormalTok{est }\OperatorTok{=}\NormalTok{ smf.ols(}\StringTok{\textquotesingle{}medv \textasciitilde{} lstat + black + ptratio + tax + rad + dis + age + rm + nox +chas + indus + zn +crim + rad * crim + crim * tax + dis * zn + chas * indus + nox * indus + age * indus + dis * indus + rad * indus + tax * indus + lstat * indus + nox * dis + nox * age + nox * rad + nox * tax + rm * lstat + age * dis + age * lstat + rad * tax\textquotesingle{}}\NormalTok{, boston).fit()}
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ est.predict()}

\NormalTok{sklearn.metrics.r2\_score(y, y\_pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.8276188554359157
\end{verbatim}

Find the Categorical Variables and dummify

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{\textquotesingle{}rad: \textquotesingle{}}\NormalTok{ , boston[}\StringTok{\textquotesingle{}rad\textquotesingle{}}\NormalTok{].nunique())}
\BuiltInTok{print}\NormalTok{(}\StringTok{\textquotesingle{}zn: \textquotesingle{}}\NormalTok{ ,boston[}\StringTok{\textquotesingle{}zn\textquotesingle{}}\NormalTok{].nunique())}
\BuiltInTok{print}\NormalTok{(}\StringTok{\textquotesingle{}indus: \textquotesingle{}}\NormalTok{ ,boston[}\StringTok{\textquotesingle{}indus\textquotesingle{}}\NormalTok{].nunique())}
\BuiltInTok{print}\NormalTok{(}\StringTok{\textquotesingle{}chas: \textquotesingle{}}\NormalTok{ ,boston[}\StringTok{\textquotesingle{}chas\textquotesingle{}}\NormalTok{].nunique())}
\BuiltInTok{print}\NormalTok{(}\StringTok{\textquotesingle{}dis: \textquotesingle{}}\NormalTok{ ,boston[}\StringTok{\textquotesingle{}dis\textquotesingle{}}\NormalTok{].nunique())}
\BuiltInTok{print}\NormalTok{(}\StringTok{\textquotesingle{}tax: \textquotesingle{}}\NormalTok{ ,boston[}\StringTok{\textquotesingle{}tax\textquotesingle{}}\NormalTok{].nunique())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
rad:  9
zn:  26
indus:  76
chas:  2
dis:  412
tax:  66
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# add Categoricals rad and chas}
\NormalTok{est }\OperatorTok{=}\NormalTok{ smf.ols(}\StringTok{\textquotesingle{}medv \textasciitilde{} lstat + black + ptratio + tax + C(rad) + dis + age + rm + nox + C(chas) + indus + zn + crim + crim * zn + crim * indus + crim * chas + crim * nox + crim *rm + crim * age + crim * dis + crim * rad + crim * tax + crim * ptratio + crim * black + crim * lstat + zn * indus + zn * chas + zn * nox + zn * rm + zn * age + zn * dis + zn * rad + zn * tax + zn * ptratio + zn * black + zn * lstat + indus * chas + indus * nox + indus * rm + indus * age + indus * dis + indus * rad + indus * tax + indus * ptratio + indus * black + indus * lstat + chas * nox + chas * rm + chas * age + chas * dis + chas * rad + chas * tax + chas * ptratio + chas * black + chas * lstat + nox * rm + nox * age + nox * dis + nox * rad + nox * tax + nox * ptratio + nox * black + nox * lstat + rm * age + rm * dis + rm *rad + rm * tax + rm * ptratio + rm *black + rm * lstat + age * dis + age * rad + age * tax + age * ptratio + age * black + age * lstat + dis * rad + dis * tax + dis *ptratio + dis * black + dis * lstat + rad * tax + rad * ptratio + rad * black + rad * lstat + tax * ptratio + tax * black + tax * lstat + black * lstat \textquotesingle{}}\NormalTok{, boston).fit()}
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ est.predict()}

\NormalTok{sklearn.metrics.r2\_score(y, y\_pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.9247310711792476
\end{verbatim}

most likely Overfitted Model with \(R^2\) of 0.92

\hypertarget{the-perfect-fit}{%
\subsection{The perfect fit}\label{the-perfect-fit}}

Maximal Overfitted Model all predicting Variables as Categories + all
possible Relationships

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est }\OperatorTok{=}\NormalTok{ smf.ols(}\StringTok{\textquotesingle{}medv \textasciitilde{} C(lstat) + C(black) + C(ptratio) + C(tax) + C(rad) + C(dis) + C(age) + C(rm) + C(nox) + C(chas) + C(indus) + C(zn) + C(crim) + crim * zn + crim * indus + crim * chas + crim * nox + crim *rm + crim * age + crim * dis + crim * rad + crim * tax + crim * ptratio + crim * black + crim * lstat + zn * indus + zn * chas + zn * nox + zn * rm + zn * age + zn * dis + zn * rad + zn * tax + zn * ptratio + zn * black + zn * lstat + indus * chas + indus * nox + indus * rm + indus * age + indus * dis + indus * rad + indus * tax + indus * ptratio + indus * black + indus * lstat + chas * nox + chas * rm + chas * age + chas * dis + chas * rad + chas * tax + chas * ptratio + chas * black + chas * lstat + nox * rm + nox * age + nox * dis + nox * rad + nox * tax + nox * ptratio + nox * black + nox * lstat + rm * age + rm * dis + rm *rad + rm * tax + rm * ptratio + rm *black + rm * lstat + age * dis + age * rad + age * tax + age * ptratio + age * black + age * lstat + dis * rad + dis * tax + dis *ptratio + dis * black + dis * lstat + rad * tax + rad * ptratio + rad * black + rad * lstat + tax * ptratio + tax * black + tax * lstat + black * lstat \textquotesingle{}}\NormalTok{, boston).fit()}
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ est.predict()}

\NormalTok{sklearn.metrics.r2\_score(y, y\_pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
1.0
\end{verbatim}

\hypertarget{train-test-split}{%
\subsection{Train Test Split}\label{train-test-split}}

How do we quantify the notion of \textbf{overfitting}, i.e.~the
(obvious?) impression that the model is ``too wiggly'', or \textbf{too
complex} ?

The \(R^2\) on the data we used to fit the model is useless for this
purpose because it seems to only improve the more complex the model
becomes!

One useful idea seems the following: if the orange line does not really
capture the ``true model'', i.e.~has adapted too much to the noise, its
performance on a test set would be worse than a simpler model.

Let us examine this idea by using a \textbf{Train Test Split}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ mean\_squared\_error}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}
\CommentTok{\#from sklearn.linear\_model import LinearRegression}
\NormalTok{boston.head()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllllllllllllll@{}}
\toprule()
& crim & zn & indus & chas & nox & rm & age & dis & rad & tax & ptratio
& black & lstat & medv \\
\midrule()
\endhead
0 & 0.00632 & 18.0 & 2.31 & 0 & 0.538 & 6.575 & 65.2 & 4.0900 & 1 & 296
& 15.3 & 396.90 & 4.98 & 24.0 \\
1 & 0.02731 & 0.0 & 7.07 & 0 & 0.469 & 6.421 & 78.9 & 4.9671 & 2 & 242 &
17.8 & 396.90 & 9.14 & 21.6 \\
2 & 0.02729 & 0.0 & 7.07 & 0 & 0.469 & 7.185 & 61.1 & 4.9671 & 2 & 242 &
17.8 & 392.83 & 4.03 & 34.7 \\
3 & 0.03237 & 0.0 & 2.18 & 0 & 0.458 & 6.998 & 45.8 & 6.0622 & 3 & 222 &
18.7 & 394.63 & 2.94 & 33.4 \\
4 & 0.06905 & 0.0 & 2.18 & 0 & 0.458 & 7.147 & 54.2 & 6.0622 & 3 & 222 &
18.7 & 396.90 & 5.33 & 36.2 \\
\bottomrule()
\end{longtable}

\begin{itemize}
\tightlist
\item
  Compute the \(R^2\) for the test portion
\item
  Compare with the adjusted \(R^2\).
\item
  Think about the model complexity parameter in OLS.
\item
  How would one choose which variables should be part of the model?
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train, test }\OperatorTok{=}\NormalTok{ train\_test\_split(boston, test\_size }\OperatorTok{=} \FloatTok{0.1}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{45}\NormalTok{)}

\NormalTok{y\_train }\OperatorTok{=}\NormalTok{ train[}\StringTok{"medv"}\NormalTok{]}
\NormalTok{y\_test }\OperatorTok{=}\NormalTok{ test[}\StringTok{"medv"}\NormalTok{]}

\CommentTok{\#will be useful later}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ train.drop(}\StringTok{"medv"}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{X\_test }\OperatorTok{=}\NormalTok{ test.drop(}\StringTok{"medv"}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Repeat one of the earlier amazing fits:}
\NormalTok{formula }\OperatorTok{=} \StringTok{\textquotesingle{}medv \textasciitilde{} lstat + black + ptratio + tax + C(rad) + dis + age + rm + nox + C(chas) + indus + zn + crim + crim * zn + crim * indus + crim * chas + crim * nox + crim *rm + crim * age + crim * dis + crim * rad + crim * tax + crim * ptratio + crim * black + crim * lstat + zn * indus + zn * chas + zn * nox + zn * rm + zn * age + zn * dis + zn * rad + zn * tax + zn * ptratio + zn * black + zn * lstat + indus * chas + indus * nox + indus * rm + indus * age + indus * dis + indus * rad + indus * tax + indus * ptratio + indus * black + indus * lstat + chas * nox + chas * rm + chas * age + chas * dis + chas * rad + chas * tax + chas * ptratio + chas * black + chas * lstat + nox * rm + nox * age + nox * dis + nox * rad + nox * tax + nox * ptratio + nox * black + nox * lstat + rm * age + rm * dis + rm *rad + rm * tax + rm * ptratio + rm *black + rm * lstat + age * dis + age * rad + age * tax + age * ptratio + age * black + age * lstat + dis * rad + dis * tax + dis *ptratio + dis * black + dis * lstat + rad * tax + rad * ptratio + rad * black + rad * lstat + tax * ptratio + tax * black + tax * lstat + black * lstat \textquotesingle{}}

\NormalTok{est }\OperatorTok{=}\NormalTok{ smf.ols(formula, train).fit()}
\NormalTok{y\_pred\_train }\OperatorTok{=}\NormalTok{ est.predict()}
\NormalTok{y\_pred\_test }\OperatorTok{=}\NormalTok{ est.predict(X\_test)}

\NormalTok{R2\_train }\OperatorTok{=}\NormalTok{ sklearn.metrics.r2\_score(y\_train, y\_pred\_train)}
\NormalTok{R2\_test }\OperatorTok{=}\NormalTok{ sklearn.metrics.r2\_score(y\_test, y\_pred\_test)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"train R2:"}\NormalTok{, R2\_train)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"test R2:"}\NormalTok{, R2\_test)}

\NormalTok{MSE\_test }\OperatorTok{=}\NormalTok{ mean\_squared\_error(y\_test, y\_pred\_test)}
\NormalTok{MSE\_train }\OperatorTok{=}\NormalTok{ mean\_squared\_error(y\_train, y\_pred\_train)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"MSE\_train:"}\NormalTok{, MSE\_train)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"MSE\_test:"}\NormalTok{, MSE\_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
train R2: 0.9274106134480603
test R2: 0.871592468610564
MSE_train: 5.96861557537797
MSE_test: 13.241124958441453
\end{verbatim}

\hypertarget{cross-validation}{%
\section{Cross Validation}\label{cross-validation}}

\textbf{Drawbacks of validation set approach}

\begin{itemize}
\tightlist
\item
  The validation estimate of the test error can be highly variable,
  depending on precisely which observations are included in the
  training/validation set.
\item
  In the validation approach, only a subset of the observations - those
  that are included in the training set rather than in the validation
  set - are used to fit the model.
\item
  This suggests that the validation set error may tend to
  \textbf{overestimate the test error} for the model fit on the entire
  data set.
\end{itemize}

\textbf{K-fold Cross-validation}

\begin{itemize}
\tightlist
\item
  randomly divide the data into K equal-sized parts. We leave out part
  k, fit the model to the other K-1 parts (combined), and then obtain
  predictions for the left-out kth part.
\item
  This is done in turn for each part \(k = 1,2, \ldots, K\), and then
  the results are combined.
\end{itemize}

\hypertarget{design-matrix}{%
\subsection{Design Matrix}\label{design-matrix}}

Get to know your friendly helper library
\href{https://learn-scikit.oneoffcoder.com/patsy.html}{patsy}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ patsy }\ImportTok{import}\NormalTok{ dmatrices}

\NormalTok{y\_0, X\_wide }\OperatorTok{=}\NormalTok{ dmatrices(formula, boston)}\CommentTok{\#, return\_type=\textquotesingle{}dataframe\textquotesingle{})}
\NormalTok{X\_wide.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(506, 99)
\end{verbatim}

\hypertarget{scoring-metrics}{%
\subsection{Scoring Metrics}\label{scoring-metrics}}

Model selection and evaluation using tools, such as
\texttt{model\_selection.GridSearchCV} and
\texttt{model\_selection.cross\_val\_score}, take a \texttt{scoring}
parameter that controls what metric they apply to the estimators
evaluated.

Find the options
\href{https://scikit-learn.org/stable/modules/model_evaluation.html\#scoring-parameter}{here}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ cross\_val\_score}

\NormalTok{reg\_all }\OperatorTok{=}\NormalTok{ LinearRegression()}
\CommentTok{\#The unified scoring API always maximizes the score, }
\CommentTok{\# so scores which need to be minimized are negated in order for the unified scoring API to work correctly.}
\NormalTok{cv\_results }\OperatorTok{=}\NormalTok{ cross\_val\_score(reg\_all, X\_wide, y\_0, cv}\OperatorTok{=}\DecValTok{10}\NormalTok{, scoring}\OperatorTok{=}\StringTok{\textquotesingle{}neg\_mean\_squared\_error\textquotesingle{}}\NormalTok{)}
\NormalTok{cv\_results }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{np.}\BuiltInTok{round}\NormalTok{(cv\_results,}\DecValTok{1}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"CV results: "}\NormalTok{, cv\_results)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"CV results mean: "}\NormalTok{, np.mean(}\OperatorTok{{-}}\NormalTok{cv\_results))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
CV results:  [ 24.61090588  10.30866313  62.18781304  44.2004828  107.53306211
  17.14942076  18.69173435 241.76832017 148.09771969  40.90216317]
CV results mean:  71.54502851092425
\end{verbatim}

\textbf{Comments}

\begin{itemize}
\tightlist
\item
  For non-equal fold sizes, we need to compute the weighted mean!
\item
  Setting \(K = n\) yields n-fold or \textbf{leave-one out
  cross-validation} (LOOCV).
\item
  With least-squares linear or polynomial regression, an amazing
  shortcut makes the cost of LOOCV the same as that of a single model
  fit! The following formula holds:
\end{itemize}

\[
CV_n = \frac{1}{n} \sum{\left( \frac{y_i - \hat{y}_i}{1-h_i} \right)^2}
\]

where \(\hat{y}_i\) is the ith fitted value from the original least
squares fit, and \(h_i\) is the leverage (see ISLR book for details.)
This is like the ordinary MSE, except the ith residual is divided by
\(1-h_i\).

\hypertarget{task-1}{%
\subsection{Task}\label{task-1}}

\begin{itemize}
\tightlist
\item
  Sketch the code for your own CV function.
\item
  Reproduce the left panel of Fig. 5.6, i.e.~the right panel of Fig 2.9
  from the ISLR book
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{classification}{%
\chapter{Classification}\label{classification}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib }\ImportTok{as}\NormalTok{ mpl}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{from}\NormalTok{ mpl\_toolkits.mplot3d }\ImportTok{import}\NormalTok{ axes3d}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}

\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ scale}
\ImportTok{import}\NormalTok{ sklearn.linear\_model }\ImportTok{as}\NormalTok{ skl\_lm}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ mean\_squared\_error, r2\_score}
\ImportTok{import}\NormalTok{ statsmodels.api }\ImportTok{as}\NormalTok{ sm}
\ImportTok{import}\NormalTok{ statsmodels.formula.api }\ImportTok{as}\NormalTok{ smf}

\ImportTok{from}\NormalTok{ scipy }\ImportTok{import}\NormalTok{ stats}
\CommentTok{\#in response to: module \textquotesingle{}scipy.stats\textquotesingle{} has no attribute \textquotesingle{}chisqprob\textquotesingle{} }
\CommentTok{\#stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)}
\CommentTok{\#this one I inserted just before class, sorry !!}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ confusion\_matrix, roc\_curve, roc\_auc\_score }\CommentTok{\#, log\_loss}

\ImportTok{from}\NormalTok{ scipy }\ImportTok{import}\NormalTok{ special}
\OperatorTok{\%}\NormalTok{matplotlib inline}
\NormalTok{sns.set\_style(}\StringTok{\textquotesingle{}white\textquotesingle{}}\NormalTok{)}

\OperatorTok{\%}\NormalTok{precision }\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'%.3f'
\end{verbatim}

\textbf{Load data}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#1. default data from ISLR}

\CommentTok{\# In R, we exported the dataset from package \textquotesingle{}ISLR\textquotesingle{} to an Excel file}
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}./data/Default.csv\textquotesingle{}}\NormalTok{,index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\CommentTok{\#2. Titanic}
\NormalTok{titanic }\OperatorTok{=}\NormalTok{ sns.load\_dataset(}\StringTok{\textquotesingle{}titanic\textquotesingle{}}\NormalTok{)}
\NormalTok{df.head(}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllll@{}}
\toprule()
& default & student & balance & income \\
\midrule()
\endhead
1 & No & No & 729.526495 & 44361.62507 \\
2 & No & Yes & 817.180407 & 12106.13470 \\
3 & No & No & 1073.549164 & 31767.13895 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note: factorize() returns two objects: a label array and an array with the unique values.}
\NormalTok{df.default.factorize()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(array([0, 0, 0, ..., 0, 0, 0]), Index(['No', 'Yes'], dtype='object'))
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We are only interested in the first object. }
\NormalTok{df[}\StringTok{\textquotesingle{}default2\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ df.default.factorize()[}\DecValTok{0}\NormalTok{]}
\NormalTok{df[}\StringTok{\textquotesingle{}student2\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ df.student.factorize()[}\DecValTok{0}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#are the data balanced}
\CommentTok{\#very unbalanced data make modeling VERY tough !!}
\NormalTok{df[}\StringTok{\textquotesingle{}default2\textquotesingle{}}\NormalTok{].value\_counts()}
\CommentTok{\#np.mean(df[\textquotesingle{}default2\textquotesingle{}])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0    9667
1     333
Name: default2, dtype: int64
\end{verbatim}

\hypertarget{data-exploration}{%
\subsection{Data Exploration}\label{data-exploration}}

\textbf{Figure 4.1 (ISLR) - Default data set}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{12}\NormalTok{,}\DecValTok{5}\NormalTok{))}
\NormalTok{gs }\OperatorTok{=}\NormalTok{ mpl.gridspec.GridSpec(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{)}
\NormalTok{ax1 }\OperatorTok{=}\NormalTok{ plt.subplot(gs[}\DecValTok{0}\NormalTok{,:}\OperatorTok{{-}}\DecValTok{2}\NormalTok{])}
\NormalTok{ax2 }\OperatorTok{=}\NormalTok{ plt.subplot(gs[}\DecValTok{0}\NormalTok{,}\OperatorTok{{-}}\DecValTok{2}\NormalTok{])}
\NormalTok{ax3 }\OperatorTok{=}\NormalTok{ plt.subplot(gs[}\DecValTok{0}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}

\CommentTok{\# Take a fraction of the samples where target value (default) is \textquotesingle{}no\textquotesingle{}}
\NormalTok{df\_no }\OperatorTok{=}\NormalTok{ df[df.default2 }\OperatorTok{==} \DecValTok{0}\NormalTok{].sample(frac}\OperatorTok{=}\FloatTok{0.15}\NormalTok{)}
\CommentTok{\# Take all samples  where target value is \textquotesingle{}yes\textquotesingle{}}
\NormalTok{df\_yes }\OperatorTok{=}\NormalTok{ df[df.default2 }\OperatorTok{==} \DecValTok{1}\NormalTok{]}
\NormalTok{df\_ }\OperatorTok{=}\NormalTok{ df\_no.append(df\_yes)}

\NormalTok{ax1.scatter(x}\OperatorTok{=}\NormalTok{df\_[df\_.default }\OperatorTok{==} \StringTok{\textquotesingle{}Yes\textquotesingle{}}\NormalTok{].balance, y}\OperatorTok{=}\NormalTok{df\_[df\_.default }\OperatorTok{==} \StringTok{\textquotesingle{}Yes\textquotesingle{}}\NormalTok{].income, s}\OperatorTok{=}\DecValTok{40}\NormalTok{, c}\OperatorTok{=}\StringTok{\textquotesingle{}orange\textquotesingle{}}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}+\textquotesingle{}}\NormalTok{,}
\NormalTok{            linewidths}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{ax1.scatter(x}\OperatorTok{=}\NormalTok{df\_[df\_.default }\OperatorTok{==} \StringTok{\textquotesingle{}No\textquotesingle{}}\NormalTok{].balance, y}\OperatorTok{=}\NormalTok{df\_[df\_.default }\OperatorTok{==} \StringTok{\textquotesingle{}No\textquotesingle{}}\NormalTok{].income, s}\OperatorTok{=}\DecValTok{40}\NormalTok{, marker}\OperatorTok{=}\StringTok{\textquotesingle{}o\textquotesingle{}}\NormalTok{, linewidths}\OperatorTok{=}\DecValTok{1}\NormalTok{,}
\NormalTok{            edgecolors}\OperatorTok{=}\StringTok{\textquotesingle{}lightblue\textquotesingle{}}\NormalTok{, facecolors}\OperatorTok{=}\StringTok{\textquotesingle{}white\textquotesingle{}}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{.6}\NormalTok{)}

\NormalTok{ax1.set\_ylim(ymin}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{ax1.set\_ylabel(}\StringTok{\textquotesingle{}Income\textquotesingle{}}\NormalTok{)}
\NormalTok{ax1.set\_xlim(xmin}\OperatorTok{={-}}\DecValTok{100}\NormalTok{)}
\NormalTok{ax1.set\_xlabel(}\StringTok{\textquotesingle{}Balance\textquotesingle{}}\NormalTok{)}

\NormalTok{c\_palette }\OperatorTok{=}\NormalTok{ \{}\StringTok{\textquotesingle{}No\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}lightblue\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Yes\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}orange\textquotesingle{}}\NormalTok{\}}
\NormalTok{sns.boxplot(x}\OperatorTok{=}\StringTok{\textquotesingle{}default\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}balance\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{df, orient}\OperatorTok{=}\StringTok{\textquotesingle{}v\textquotesingle{}}\NormalTok{, ax}\OperatorTok{=}\NormalTok{ax2, palette}\OperatorTok{=}\NormalTok{c\_palette)}
\NormalTok{sns.boxplot(x}\OperatorTok{=}\StringTok{\textquotesingle{}default\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}income\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{df, orient}\OperatorTok{=}\StringTok{\textquotesingle{}v\textquotesingle{}}\NormalTok{, ax}\OperatorTok{=}\NormalTok{ax3, palette}\OperatorTok{=}\NormalTok{c\_palette)}
\NormalTok{gs.tight\_layout(plt.gcf())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/var/folders/h4/k73g68ds6xj791sf8cpmlxlc0000gn/T/ipykernel_61849/1350372965.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  df_ = df_no.append(df_yes)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture7_files/figure-pdf/cell-7-output-2.png}

}

\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{logistic-regression}{%
\section{Logistic Regression}\label{logistic-regression}}

Recall our fit to the Titanic data from last week and the dilemma that
some predictions and interpretations (such as the intercept) often led
to survival probabilities outside the range \([0,1]\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est }\OperatorTok{=}\NormalTok{ smf.ols(}\StringTok{\textquotesingle{}survived \textasciitilde{} age + C(pclass) + C(sex)\textquotesingle{}}\NormalTok{, titanic).fit()}
\BuiltInTok{print}\NormalTok{(est.summary().tables[}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
==================================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
----------------------------------------------------------------------------------
Intercept          1.1250      0.051     22.202      0.000       1.026       1.225
C(pclass)[T.2]    -0.2077      0.042     -4.983      0.000      -0.290      -0.126
C(pclass)[T.3]    -0.4066      0.038    -10.620      0.000      -0.482      -0.331
C(sex)[T.male]    -0.4795      0.031    -15.608      0.000      -0.540      -0.419
age               -0.0055      0.001     -5.039      0.000      -0.008      -0.003
==================================================================================
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est }\OperatorTok{=}\NormalTok{ smf.logit(}\StringTok{\textquotesingle{}survived \textasciitilde{} age + C(pclass) + C(sex)\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{titanic)}
\BuiltInTok{print}\NormalTok{(est.fit().summary())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Optimization terminated successfully.
         Current function value: 0.453279
         Iterations 6
                           Logit Regression Results                           
==============================================================================
Dep. Variable:               survived   No. Observations:                  714
Model:                          Logit   Df Residuals:                      709
Method:                           MLE   Df Model:                            4
Date:                Sun, 30 May 2021   Pseudo R-squ.:                  0.3289
Time:                        20:26:47   Log-Likelihood:                -323.64
converged:                       True   LL-Null:                       -482.26
Covariance Type:            nonrobust   LLR p-value:                 2.074e-67
==================================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
----------------------------------------------------------------------------------
Intercept          3.7770      0.401      9.416      0.000       2.991       4.563
C(pclass)[T.2]    -1.3098      0.278     -4.710      0.000      -1.855      -0.765
C(pclass)[T.3]    -2.5806      0.281     -9.169      0.000      -3.132      -2.029
C(sex)[T.male]    -2.5228      0.207    -12.164      0.000      -2.929      -2.116
age               -0.0370      0.008     -4.831      0.000      -0.052      -0.022
==================================================================================
\end{verbatim}

This is not the only shortcoming of \textbf{linear} regression (LR) for
binary outcomes! Other problems include heteroskedasticity and incorrect
scaling of probabilities even inside the range \([0,1]\).

One solution is to transform the linear output of the (LR) to an S-shape
via the \textbf{sigmoidal} function \(s(z) = 1/(1+exp(-z))\), which is
the strategy taken by \textbf{logistic regression} (example: Figure 4.2
from the ISLR book):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#passenger Joe, aged 25, Pclass3:}
\NormalTok{oddsJoe }\OperatorTok{=}\NormalTok{ np.exp(}\FloatTok{3.777} \OperatorTok{{-}}\FloatTok{2.5806}  \OperatorTok{{-}}\FloatTok{2.5228}  \OperatorTok{{-}}\FloatTok{0.0370}\OperatorTok{*}\DecValTok{25}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"The odds of survival for Joe are"}\NormalTok{, }\BuiltInTok{str}\NormalTok{(oddsJoe))}
\NormalTok{survProbJoe }\OperatorTok{=}\NormalTok{ oddsJoe}\OperatorTok{/}\NormalTok{(}\DecValTok{1}\OperatorTok{+}\NormalTok{oddsJoe)}
\NormalTok{survProbJoe}
\BuiltInTok{print}\NormalTok{(}\StringTok{"The probability of survival for Joe are"}\NormalTok{, }\BuiltInTok{str}\NormalTok{(survProbJoe))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The odds of survival for Joe are 0.1052517688905321
The probability of survival for Joe are 0.09522877217033127
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ classification\_report}
\NormalTok{y\_true }\OperatorTok{=}\NormalTok{ y}
\CommentTok{\#The 50\% cutoff/threshold is chosen by the user !!}
\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ prob\_train[:,}\DecValTok{0}\NormalTok{] }\OperatorTok{\textgreater{}} \FloatTok{0.99}
\CommentTok{\#y\_pred}

\BuiltInTok{print}\NormalTok{(classification\_report(y\_true, y\_pred))}\CommentTok{\#, target\_names=[\textquotesingle{}class 0\textquotesingle{}, \textquotesingle{}class 1\textquotesingle{}]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              precision    recall  f1-score   support

           0       0.16      0.00      0.00      9667
           1       0.03      0.92      0.06       333

    accuracy                           0.03     10000
   macro avg       0.10      0.46      0.03     10000
weighted avg       0.16      0.03      0.00     10000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pd.crosstab(y\_true, y\_pred)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule()
col\_0 & False & True \\
default2 & & \\
\midrule()
\endhead
0 & 42 & 9625 \\
1 & 100 & 233 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Based on the confusion matrix:}
\CommentTok{\#for "1=positive class" we get}
\CommentTok{\#precision TP/(TP+FP):}
\BuiltInTok{print}\NormalTok{(}\StringTok{"precision TP/(TP+FP)"}\NormalTok{, }\BuiltInTok{str}\NormalTok{(np.}\BuiltInTok{round}\NormalTok{(}\DecValTok{233}\OperatorTok{/}\NormalTok{(}\DecValTok{9625}\OperatorTok{+}\DecValTok{233}\NormalTok{),}\DecValTok{3}\NormalTok{)))}
\CommentTok{\#precision TP/(TP+FN):}
\BuiltInTok{print}\NormalTok{(}\StringTok{"recall TP/(TP+FN)"}\NormalTok{, }\BuiltInTok{str}\NormalTok{(np.}\BuiltInTok{round}\NormalTok{(}\DecValTok{233}\OperatorTok{/}\NormalTok{(}\DecValTok{100}\OperatorTok{+}\DecValTok{233}\NormalTok{),}\DecValTok{3}\NormalTok{)))}

\CommentTok{\#for "0=positive class" we get}
\CommentTok{\#precision TP/(TP+FP):}
\BuiltInTok{print}\NormalTok{(}\StringTok{"precision TP/(TP+FP)"}\NormalTok{, }\BuiltInTok{str}\NormalTok{(np.}\BuiltInTok{round}\NormalTok{(}\DecValTok{42}\OperatorTok{/}\NormalTok{(}\DecValTok{100}\OperatorTok{+}\DecValTok{42}\NormalTok{),}\DecValTok{3}\NormalTok{)))}
\CommentTok{\#precision TP/(TP+FN):}
\BuiltInTok{print}\NormalTok{(}\StringTok{"recall TP/(TP+FN)"}\NormalTok{, }\BuiltInTok{str}\NormalTok{(np.}\BuiltInTok{round}\NormalTok{(}\DecValTok{42}\OperatorTok{/}\NormalTok{(}\DecValTok{9625}\OperatorTok{+}\DecValTok{42}\NormalTok{),}\DecValTok{3}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
precision TP/(TP+FP) 0.024
recall TP/(TP+FN) 0.7
precision TP/(TP+FP) 0.296
recall TP/(TP+FN) 0.004
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est }\OperatorTok{=}\NormalTok{ smf.logit(}\StringTok{\textquotesingle{}default2 \textasciitilde{} balance\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{df)}
\BuiltInTok{print}\NormalTok{(est.fit().summary())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Optimization terminated successfully.
         Current function value: 0.079823
         Iterations 10
                           Logit Regression Results                           
==============================================================================
Dep. Variable:               default2   No. Observations:                10000
Model:                          Logit   Df Residuals:                     9998
Method:                           MLE   Df Model:                            1
Date:                Sun, 30 May 2021   Pseudo R-squ.:                  0.4534
Time:                        20:26:48   Log-Likelihood:                -798.23
converged:                       True   LL-Null:                       -1460.3
Covariance Type:            nonrobust   LLR p-value:                6.233e-290
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept    -10.6513      0.361    -29.491      0.000     -11.359      -9.943
balance        0.0055      0.000     24.952      0.000       0.005       0.006
==============================================================================

Possibly complete quasi-separation: A fraction 0.13 of observations can be
perfectly predicted. This might indicate that there is complete
quasi-separation. In this case some parameters will not be identified.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Mini Tasks: fit a logistic regression to the Titanic data}
\CommentTok{\#Try to make sense of the coefficients!}
\NormalTok{est }\OperatorTok{=}\NormalTok{ smf.logit(}\StringTok{\textquotesingle{}survived \textasciitilde{}  C(pclass) + C(sex)\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{titanic)}
\BuiltInTok{print}\NormalTok{(est.fit().summary().tables[}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Optimization terminated successfully.
         Current function value: 0.464023
         Iterations 6
==================================================================================
                     coef    std err          z      P>|z|      [0.025      0.975]
----------------------------------------------------------------------------------
Intercept          2.2971      0.219     10.490      0.000       1.868       2.726
C(pclass)[T.2]    -0.8380      0.245     -3.424      0.001      -1.318      -0.358
C(pclass)[T.3]    -1.9055      0.214     -8.898      0.000      -2.325      -1.486
C(sex)[T.male]    -2.6419      0.184    -14.350      0.000      -3.003      -2.281
==================================================================================
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pHat }\OperatorTok{=}\NormalTok{ est.fit().predict()}
\NormalTok{pred }\OperatorTok{=}\NormalTok{ pHat }\OperatorTok{\textgreater{}} \FloatTok{0.5}
\NormalTok{pd.crosstab(pred, titanic.survived)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Optimization terminated successfully.
         Current function value: 0.464023
         Iterations 6
\end{verbatim}

\begin{longtable}[]{@{}lll@{}}
\toprule()
survived & 0 & 1 \\
row\_0 & & \\
\midrule()
\endhead
False & 468 & 109 \\
True & 81 & 233 \\
\bottomrule()
\end{longtable}

\hypertarget{coefficients-as-odds}{%
\subsection{Coefficients as Odds}\label{coefficients-as-odds}}

For ``normal regression'' we know that the value of \(\beta_j\) simply
gives us \(\Delta y\) if \(x_j\) is increased by one unit.

In order to fully understand the exact meaning of the coefficients for a
LR model we need to first warm up to the definition of a \textbf{link
function} and the concept of \textbf{probability odds}.

Using linear regression as a starting point

\[
y_i = \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \ldots +\beta_k x_{k,i} + \epsilon_i
\]

we modify the right hand side such that (i) the model is still basically
a linear combination of the \(x_j\)s but (ii) the output is -like a
probability- bounded between 0 and 1. This is achieved by ``wrapping'' a
sigmoid function \(s(z) = 1/(1+exp(-z))\) around the weighted sum of the
\(x_j\)s:

\[
y_i = s(\beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \ldots +\beta_k x_{k,i} + \epsilon_i)
\]

The sigmoid function, depicted below to the left, transforms the real
axis to the interval \((0;1)\) and can be interpreted as a probability.

The inverse of the sigmoid is the \emph{logit} (depicted above to the
right), which is defined as \(log(p/(1-p))\). For the case where p is a
probability we call the ratio \(p/(1-p)\) the \textbf{probability odds}.
Thus, the logit is the log of the odds and logistic regression models
these \emph{log-odds} as a linear combination of the values of x.

Finally, we can interpret the coefficients directly: the odds of a
positive outcome are multiplied by a factor of \(exp(\beta_j)\) for
every unit change in \(x_j\). (In that light, logistic regression is
reminiscient of linear regression with logarithmically transformed
dependent variable which also leads to multiplicative rather than
additive effects.)

Summary

\[
p(x) = \frac{e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k}}{1 + e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k}}
\]

Odds

\[
\frac{p(x)}{1-p(x)} = e^{\beta_0 + \beta_1 x_1 + \ldots + \beta_k x_k}
\]

This post has a more detailed view on the interpretations of the
coefficients:

https://blog.hwr-berlin.de/codeandstats/interpretation-of-the-coefficients-in-logistic-regression/

\hypertarget{comments}{%
\subsubsection{Comments}\label{comments}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  When your data are \textbf{linearly separable} there is (ironically) a
  fitting problem ! See iris example below
\item
  \emph{Logistic regression preserves the marginal probabilities.} The
  sum of the predicted probability scores for any subgroup of the
  training data (which includes all of it) will be equal to the number
  of positives.
\item
  \emph{What is deviance ?} Deviance (also referred to as \emph{log
  loss}) is a measure of how well the model fits the data. It is 2 times
  the negative log likelihood of the dataset, given the model.
\end{enumerate}

\[
dev = - \sum_i{y_i \cdot \log p_i + (1-y_i) \cdot \log (1-p_i)}
\]

In Python, you can use the log\_loss function from scikit-learn, with
documentation found
\href{http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html}{here}.
If you think of deviance as analogous to variance, then the null
deviance is similar to the variance of the data around the average rate
of positive examples. The residual deviance is similar to the variance
of the data around the model. As an exercise we will calculate the
deviances in a homework.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Pseudo \(R^2\)}
  \href{http://thestatsgeek.com/2014/02/08/r-squared-in-logistic-regression/}{McFadden's
  \(R^2\)} is defined as \(1−LL_{mod}/LL_0\), where \(LL_{mod}\) is the
  log likelihood value for the fitted model and \(LL_{0}\) is the log
  likelihood for the null model which includes only an intercept as
  predictor (so that every individual is predicted the same probability
  of `success').

  \begin{itemize}
  \tightlist
  \item
    For a logistic regression model the log likelihood value is always
    negative (because the likelihood contribution from each observation
    is a probability between 0 and 1). If your model doesn't really
    predict the outcome better than the null model, \(LL_{mod}\) will
    not be much larger than \(LL_{0}\) , and so \(LL_{mod}/LL_0 \sim 1\)
    , and McFadden's pseudo-R2 is close to 0 (your model has no
    predictive value).
  \item
    Conversely if your model was really good, those individuals with a
    success (1) outcome would have a fitted probability close to 1, and
    vice versa for those with a failure (0) outcome. In this case if you
    go through the likelihood calculation the likelihood contribution
    from each individual for your model will be close to zero, such that
    \(LL_{mod}\) is close to zero, and McFadden's pseudo-R2 squared is
    close to 1, indicating very good predictive ability.
  \end{itemize}
\end{enumerate}

\hypertarget{roc-curves}{%
\section{ROC curves}\label{roc-curves}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fpr, tpr, thresholds }\OperatorTok{=}\NormalTok{ metrics.roc\_curve(y, scores)}
\NormalTok{roc\_display }\OperatorTok{=}\NormalTok{ RocCurveDisplay(fpr}\OperatorTok{=}\NormalTok{fpr, tpr}\OperatorTok{=}\NormalTok{tpr).plot()}
\CommentTok{\#or}

\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ RocCurveDisplay}

\NormalTok{RocCurveDisplay.from\_predictions(y\_test, y\_pred)}
\NormalTok{plt.show()}

\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ logloss(true\_label, predicted, eps}\OperatorTok{=}\FloatTok{1e{-}15}\NormalTok{):}
\NormalTok{    p }\OperatorTok{=}\NormalTok{ np.clip(predicted, eps, }\DecValTok{1} \OperatorTok{{-}}\NormalTok{ eps)}
    \ControlFlowTok{if}\NormalTok{ true\_label }\OperatorTok{==} \DecValTok{1}\NormalTok{:}
        \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{np.log(p)}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{np.log(}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ p)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p}\OperatorTok{=}\NormalTok{ np.linspace(}\FloatTok{0.001}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{100}\NormalTok{)}\CommentTok{\#.reshape({-}1,1)}
\NormalTok{plt.plot(logloss(}\DecValTok{1}\NormalTok{,p), }\StringTok{"b{-}"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture7_files/figure-pdf/cell-19-output-1.png}

}

\end{figure}

\hypertarget{think-stats-data}{%
\subsection{Think Stats Data}\label{think-stats-data}}

The NSFG dataset includes 244 variables about each pregnancy and another
3087 variables about each respondent. Maybe some of those variables have
predictive power. To find out which ones are most useful, why not try
them all? Testing the variables in the pregnancy table is easy, but in
order to use the variables in the respondent table, we have to match up
each pregnancy with a respondent. In theory we could iterate through the
rows of the pregnancy table, use the caseid to find the corresponding
respondent, and copy the values from the correspondent table into the
pregnancy table. But that would be slow.

A better option is to recognize this process as a join operation as
defined in SQL and other relational database languages
(\href{https://en.wikipedia.org/wiki/Join_(SQL)}{see}). Join is
implemented as a DataFrame method, so we can perform the operation like
this:

do not run this cell, for completeness, I show how the joined dataframe
was created:

\begin{verbatim}
from __future__ import print_function, division
import numpy as np
import nsfg

preg = pd.read_hdf('../data/pregNSFG.h5', 'df')
#only look at live births
live = preg[preg.outcome == 1]

live = live[live.prglngth>30]
resp = nsfg.ReadFemResp()
resp.index = resp.caseid
join = live.join(resp, on='caseid', rsuffix='_r')
#save to native python format:
#http://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.to_hdf.html
join.to_hdf('JoinedpregNSFG.h5', key='df', format='table',complevel =9)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{live }\OperatorTok{=}\NormalTok{ pd.read\_hdf(}\StringTok{\textquotesingle{}../data/JoinedpregNSFG.h5\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}df\textquotesingle{}}\NormalTok{)}
\NormalTok{live.head()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#define first babies}
\NormalTok{firsts }\OperatorTok{=}\NormalTok{ live[live.birthord }\OperatorTok{==} \DecValTok{1}\NormalTok{]}
\CommentTok{\#and all others:}
\NormalTok{others }\OperatorTok{=}\NormalTok{ live[live.birthord }\OperatorTok{!=} \DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# from this discussion, it seems that statsmodels still uses the defunct}
\CommentTok{\# chisqprob, so we have to define it ourselves:}
\CommentTok{\# https://github.com/statsmodels/statsmodels/issues/3931}
\ImportTok{from}\NormalTok{ scipy }\ImportTok{import}\NormalTok{ stats}
\NormalTok{stats.chisqprob }\OperatorTok{=} \KeywordTok{lambda}\NormalTok{ chisq, df: stats.chi2.sf(chisq, df)}
\NormalTok{stats.chisqprob(}\DecValTok{10}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The mother's age seems to have a small, non significant effect.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{live[}\StringTok{\textquotesingle{}boy\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ (live.babysex}\OperatorTok{==}\DecValTok{1}\NormalTok{).astype(}\BuiltInTok{int}\NormalTok{)}
\NormalTok{SexvsAge }\OperatorTok{=}\NormalTok{ smf.logit(}\StringTok{\textquotesingle{}boy \textasciitilde{} agepreg\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{live)}
\NormalTok{results }\OperatorTok{=}\NormalTok{ SexvsAge.fit()}
\BuiltInTok{print}\NormalTok{(results.summary())}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{live[}\StringTok{"fmarout5"}\NormalTok{].value\_counts()}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-trivers-willard-hypothesis}{%
\subsection{The Trivers-Willard
hypothesis}\label{the-trivers-willard-hypothesis}}

\textbf{Exercise 11.2} The Trivers-Willard hypothesis suggests that for
many mammals the sex ratio depends on ``maternal condition''; that is,
factors like the mother's age, size, health, and social status.
\href{https://en.wikipedia.org/wiki/Trivers-Willard_hypothesis}{See}.
Some studies have shown this effect among humans, but results are mixed.
As an exercise, use a data mining approach to test the other variables
in the pregnancy and respondent files.

In the solution for exercise 11.2 the author uses a data mining approach
to find the ``best'' model:

(Task: can we find out the meaning of the 2 new variables??)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula}\OperatorTok{=}\StringTok{\textquotesingle{}boy \textasciitilde{} agepreg + fmarout5==5 + infever==1\textquotesingle{}}
\NormalTok{model }\OperatorTok{=}\NormalTok{ smf.logit(formula, data}\OperatorTok{=}\NormalTok{live)}
\NormalTok{results }\OperatorTok{=}\NormalTok{ model.fit()}
\BuiltInTok{print}\NormalTok{(results.summary())}
\end{Highlighting}
\end{Shaded}

\hypertarget{tasks-5}{%
\subsection{Tasks}\label{tasks-5}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the ROC curve and AUC for the NSFG data
\item
  Use cross validation to estimate some accuracy measure of
  classification for the

  \begin{itemize}
  \tightlist
  \item
    Titanic survival
  \item
    sex prediction for the NSFG data
  \end{itemize}
\item
  Translate the coefficient for Pclass 3 into both odds and probability
  of survival (compared to the reference level Pclass 1).
\item
  Compute the survival probability of the first passenger in the data
  set.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ patsy }

\NormalTok{y, X }\OperatorTok{=}\NormalTok{ patsy.dmatrices(}\StringTok{\textquotesingle{}survived \textasciitilde{} age + C(pclass) + C(sex)\textquotesingle{}}\NormalTok{, titanic)}
\CommentTok{\#y = titanic["survived"]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ cross\_val\_score}
\NormalTok{clf }\OperatorTok{=}\NormalTok{ skl\_lm.LogisticRegression() }\CommentTok{\# solver=\textquotesingle{}newton{-}cg\textquotesingle{})}

\NormalTok{cv\_results }\OperatorTok{=}\NormalTok{ cross\_val\_score(clf, X, np.ravel(y), cv}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(np.}\BuiltInTok{round}\NormalTok{(cv\_results,}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[0.78 0.74 0.81 0.88 0.75 0.79 0.79 0.73 0.82 0.82]
\end{verbatim}

\hypertarget{the-iris-dataset}{%
\subsection{The Iris dataset}\label{the-iris-dataset}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ datasets}

\NormalTok{plt.style.use(}\StringTok{\textquotesingle{}ggplot\textquotesingle{}}\NormalTok{)}
\NormalTok{iris }\OperatorTok{=}\NormalTok{ datasets.load\_iris()}
\BuiltInTok{print}\NormalTok{(iris.data.shape)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ iris.data}
\NormalTok{y }\OperatorTok{=}\NormalTok{ iris.target}
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(X, columns}\OperatorTok{=}\NormalTok{iris.feature\_names)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ pd.plotting.scatter\_matrix(df, c }\OperatorTok{=}\NormalTok{ y, figsize }\OperatorTok{=}\NormalTok{ [}\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{],s}\OperatorTok{=}\DecValTok{150}\NormalTok{, marker }\OperatorTok{=} \StringTok{\textquotesingle{}D\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(150, 4)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture7_files/figure-pdf/cell-28-output-2.png}

}

\end{figure}

Looks like we could build a perfect classifier with just \emph{petal
width} ?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris.feature\_names}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['sepal length (cm)',
 'sepal width (cm)',
 'petal length (cm)',
 'petal width (cm)']
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df[}\StringTok{"setosa"}\NormalTok{]}\OperatorTok{=}\NormalTok{ (y}\OperatorTok{==}\DecValTok{0}\NormalTok{)}
\NormalTok{df.head()}
\NormalTok{df.columns}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Index(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',
       'petal width (cm)', 'setosa'],
      dtype='object')
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ iris[}\StringTok{"data"}\NormalTok{][:,}\DecValTok{3}\NormalTok{:]  }\CommentTok{\# petal width}

\NormalTok{logit }\OperatorTok{=}\NormalTok{ sm.Logit((iris[}\StringTok{"target"}\NormalTok{]}\OperatorTok{==}\DecValTok{0}\NormalTok{).astype(np.}\BuiltInTok{int}\NormalTok{), X)}
\NormalTok{logit.fit().params}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Optimization terminated successfully.
         Current function value: 0.354801
         Iterations 7
\end{verbatim}

\begin{verbatim}
array([-1.847])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#sklearn}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}

\KeywordTok{def}\NormalTok{ LogReg(xCol}\OperatorTok{=}\DecValTok{3}\NormalTok{, target}\OperatorTok{=}\DecValTok{2}\NormalTok{,penalty}\OperatorTok{=}\StringTok{"l2"}\NormalTok{):}
\NormalTok{    X }\OperatorTok{=}\NormalTok{ iris[}\StringTok{"data"}\NormalTok{][:,xCol:]  }\CommentTok{\# petal width}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ (iris[}\StringTok{"target"}\NormalTok{]}\OperatorTok{==}\NormalTok{target).astype(np.}\BuiltInTok{int}\NormalTok{)}

\NormalTok{    log\_reg }\OperatorTok{=}\NormalTok{ LogisticRegression(penalty}\OperatorTok{=}\NormalTok{penalty)}
\NormalTok{    log\_reg.fit(X,y)}

\NormalTok{    X\_new }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1000}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{    y\_proba }\OperatorTok{=}\NormalTok{ log\_reg.predict\_proba(X\_new)}

\NormalTok{    flowerType}\OperatorTok{=}\NormalTok{[}\StringTok{"setosa"}\NormalTok{, }\StringTok{"versicolor"}\NormalTok{, }\StringTok{"virginica"}\NormalTok{]}
 
\NormalTok{    plt.plot(X,y,}\StringTok{"b."}\NormalTok{)}
\NormalTok{    plt.plot(X\_new,y\_proba[:,}\DecValTok{1}\NormalTok{],}\StringTok{"g{-}"}\NormalTok{,label}\OperatorTok{=}\NormalTok{flowerType[target])}
\NormalTok{    plt.plot(X\_new,y\_proba[:,}\DecValTok{0}\NormalTok{],}\StringTok{"b{-}{-}"}\NormalTok{,label}\OperatorTok{=}\StringTok{"not "} \OperatorTok{+}\NormalTok{ flowerType[target])}
\NormalTok{    plt.xlabel(iris.feature\_names[xCol], fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{)}
\NormalTok{    plt.ylabel(}\StringTok{"Probability"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{)}
\NormalTok{    plt.legend(loc}\OperatorTok{=}\StringTok{"upper left"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{14}\NormalTok{)}
\NormalTok{    plt.show()}
    
    \ControlFlowTok{return}\NormalTok{ log\_reg}

\NormalTok{log\_reg }\OperatorTok{=}\NormalTok{ LogReg()}

\NormalTok{log\_reg.predict([[}\FloatTok{1.7}\NormalTok{],[}\FloatTok{1.5}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture7_files/figure-pdf/cell-32-output-1.png}

}

\end{figure}

\begin{verbatim}
array([1, 0])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log\_reg }\OperatorTok{=}\NormalTok{ LogReg(target}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture7_files/figure-pdf/cell-33-output-1.png}

}

\end{figure}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{other-classifiers}{%
\section{Other Classifiers}\label{other-classifiers}}

\hypertarget{k-nearest-neighbors}{%
\subsection{K Nearest Neighbors}\label{k-nearest-neighbors}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.neighbors }\ImportTok{import}\NormalTok{ KNeighborsClassifier}
\NormalTok{iris }\OperatorTok{=}\NormalTok{ datasets.load\_iris()}
\NormalTok{knn }\OperatorTok{=}\NormalTok{ KNeighborsClassifier(n\_neighbors}\OperatorTok{=}\DecValTok{6}\NormalTok{)}
\NormalTok{knn.fit(iris[}\StringTok{\textquotesingle{}data\textquotesingle{}}\NormalTok{], iris[}\StringTok{\textquotesingle{}target\textquotesingle{}}\NormalTok{])}

\end{Highlighting}
\end{Shaded}

\begin{verbatim}
KNeighborsClassifier(n_neighbors=6)
\end{verbatim}

Task: Split the iris data into training and test. Predict on test

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#prediction = knn.predict(X\_test)}
\CommentTok{\#print(\textquotesingle{}Prediction \{\}’.format(prediction))}
\end{Highlighting}
\end{Shaded}

\hypertarget{multinomial-logistic-regression}{%
\subsection{Multinomial Logistic
Regression}\label{multinomial-logistic-regression}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ statsmodels.api }\ImportTok{as}\NormalTok{ st}
\CommentTok{\#different way of importing data}
\NormalTok{iris }\OperatorTok{=}\NormalTok{ st.datasets.get\_rdataset(}\StringTok{\textquotesingle{}iris\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}datasets\textquotesingle{}}\NormalTok{)}
 
\NormalTok{y }\OperatorTok{=}\NormalTok{ iris.data.Species}
 
\NormalTok{y.head(}\DecValTok{3}\NormalTok{)}

\NormalTok{x }\OperatorTok{=}\NormalTok{ iris.data.iloc[:, }\DecValTok{0}\NormalTok{]}
 
\NormalTok{x }\OperatorTok{=}\NormalTok{ st.add\_constant(x, prepend }\OperatorTok{=} \VariableTok{False}\NormalTok{)}
 
\NormalTok{x.head()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule()
& Sepal.Length & const \\
\midrule()
\endhead
0 & 5.1 & 1.0 \\
1 & 4.9 & 1.0 \\
2 & 4.7 & 1.0 \\
3 & 4.6 & 1.0 \\
4 & 5.0 & 1.0 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mdl }\OperatorTok{=}\NormalTok{ st.MNLogit(y, x)}
 
\NormalTok{mdl\_fit }\OperatorTok{=}\NormalTok{ mdl.fit()}

\BuiltInTok{print}\NormalTok{(mdl\_fit.summary())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Optimization terminated successfully.
         Current function value: 0.606893
         Iterations 8
                          MNLogit Regression Results                          
==============================================================================
Dep. Variable:                Species   No. Observations:                  150
Model:                        MNLogit   Df Residuals:                      146
Method:                           MLE   Df Model:                            2
Date:                Sun, 30 May 2021   Pseudo R-squ.:                  0.4476
Time:                        20:27:58   Log-Likelihood:                -91.034
converged:                       True   LL-Null:                       -164.79
Covariance Type:            nonrobust   LLR p-value:                 9.276e-33
=====================================================================================
Species=versicolor       coef    std err          z      P>|z|      [0.025      0.975]
--------------------------------------------------------------------------------------
Sepal.Length           4.8157      0.907      5.310      0.000       3.038       6.593
const                -26.0819      4.889     -5.335      0.000     -35.665     -16.499
--------------------------------------------------------------------------------------
Species=virginica       coef    std err          z      P>|z|      [0.025      0.975]
-------------------------------------------------------------------------------------
Sepal.Length          6.8464      1.022      6.698      0.000       4.843       8.850
const               -38.7590      5.691     -6.811      0.000     -49.913     -27.605
=====================================================================================
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\#\# marginal effects }\AlertTok{\#\#\#}
 
\NormalTok{mdl\_margeff }\OperatorTok{=}\NormalTok{ mdl\_fit.get\_margeff()}
 
\BuiltInTok{print}\NormalTok{(mdl\_margeff.summary())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       MNLogit Marginal Effects      
=====================================
Dep. Variable:                Species
Method:                          dydx
At:                           overall
=====================================================================================
    Species=setosa      dy/dx    std err          z      P>|z|      [0.025      0.975]
--------------------------------------------------------------------------------------
Sepal.Length          -0.3785      0.003   -116.793      0.000      -0.385      -0.372
--------------------------------------------------------------------------------------
Species=versicolor      dy/dx    std err          z      P>|z|      [0.025      0.975]
--------------------------------------------------------------------------------------
Sepal.Length           0.0611      0.022      2.778      0.005       0.018       0.104
--------------------------------------------------------------------------------------
Species=virginica      dy/dx    std err          z      P>|z|      [0.025      0.975]
-------------------------------------------------------------------------------------
Sepal.Length          0.3173      0.022     14.444      0.000       0.274       0.360
=====================================================================================
\end{verbatim}

\bookmarksetup{startatroot}

\hypertarget{advanced-topics-1}{%
\chapter{Advanced Topics}\label{advanced-topics-1}}

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{\%}\NormalTok{precision }\DecValTok{3}

\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib }\ImportTok{as}\NormalTok{ mpl}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\ImportTok{from}\NormalTok{ sklearn.neighbors }\ImportTok{import}\NormalTok{ KNeighborsClassifier}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}
\ImportTok{from}\NormalTok{ sklearn }\ImportTok{import}\NormalTok{ datasets}

\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'%.3f'
\end{verbatim}

\hypertarget{k-nearest-neighbors-knn}{%
\subsection{K-Nearest Neighbors (KNN)}\label{k-nearest-neighbors-knn}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\OperatorTok{=}\NormalTok{ datasets.load\_iris()}
\NormalTok{knn }\OperatorTok{=}\NormalTok{ KNeighborsClassifier(n\_neighbors}\OperatorTok{=}\DecValTok{6}\NormalTok{)}
\NormalTok{knn.fit(iris[}\StringTok{\textquotesingle{}data\textquotesingle{}}\NormalTok{], iris[}\StringTok{\textquotesingle{}target\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
KNeighborsClassifier(n_neighbors=6)
\end{verbatim}

Task: Split the iris data into training and test. Predict on test

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Split into training and test set}
\NormalTok{X }\OperatorTok{=}\NormalTok{ iris[}\StringTok{\textquotesingle{}data\textquotesingle{}}\NormalTok{]}
\NormalTok{y }\OperatorTok{=}\NormalTok{ iris[}\StringTok{\textquotesingle{}target\textquotesingle{}}\NormalTok{]}
\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(X, y, test\_size }\OperatorTok{=} \FloatTok{0.3}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{)}

\CommentTok{\# Create a k{-}NN classifier with 7 neighbors: knn}
\NormalTok{knn }\OperatorTok{=}\NormalTok{ KNeighborsClassifier(n\_neighbors}\OperatorTok{=}\DecValTok{6}\NormalTok{)}

\CommentTok{\# Fit the classifier to the training data}
\NormalTok{knn.fit(X\_train, y\_train)}

\CommentTok{\# Print the accuracy}
\BuiltInTok{print}\NormalTok{(knn.score(X\_test, y\_test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
1.0
\end{verbatim}

Simple Idea, no modeling assumptions at all !! Think about the
following:

\begin{itemize}
\tightlist
\item
  What is ``the model'', i.e.~what needs to be stored ? (coefficients,
  functions, \ldots)
\item
  What is the model complexity ?
\item
  Does this only work for classification ? What would be the regression
  analogy ?
\item
  What improvements could we make to the simple idea ?
\item
  In the modeling world:

  \begin{itemize}
  \tightlist
  \item
    linear ?
  \item
    local vs.~global
  \item
    memory/CPU requirements
  \item
    wide versus tall data ?
  \end{itemize}
\end{itemize}

\hypertarget{handwritten-digits}{%
\subsubsection{Handwritten Digits}\label{handwritten-digits}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{digits }\OperatorTok{=}\NormalTok{ datasets.load\_digits()}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\CommentTok{\#how can I improve the plots ? (i..e no margins, box around plot...)}
\NormalTok{plt.figure(}\DecValTok{1}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ np.arange(}\DecValTok{10}\NormalTok{)}\OperatorTok{+}\DecValTok{1}\NormalTok{:}
\NormalTok{    plt.subplot(}\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, i)}
\NormalTok{    plt.axis(}\StringTok{\textquotesingle{}off\textquotesingle{}}\NormalTok{)}
    \CommentTok{\#plt.gray() }
    \CommentTok{\#plt.matshow(digits.images[i{-}1]) }
\NormalTok{    plt.imshow(digits.images[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{], cmap}\OperatorTok{=}\NormalTok{plt.cm.gray\_r, interpolation}\OperatorTok{=}\StringTok{\textquotesingle{}nearest\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.show() }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture8_files/figure-pdf/cell-5-output-1.png}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create feature and target arrays}
\NormalTok{X }\OperatorTok{=}\NormalTok{ digits.data}
\NormalTok{y }\OperatorTok{=}\NormalTok{ digits.target}

\CommentTok{\# Split into training and test set}
\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(X, y, test\_size }\OperatorTok{=} \FloatTok{0.2}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{, stratify}\OperatorTok{=}\NormalTok{y)}

\CommentTok{\# Create a k{-}NN classifier with 7 neighbors: knn}
\NormalTok{knn }\OperatorTok{=}\NormalTok{ KNeighborsClassifier(n\_neighbors}\OperatorTok{=}\DecValTok{7}\NormalTok{)}

\CommentTok{\# Fit the classifier to the training data}
\NormalTok{knn.fit(X\_train, y\_train)}

\CommentTok{\# Print the accuracy}
\BuiltInTok{print}\NormalTok{(knn.score(X\_test, y\_test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.9833333333333333
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Big Confusion Matrix}
\NormalTok{preds }\OperatorTok{=}\NormalTok{ knn.predict(X\_test)}

\NormalTok{pd.crosstab(preds,y\_test)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllllllllll@{}}
\toprule()
col\_0 & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
row\_0 & & & & & & & & & & \\
\midrule()
\endhead
0 & 36 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
1 & 0 & 36 & 0 & 0 & 0 & 0 & 0 & 0 & 2 & 0 \\
2 & 0 & 0 & 35 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
3 & 0 & 0 & 0 & 37 & 0 & 0 & 0 & 0 & 0 & 0 \\
4 & 0 & 0 & 0 & 0 & 36 & 0 & 0 & 0 & 0 & 1 \\
5 & 0 & 0 & 0 & 0 & 0 & 37 & 0 & 0 & 0 & 0 \\
6 & 0 & 0 & 0 & 0 & 0 & 0 & 35 & 0 & 0 & 0 \\
7 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 36 & 1 & 0 \\
8 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 32 & 1 \\
9 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 34 \\
\bottomrule()
\end{longtable}

\textbf{Task}

Construct a model complexity curve for the digits dataset! In this
exercise, you will compute and plot the training and testing accuracy
scores for a variety of different neighbor values. By observing how the
accuracy scores differ for the training and testing sets with different
values of k, you will develop your intuition for overfitting and
underfitting.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# \# Setup arrays to store train and test accuracies}
\CommentTok{\# neighbors = np.arange(1, 20)}
\CommentTok{\# train\_accuracy = np.empty(len(neighbors))}
\CommentTok{\# test\_accuracy = np.empty(len(neighbors))}
\CommentTok{\# }
\CommentTok{\# \# Loop over different values of k}
\CommentTok{\# for i, k in enumerate(neighbors):}
\CommentTok{\#     \# Setup a k{-}NN Classifier with k neighbors: knn}
\CommentTok{\#     knn = KNeighborsClassifier(\_\_\_)}
\CommentTok{\# }
\CommentTok{\#     \# Fit the classifier to the training data}
\CommentTok{\#     \_\_\_}
\CommentTok{\#     }
\CommentTok{\#     \#Compute accuracy on the training set}
\CommentTok{\#     train\_accuracy[i] = \_\_\_}
\CommentTok{\# }
\CommentTok{\#     \#Compute accuracy on the testing set}
\CommentTok{\#     test\_accuracy[i] = \_\_\_}
\CommentTok{\# }
\CommentTok{\# \# Generate plot}
\CommentTok{\# plt.title(\textquotesingle{}k{-}NN: Varying Number of Neighbors\textquotesingle{})}
\CommentTok{\# plt.plot(\_\_\_, label = \textquotesingle{}Testing Accuracy\textquotesingle{})}
\CommentTok{\# plt.plot(\_\_\_, label = \textquotesingle{}Training Accuracy\textquotesingle{})}
\CommentTok{\# plt.legend()}
\CommentTok{\# plt.xlabel(\textquotesingle{}Number of Neighbors\textquotesingle{})}
\CommentTok{\# plt.ylabel(\textquotesingle{}Accuracy\textquotesingle{})}
\CommentTok{\# plt.show()}
\end{Highlighting}
\end{Shaded}

\hypertarget{multinomial-logistic-regression-1}{%
\subsection{Multinomial Logistic
Regression}\label{multinomial-logistic-regression-1}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LogisticRegression}
\NormalTok{iris }\OperatorTok{=}\NormalTok{ datasets.load\_iris()}

\NormalTok{log\_reg }\OperatorTok{=}\NormalTok{ LogisticRegression(multi\_class}\OperatorTok{=}\StringTok{\textquotesingle{}multinomial\textquotesingle{}}\NormalTok{,solver}\OperatorTok{=}\StringTok{\textquotesingle{}sag\textquotesingle{}}\NormalTok{, max\_iter}\OperatorTok{=}\DecValTok{100}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{)}
\NormalTok{log\_reg.fit(iris[}\StringTok{"data"}\NormalTok{][:,}\DecValTok{3}\NormalTok{:],iris[}\StringTok{"target"}\NormalTok{])}
    
\NormalTok{preds }\OperatorTok{=}\NormalTok{ log\_reg.predict\_proba(iris[}\StringTok{"data"}\NormalTok{][:,}\DecValTok{3}\NormalTok{:])}

\NormalTok{preds[}\DecValTok{1}\NormalTok{:}\DecValTok{5}\NormalTok{,:]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[9.259e-01, 7.396e-02, 1.788e-04],
       [9.259e-01, 7.396e-02, 1.788e-04],
       [9.259e-01, 7.396e-02, 1.788e-04],
       [9.259e-01, 7.396e-02, 1.788e-04]])
\end{verbatim}

\textbf{Task}: Compute a confusion matrix

\textbf{Further Reading and Explorations:} * Read about ``one versus
all'' pitted against multinomial. Check out this
\href{plot_logistic_multinomial.ipynb}{notebook}. * Read about
\href{http://data.princeton.edu/wws509/stata/mlogit.html}{marginal
probabilities}.

\hypertarget{roc-curves-1}{%
\subsection{ROC Curves}\label{roc-curves-1}}

Simplest to go back to 2 labels from lesson 7:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris }\OperatorTok{=}\NormalTok{ datasets.load\_iris()}
\NormalTok{X }\OperatorTok{=}\NormalTok{ iris[}\StringTok{"data"}\NormalTok{][:,}\DecValTok{3}\NormalTok{:]}
\NormalTok{y }\OperatorTok{=}\NormalTok{ (iris[}\StringTok{"target"}\NormalTok{]}\OperatorTok{==}\DecValTok{2}\NormalTok{).astype(}\BuiltInTok{int}\NormalTok{)}
\CommentTok{\# Split into training and test set}
\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(X, y, test\_size }\OperatorTok{=} \FloatTok{0.3}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{)}

\NormalTok{log\_reg }\OperatorTok{=}\NormalTok{ LogisticRegression()}
\NormalTok{log\_reg.fit(X\_train,y\_train)}

\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ log\_reg.predict\_proba(X\_test) }
\NormalTok{y\_pred\_prob }\OperatorTok{=}\NormalTok{ y\_pred[:,}\DecValTok{1}\NormalTok{]}

\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ classification\_report}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ confusion\_matrix}

\BuiltInTok{print}\NormalTok{(confusion\_matrix(y\_test, y\_pred\_prob }\OperatorTok{\textgreater{}} \FloatTok{0.25}\NormalTok{ ))}

\BuiltInTok{print}\NormalTok{(confusion\_matrix(y\_test, y\_pred\_prob }\OperatorTok{\textgreater{}} \FloatTok{0.5}\NormalTok{ ))}

\BuiltInTok{print}\NormalTok{(confusion\_matrix(y\_test, y\_pred\_prob }\OperatorTok{\textgreater{}} \FloatTok{0.75}\NormalTok{ ))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[24  8]
 [ 0 13]]
[[32  0]
 [ 0 13]]
[[32  0]
 [ 4  9]]
\end{verbatim}

\textbf{The need for more sophisticated metrics than accuracy and single
thresholding}

This is particularly relevant for imbalanced classes, example: Emails

\begin{itemize}
\tightlist
\item
  Spam classification

  \begin{itemize}
  \tightlist
  \item
    99\% of emails are real; 1\% of emails are spam
  \end{itemize}
\item
  Could build a classifier that predicts ALL emails as real

  \begin{itemize}
  \tightlist
  \item
    99\% accurate!
  \item
    But horrible at actually classifying spam
  \item
    Fails at its original purpose
  \end{itemize}
\end{itemize}

\textbf{Metrics from CM}

\begin{itemize}
\tightlist
\item
  Precision: \(\frac{TP}{TP+FP}\)
\item
  Recall: \(\frac{TP}{TP+FN}\)
\item
  F1 score:
  \(2 \cdot \frac{precision \cdot recall}{precision + recall}\)
\end{itemize}

The F1 score is the harmonic average of the precision and recall, where
an F1 score reaches its best value at 1 (perfect precision and recall)
and worst at 0. () - High precision: Not many real emails predicted as
spam - High recall: Predicted most spam emails correctly

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Example:}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(classification\_report(y\_test, y\_pred\_prob }\OperatorTok{\textgreater{}} \FloatTok{0.5}\NormalTok{ ))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        32
           1       1.00      1.00      1.00        13

    accuracy                           1.00        45
   macro avg       1.00      1.00      1.00        45
weighted avg       1.00      1.00      1.00        45
\end{verbatim}

But we still need to fix a threshold for any of the metrics above to
work !

Wouldn't it be best to communicate the prediction quality over a wide
range (all!) of thresholds and enable the user to choose the most
suitable one for his/her application ! That is the idea of the
\textbf{Receiver Operating Characteristic (ROC)} curve!

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#The pima{-}indians{-} diabetes data:}
\CommentTok{\#  https://www.kaggle.com/uciml/pima{-}indians{-}diabetes{-}database\#diabetes.csv}

\NormalTok{col\_names }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}pregnant\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}glucose\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}bp\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}skin\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}insulin\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}bmi\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}pedigree\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}label\textquotesingle{}}\NormalTok{]}
\CommentTok{\# load dataset}
\NormalTok{pima }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"../data/diabetes.csv"}\NormalTok{, header}\OperatorTok{=}\VariableTok{None}\NormalTok{, names}\OperatorTok{=}\NormalTok{col\_names,skiprows}\OperatorTok{=}\NormalTok{[}\DecValTok{0}\NormalTok{])}
\NormalTok{pima.head()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llllllllll@{}}
\toprule()
& pregnant & glucose & bp & skin & insulin & bmi & pedigree & age &
label \\
\midrule()
\endhead
0 & 6 & 148 & 72 & 35 & 0 & 33.6 & 0.627 & 50 & 1 \\
1 & 1 & 85 & 66 & 29 & 0 & 26.6 & 0.351 & 31 & 0 \\
2 & 8 & 183 & 64 & 0 & 0 & 23.3 & 0.672 & 32 & 1 \\
3 & 1 & 89 & 66 & 23 & 94 & 28.1 & 0.167 & 21 & 0 \\
4 & 0 & 137 & 40 & 35 & 168 & 43.1 & 2.288 & 33 & 1 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pima.info()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 768 entries, 0 to 767
Data columns (total 9 columns):
 #   Column    Non-Null Count  Dtype  
---  ------    --------------  -----  
 0   pregnant  768 non-null    int64  
 1   glucose   768 non-null    int64  
 2   bp        768 non-null    int64  
 3   skin      768 non-null    int64  
 4   insulin   768 non-null    int64  
 5   bmi       768 non-null    float64
 6   pedigree  768 non-null    float64
 7   age       768 non-null    int64  
 8   label     768 non-null    int64  
dtypes: float64(2), int64(7)
memory usage: 54.1 KB
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a }\OperatorTok{=}\NormalTok{ [}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{]}
\NormalTok{b }\OperatorTok{=}\NormalTok{ [}\OperatorTok{*}\NormalTok{a]}
\NormalTok{b}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1, 2, 3]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#split dataset in features and target variable}
\NormalTok{feature\_cols }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}pregnant\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}insulin\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}bmi\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}glucose\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}bp\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}pedigree\textquotesingle{}}\NormalTok{]}
\NormalTok{X }\OperatorTok{=}\NormalTok{ pima[feature\_cols] }\CommentTok{\# Features}
\NormalTok{y }\OperatorTok{=}\NormalTok{ pima.label }\CommentTok{\# Target variable}

\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(X, y, test\_size }\OperatorTok{=} \FloatTok{0.3}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log\_reg }\OperatorTok{=}\NormalTok{ LogisticRegression(max\_iter}\OperatorTok{=}\DecValTok{200}\NormalTok{)}
\NormalTok{log\_reg.fit(X\_train,y\_train)}

\NormalTok{y\_pred }\OperatorTok{=}\NormalTok{ log\_reg.predict\_proba(X\_test) }
\NormalTok{y\_pred\_prob }\OperatorTok{=}\NormalTok{ y\_pred[:,}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ roc\_curve}

\NormalTok{fpr, tpr, thresholds }\OperatorTok{=}\NormalTok{ roc\_curve(y\_test, y\_pred\_prob)}
\NormalTok{plt.plot([}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], }\StringTok{\textquotesingle{}k{-}{-}\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.plot(fpr, tpr, label}\OperatorTok{=}\StringTok{\textquotesingle{}Logistic Regression\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}False Positive Rate\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}True Positive Rate\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{\textquotesingle{}Logistic Regression ROC Curve\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.show()}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture8_files/figure-pdf/cell-18-output-1.png}

}

\end{figure}

\textbf{Area under the ROC curve (AUC)}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ roc\_auc\_score}

\NormalTok{roc\_auc\_score(y\_test, y\_pred\_prob)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.7969370860927152
\end{verbatim}

\textbf{AUC using cross-validation}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ cross\_val\_score}

\NormalTok{cv\_scores }\OperatorTok{=}\NormalTok{ cross\_val\_score(log\_reg, X, y, cv}\OperatorTok{=}\DecValTok{10}\NormalTok{, scoring}\OperatorTok{=}\StringTok{\textquotesingle{}roc\_auc\textquotesingle{}}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(cv\_scores)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[0.789 0.83  0.838 0.773 0.812 0.848 0.819 0.921 0.857 0.833]
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{regularized-regression}{%
\subsection{Regularized Regression}\label{regularized-regression}}

We will illustrate the concepts on the Boston housing data set

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../data/boston.csv\textquotesingle{}}\NormalTok{)}
\CommentTok{\#print(boston.head())}
\NormalTok{X }\OperatorTok{=}\NormalTok{ boston.drop(}\StringTok{\textquotesingle{}medv\textquotesingle{}}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{).values}
\NormalTok{y }\OperatorTok{=}\NormalTok{ boston[}\StringTok{\textquotesingle{}medv\textquotesingle{}}\NormalTok{].values}
\end{Highlighting}
\end{Shaded}

\textbf{Variable Selection}

\href{../figures/model_selection.pdf}{ISLR slides on model selection}

\hypertarget{l2-regression}{%
\subsubsection{L2 Regression}\label{l2-regression}}

Recall: Linear regression minimizes a loss function - It chooses a
coefficient for each feature variable - Large coefficients can lead to
overfitting - Penalizing large coefficients: Regularization

\textbf{Detour: \(L_p\) norms}

http://mathworld.wolfram.com/VectorNorm.html

Our new penalty term in finding the coefficients \(\beta_j\) is the
minimization

\[
\sum_{i=1}^n{\left( y_i - \beta_0 - \sum_{j=1}^p{\beta_j x_{ij}} \right)^2} + \lambda \sum_{j=1}^p{\beta_j^2} = RSS + \lambda \sum_{j=1}^p{\beta_j^2}
\]

Instead of obtaining \textbf{one} set of coefficients we have a
dependency of \(\beta_j\) on \(\lambda\):

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ Ridge}
\NormalTok{X\_train, X\_test, y\_train, y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(X, y,test\_size }\OperatorTok{=} \FloatTok{0.3}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{42}\NormalTok{)}

\NormalTok{ridge }\OperatorTok{=}\NormalTok{ Ridge(alpha}\OperatorTok{=}\FloatTok{0.1}\NormalTok{, normalize}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{ridge.fit(X\_train, y\_train)}
\NormalTok{ridge\_pred }\OperatorTok{=}\NormalTok{ ridge.predict(X\_test)}\CommentTok{\#?does this automatically take care of nromalization ??}
\CommentTok{\#Returns the coefficient of determination R\^{}2 of the prediction.}
\NormalTok{ridge.score(X\_test, y\_test)}
\NormalTok{ridge\_pred[}\DecValTok{0}\NormalTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([27.97 , 35.383, 16.826, 25.145, 18.749])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge2 }\OperatorTok{=}\NormalTok{ Ridge(alpha}\OperatorTok{=}\FloatTok{0.1}\NormalTok{, normalize}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{ridge2.fit(X\_train, y\_train)}
\NormalTok{ridge2\_pred }\OperatorTok{=}\NormalTok{ ridge2.predict(X\_test)}
\NormalTok{ridge2\_pred[}\DecValTok{0}\NormalTok{:}\DecValTok{5}\NormalTok{]}
\NormalTok{ridge2.coef\_}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([-1.325e-01,  3.599e-02,  4.385e-02,  3.096e+00, -1.406e+01,
        4.061e+00, -1.200e-02, -1.365e+00,  2.395e-01, -8.817e-03,
       -8.955e-01,  1.183e-02, -5.498e-01])
\end{verbatim}

\hypertarget{l1-regression}{%
\subsubsection{L1 Regression}\label{l1-regression}}

Our penalty termy looks slightly different (with big consequences for
\textbf{sparsity})

\[
\sum_{i=1}^n{\left( y_i - \beta_0 - \sum_{j=1}^p{\beta_j x_{ij}} \right)^2} + \lambda \sum_{j=1}^p{| \beta_j |} = RSS + \lambda \sum_{j=1}^p{| \beta_j |}
\]

(Comment: \emph{LASSO} = ``least absolute shrinkage and selection
operator'')

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ Lasso}

\NormalTok{lasso }\OperatorTok{=}\NormalTok{ Lasso(alpha}\OperatorTok{=}\FloatTok{0.1}\NormalTok{, normalize}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{lasso.fit(X\_train, y\_train)}
\NormalTok{lasso\_pred }\OperatorTok{=}\NormalTok{ lasso.predict(X\_test)}
\CommentTok{\#Returns the coefficient of determination R\^{}2 of the prediction.}
\NormalTok{lasso.score(X\_test, y\_test)}
\NormalTok{lasso.coef\_}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([-0.   ,  0.   , -0.   ,  0.   , -0.   ,  3.189, -0.   , -0.   ,
       -0.   , -0.   , -0.307,  0.   , -0.487])
\end{verbatim}

\textbf{Feature Selection Property of the LASSO}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{names }\OperatorTok{=}\NormalTok{ boston.drop(}\StringTok{\textquotesingle{}medv\textquotesingle{}}\NormalTok{, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{).columns}
\NormalTok{lasso\_coef }\OperatorTok{=}\NormalTok{ lasso.fit(X, y).coef\_}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.plot(}\BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(names)), lasso\_coef)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.xticks(}\BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(names)), names, rotation}\OperatorTok{=}\DecValTok{60}\NormalTok{)}
\NormalTok{\_ }\OperatorTok{=}\NormalTok{ plt.ylabel(}\StringTok{\textquotesingle{}Coefficients\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture8_files/figure-pdf/cell-25-output-1.png}

}

\end{figure}

\textbf{Tuning the shrinkage parameter with CV}

(It seems that \(\lambda\) is often referred to as \(\alpha\))

From sklearn.linear\_model:

\begin{itemize}
\tightlist
\item
  LassoCV
\item
  RidgeCV
\item
  GridSearchCV
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ RidgeCV}

\NormalTok{reg }\OperatorTok{=}\NormalTok{ RidgeCV(alphas}\OperatorTok{=}\NormalTok{[}\FloatTok{1e{-}3}\NormalTok{, }\FloatTok{1e{-}2}\NormalTok{, }\FloatTok{1e{-}1}\NormalTok{, }\DecValTok{1}\NormalTok{],store\_cv\_values }\OperatorTok{=} \VariableTok{True}\NormalTok{)}
\NormalTok{reg.fit(X, y)}
\NormalTok{reg.score(X, y)}

\NormalTok{reg.cv\_values\_}
\CommentTok{\#Open Questions: }
\CommentTok{\#1. how to extract all scores, possibly even the individual folds?}
\CommentTok{\#2. What is the optimal alpha ??}
\CommentTok{\#reg.cv\_values\_}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[ 37.303,  37.347,  37.766,  40.434],
       [ 11.997,  11.974,  11.757,  10.501],
       [ 17.488,  17.492,  17.532,  17.786],
       ...,
       [ 14.642,  14.676,  15.003,  17.073],
       [ 17.862,  17.902,  18.281,  20.683],
       [113.709, 113.813, 114.793, 120.997]])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LassoCV}

\NormalTok{reg }\OperatorTok{=}\NormalTok{ LassoCV(cv}\OperatorTok{=}\DecValTok{5}\NormalTok{, random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{).fit(X, y)}
\NormalTok{reg.score(X, y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.7024437179872696
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ KFold}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ GridSearchCV}

\NormalTok{diabetes }\OperatorTok{=}\NormalTok{ datasets.load\_diabetes()}
\NormalTok{X }\OperatorTok{=}\NormalTok{ diabetes.data[:}\DecValTok{150}\NormalTok{]}
\NormalTok{y }\OperatorTok{=}\NormalTok{ diabetes.target[:}\DecValTok{150}\NormalTok{]}


\NormalTok{lasso }\OperatorTok{=}\NormalTok{ Lasso(random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{,max\_iter}\OperatorTok{=}\DecValTok{2000}\NormalTok{)}
\CommentTok{\#logarithmically spaced sequence}
\NormalTok{alphas }\OperatorTok{=}\NormalTok{ np.logspace(}\OperatorTok{{-}}\DecValTok{4}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\DecValTok{20}\NormalTok{)}

\NormalTok{tuned\_parameters }\OperatorTok{=}\NormalTok{ [\{}\StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{: alphas\}]}
\NormalTok{n\_folds }\OperatorTok{=} \DecValTok{5}

\NormalTok{reg }\OperatorTok{=}\NormalTok{ GridSearchCV(lasso, tuned\_parameters, cv}\OperatorTok{=}\NormalTok{n\_folds, refit}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{reg.fit(X, y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
GridSearchCV(cv=5, estimator=Lasso(max_iter=2000, random_state=0),
             param_grid=[{'alpha': array([1.000e-04, 1.528e-04, 2.336e-04, 3.570e-04, 5.456e-04, 8.338e-04,
       1.274e-03, 1.947e-03, 2.976e-03, 4.549e-03, 6.952e-03, 1.062e-02,
       1.624e-02, 2.482e-02, 3.793e-02, 5.796e-02, 8.859e-02, 1.354e-01,
       2.069e-01, 3.162e-01])}],
             refit=False)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scores }\OperatorTok{=}\NormalTok{ reg.cv\_results\_[}\StringTok{\textquotesingle{}mean\_test\_score\textquotesingle{}}\NormalTok{]}
\NormalTok{scores\_std }\OperatorTok{=}\NormalTok{ reg.cv\_results\_[}\StringTok{\textquotesingle{}std\_test\_score\textquotesingle{}}\NormalTok{]}
\NormalTok{plt.figure().set\_size\_inches(}\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\NormalTok{plt.semilogx(alphas, scores)}

\CommentTok{\# plot error lines showing +/{-} std. errors of the scores}
\NormalTok{std\_error }\OperatorTok{=}\NormalTok{ scores\_std }\OperatorTok{/}\NormalTok{ np.sqrt(n\_folds)}

\NormalTok{plt.semilogx(alphas, scores }\OperatorTok{+}\NormalTok{ std\_error, }\StringTok{\textquotesingle{}b{-}{-}\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.semilogx(alphas, scores }\OperatorTok{{-}}\NormalTok{ std\_error, }\StringTok{\textquotesingle{}b{-}{-}\textquotesingle{}}\NormalTok{)}

\CommentTok{\# alpha=0.2 controls the translucency of the fill color}
\NormalTok{plt.fill\_between(alphas, scores }\OperatorTok{+}\NormalTok{ std\_error, scores }\OperatorTok{{-}}\NormalTok{ std\_error, alpha}\OperatorTok{=}\FloatTok{0.2}\NormalTok{)}

\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}CV score +/{-} std error\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}alpha\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.axhline(np.}\BuiltInTok{max}\NormalTok{(scores), linestyle}\OperatorTok{=}\StringTok{\textquotesingle{}{-}{-}\textquotesingle{}}\NormalTok{, color}\OperatorTok{=}\StringTok{\textquotesingle{}.5\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlim([alphas[}\DecValTok{0}\NormalTok{], alphas[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(0.0001, 0.31622776601683794)
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{./DS_Lecture8_files/figure-pdf/cell-29-output-2.png}

}

\end{figure}

\textbf{How much can you trust the selection of alpha?}

Task: Find the opimal Alpha parameters (maximising the generalization
score) on different subsets of the data

\hypertarget{elasticnet}{%
\subsubsection{ElasticNet}\label{elasticnet}}

\textbf{The mystery of the additional \(\alpha\) paramater}

\begin{itemize}
\tightlist
\item
  The elastic net for correlated variables, which uses a penalty that is
  part L1, part L2.
\item
  Compromise between the ridge regression penalty \((\alpha = 0)\) and
  the lasso penalty \((\alpha = 1)\).
\item
  This penalty is particularly useful in the \(p >> N\) situation, or
  any situation where there are many correlated predictor variables.
\end{itemize}

\$ RSS + \lambda \sum\_\{j=1\}\^{}p\{ \left( \frac{1}{2} (1-\alpha)
\beta\_j\^{}2 + \alpha \textbar{} \beta\_j \textbar{} \right)\} \$

The right hand side can be written as

\(\sum_{j=1}^p{ \frac{1}{2} \lambda (1-\alpha) \beta_j^2 + \alpha \lambda | \beta_j |} = \sum_{j=1}^p{ \lambda_R \beta_j^2 + \lambda_L | \beta_j |}\)

with the Ridge penalty parameter
\(\lambda_R \equiv \frac{1}{2} \lambda (1-\alpha)\) and the lasso
penalty parameter \(\lambda_L \equiv \alpha \lambda\).

So we see that with

\(\alpha = \frac{\lambda_L}{\lambda_L+ 2 \lambda_R}, \text{ and } \lambda= \lambda_L+ 2 \lambda_R\)

Further Reading: - this
\href{plot_lasso_coordinate_descent_path.ipynb}{notebook} shows how to
plot the entire ``path'' of coefficients.

\hypertarget{kaggle}{%
\subsection{Kaggle}\label{kaggle}}

\hypertarget{housing-data}{%
\subsubsection{\texorpdfstring{\href{https://www.kaggle.com/c/house-prices-advanced-regression-techniques}{Housing
Data}}{Housing Data}}\label{housing-data}}

This \href{../data/kaggle/HousePrices/EDA.ipynb}{notebook} (despite its
annoying ``humor'') is a good start. (Get it
\href{https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python}{directly}
from kaggle)

\hypertarget{your-first-submission}{%
\subsubsection{Your first submission}\label{your-first-submission}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_train }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../data/kaggle/HousePrices/train.csv\textquotesingle{}}\NormalTok{)}
\NormalTok{df\_test }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}../data/kaggle/HousePrices/test.csv\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_train}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llllllllllllllllllllll@{}}
\toprule()
& Id & MSSubClass & MSZoning & LotFrontage & LotArea & Street & Alley &
LotShape & LandContour & Utilities & ... & PoolArea & PoolQC & Fence &
MiscFeature & MiscVal & MoSold & YrSold & SaleType & SaleCondition &
SalePrice \\
\midrule()
\endhead
0 & 1 & 60 & RL & 65.0 & 8450 & Pave & NaN & Reg & Lvl & AllPub & ... &
0 & NaN & NaN & NaN & 0 & 2 & 2008 & WD & Normal & 208500 \\
1 & 2 & 20 & RL & 80.0 & 9600 & Pave & NaN & Reg & Lvl & AllPub & ... &
0 & NaN & NaN & NaN & 0 & 5 & 2007 & WD & Normal & 181500 \\
2 & 3 & 60 & RL & 68.0 & 11250 & Pave & NaN & IR1 & Lvl & AllPub & ... &
0 & NaN & NaN & NaN & 0 & 9 & 2008 & WD & Normal & 223500 \\
3 & 4 & 70 & RL & 60.0 & 9550 & Pave & NaN & IR1 & Lvl & AllPub & ... &
0 & NaN & NaN & NaN & 0 & 2 & 2006 & WD & Abnorml & 140000 \\
4 & 5 & 60 & RL & 84.0 & 14260 & Pave & NaN & IR1 & Lvl & AllPub & ... &
0 & NaN & NaN & NaN & 0 & 12 & 2008 & WD & Normal & 250000 \\
... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... &
... & ... & ... & ... & ... & ... & ... & ... & ... & ... \\
1455 & 1456 & 60 & RL & 62.0 & 7917 & Pave & NaN & Reg & Lvl & AllPub &
... & 0 & NaN & NaN & NaN & 0 & 8 & 2007 & WD & Normal & 175000 \\
1456 & 1457 & 20 & RL & 85.0 & 13175 & Pave & NaN & Reg & Lvl & AllPub &
... & 0 & NaN & MnPrv & NaN & 0 & 2 & 2010 & WD & Normal & 210000 \\
1457 & 1458 & 70 & RL & 66.0 & 9042 & Pave & NaN & Reg & Lvl & AllPub &
... & 0 & NaN & GdPrv & Shed & 2500 & 5 & 2010 & WD & Normal & 266500 \\
1458 & 1459 & 20 & RL & 68.0 & 9717 & Pave & NaN & Reg & Lvl & AllPub &
... & 0 & NaN & NaN & NaN & 0 & 4 & 2010 & WD & Normal & 142125 \\
1459 & 1460 & 20 & RL & 75.0 & 9937 & Pave & NaN & Reg & Lvl & AllPub &
... & 0 & NaN & NaN & NaN & 0 & 6 & 2008 & WD & Normal & 147500 \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Submission:}
\CommentTok{\# pred\_df = pd.DataFrame(y\_pred, index=df\_test.index, columns=["SalePrice"])}
\CommentTok{\# pred\_df.to\_csv(\textquotesingle{}../data/kaggle/HousePrices/submissions/en1.csv\textquotesingle{}, header=True, index\_label=\textquotesingle{}Id\textquotesingle{})}
\end{Highlighting}
\end{Shaded}




\end{document}
