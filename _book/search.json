[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science in Python",
    "section": "",
    "text": "Lecturer\nProf. Dr. Markus Loecher\nProfessor for Mathematics and Statistics\nBerlin School of Economics and Law\nlinkedin\nresearchgate\nmy blog"
  },
  {
    "objectID": "IntroCoding_Lecture0.html",
    "href": "IntroCoding_Lecture0.html",
    "title": "1  Data Types",
    "section": "",
    "text": "Knowing about data types in Python is crucial for several reasons:\nOverall, having knowledge of data types in Python enables you to write robust, efficient, and understandable code that operates correctly with the data it handles. It empowers you to make informed decisions about memory usage, data manipulation, input validation, and code integration, leading to better overall programming proficiency."
  },
  {
    "objectID": "IntroCoding_Lecture0.html#scalar-types",
    "href": "IntroCoding_Lecture0.html#scalar-types",
    "title": "1  Data Types",
    "section": "Scalar Types",
    "text": "Scalar Types\n\nPython has a small set of built-in types for handling numerical data, strings, Boolean (True or False) values, and dates and time. These “single value” types are sometimes called scalar types, and we refer to them in this book as scalars . See Standard Python scalar types for a list of the main scalar types. Date and time handling will be discussed separately, as these are provided by the datetime module in the standard library.\n\n\n\n?(caption)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStandard Python scalar types\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\n\n\n\n\n\n\n\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNone\n\n\n\n\n\n\n\n\n\n\n\n\nThe Python “null” value (only one instance of the None object exists)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstr\n\n\n\n\n\n\n\n\n\n\n\n\nString type; holds Unicode strings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbytes\n\n\n\n\n\n\n\n\n\n\n\n\nRaw binary data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfloat\n\n\n\n\n\n\n\n\n\n\n\n\nDouble-precision floating-point number (note there is no separate double type)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbool\n\n\n\n\n\n\n\n\n\n\n\n\nA Boolean True or False value\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nint\n\n\n\n\n\n\n\n\n\n\n\n\nArbitrary precision integer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = 5\ntype(x)\n\nint\n\n\n\ny=7.5\ntype(y)\n\nfloat\n\n\n\ntype(x+y)\n\nfloat\n\n\n\nNumeric Type\n\nThe primary Python types for numbers are int and float. An int can store arbitrarily large numbers:\n\n\nival = 17239871\nival ** 6\n\n26254519291092456596965462913230729701102721\n\n\n\n\nBooleans\ncan take only two values: True and False.\n\nprint(4 == 5)\nprint(4 < 5)\nb = 4 != 5\nprint(b)\nprint(int(b))\n\nFalse\nTrue\nTrue\n1"
  },
  {
    "objectID": "IntroCoding_Lecture0.html#strings",
    "href": "IntroCoding_Lecture0.html#strings",
    "title": "1  Data Types",
    "section": "Strings",
    "text": "Strings\n\nMany people use Python for its built-in string handling capabilities. You can write string literals using either single quotes ’ or double quotes “ (double quotes are generally favored):\n\n\na = 'one way of writing a string'\nb = \"another way\"\n\ntype(a)\n\nstr\n\n\nFor multiline strings with line breaks, you can use triple quotes, either ''' or \"\"\":\n\nc = \"\"\"\nThis is a longer string that\nspans multiple lines\n\"\"\"\n\n\nc.count(\"\\n\")\n\n3\n\n\n\nStrings built-in methods\n\nNote: All string methods return new values. They do not change the original string.\n\n\n\n\n\nMethod\n\n\nDescription\n\n\n\n\ncapitalize()\n\n\nConverts the first character to upper case\n\n\n\n\ncasefold()\n\n\nConverts string into lower case\n\n\n\n\ncenter()\n\n\nReturns a centered string\n\n\n\n\ncount()\n\n\nReturns the number of times a specified value occurs in a string\n\n\n\n\nencode()\n\n\nReturns an encoded version of the string\n\n\n\n\nendswith()\n\n\nReturns true if the string ends with the specified value\n\n\n\n\nexpandtabs()\n\n\nSets the tab size of the string\n\n\n\n\nfind()\n\n\nSearches the string for a specified value and returns the position of where it was found\n\n\n\n\nformat()\n\n\nFormats specified values in a string\n\n\n\n\nformat_map()\n\n\nFormats specified values in a string\n\n\n\n\nindex()\n\n\nSearches the string for a specified value and returns the position of where it was found\n\n\n\n\nisalnum()\n\n\nReturns True if all characters in the string are alphanumeric\n\n\n\n\nisalpha()\n\n\nReturns True if all characters in the string are in the alphabet\n\n\n\n\nisascii()\n\n\nReturns True if all characters in the string are ascii characters\n\n\n\n\nisdecimal()\n\n\nReturns True if all characters in the string are decimals\n\n\n\n\nisdigit()\n\n\nReturns True if all characters in the string are digits\n\n\n\n\nisidentifier()\n\n\nReturns True if the string is an identifier\n\n\n\n\nislower()\n\n\nReturns True if all characters in the string are lower case\n\n\n\n\nisnumeric()\n\n\nReturns True if all characters in the string are numeric\n\n\n\n\nisprintable()\n\n\nReturns True if all characters in the string are printable\n\n\n\n\nisspace()\n\n\nReturns True if all characters in the string are whitespaces\n\n\n\n\nistitle()\n\n\nReturns True if the string follows the rules of a title\n\n\n\n\nisupper()\n\n\nReturns True if all characters in the string are upper case\n\n\n\n\njoin()\n\n\nConverts the elements of an iterable into a string\n\n\n\n\nljust()\n\n\nReturns a left justified version of the string\n\n\n\n\nlower()\n\n\nConverts a string into lower case\n\n\n\n\nlstrip()\n\n\nReturns a left trim version of the string\n\n\n\n\nmaketrans()\n\n\nReturns a translation table to be used in translations\n\n\n\n\npartition()\n\n\nReturns a tuple where the string is parted into three parts\n\n\n\n\nreplace()\n\n\nReturns a string where a specified value is replaced with a specified value\n\n\n\n\nrfind()\n\n\nSearches the string for a specified value and returns the last position of where it was found\n\n\n\n\nrindex()\n\n\nSearches the string for a specified value and returns the last position of where it was found\n\n\n\n\nrjust()\n\n\nReturns a right justified version of the string\n\n\n\n\nrpartition()\n\n\nReturns a tuple where the string is parted into three parts\n\n\n\n\nrsplit()\n\n\nSplits the string at the specified separator, and returns a list\n\n\n\n\nrstrip()\n\n\nReturns a right trim version of the string\n\n\n\n\nsplit()\n\n\nSplits the string at the specified separator, and returns a list\n\n\n\n\nsplitlines()\n\n\nSplits the string at line breaks and returns a list\n\n\n\n\nstartswith()\n\n\nReturns true if the string starts with the specified value\n\n\n\n\nstrip()\n\n\nReturns a trimmed version of the string\n\n\n\n\nswapcase()\n\n\nSwaps cases, lower case becomes upper case and vice versa\n\n\n\n\ntitle()\n\n\nConverts the first character of each word to upper case\n\n\n\n\ntranslate()\n\n\nReturns a translated string\n\n\n\n\nupper()\n\n\nConverts a string into upper case\n\n\n\n\nzfill()\n\n\nFills the string with a specified number of 0 values at the beginning\n\n\n\n\n\n\nMethods\nMany operations/functions in python are specific to the data type even though we use the same syntax:\n\nprint(x+y)\nprint(a+b)\n\n12.5\none way of writing a stringanother way\n\n\n\nType conversion\nWe can often convert from one type to another if it makes sense:\n\nstr(x)\n\n'5'\n\n\n\nfloat(x)\n\n5.0"
  },
  {
    "objectID": "IntroCoding_Lecture0.html#immutable-objects",
    "href": "IntroCoding_Lecture0.html#immutable-objects",
    "title": "1  Data Types",
    "section": "Immutable Objects",
    "text": "Immutable Objects\nMutable objects are those that allow you to change their value or data in place without affecting the object’s identity. In contrast, immutable objects don’t allow this kind of operation. You’ll just have the option of creating new objects of the same type with different values.\nIn Python, mutability is a characteristic that may profoundly influence your decision when choosing which data type to use in solving a given programming problem. Therefore, you need to know how mutable and immutable objects work in Python.\nIn Python, variables don’t have an associated type or size, as they’re labels attached to objects in memory. They point to the memory position where concrete objects live. In other words, a Python variable is a name that refers to or holds a reference to a concrete object. In contrast, Python objects are concrete pieces of information that live in specific memory positions on your computer.\nThe main takeaway here is that variables and objects are two different animals in Python:\n\nVariables hold references to objects.\nObjects live in concrete memory positions.\n\nRead more about this topic\nStrings and tuples are immutable:\n\na = \"this is a string\"\n\na[10] = \"f\"\n\nTypeError: 'str' object does not support item assignment"
  },
  {
    "objectID": "IntroCoding_Lecture0.html#tuples",
    "href": "IntroCoding_Lecture0.html#tuples",
    "title": "1  Data Types",
    "section": "Tuples",
    "text": "Tuples\n\nA tuple is a fixed-length, immutable sequence of Python objects which, once assigned, cannot be changed. The easiest way to create one is with a comma-separated sequence of values wrapped in parentheses:\n\n\ntup = (4, 5, 6)\nprint(tup)\ntup = (4, \"Ray\", 6)\nprint(tup)\n#In many contexts, the parentheses can be omitted\ntup = 4, \"Ray\", 6\nprint(tup)\n\n(4, 5, 6)\n(4, 'Ray', 6)\n(4, 'Ray', 6)\n\n\nElements can be accessed with square brackets [] as with most other sequence types. As in C, C++, Java, and many other languages, sequences are 0-indexed in Python:\n\ntup[0]\n\n4\n\n\n\n#but you cannot change the value:\ntup[0] = 3\n\nNameError: name 'tup' is not defined\n\n\nYou can concatenate tuples using the + operator to produce longer tuples:\n\n(4, None, 'foo') + (6, 0) + ('bar',)\n\n(4, None, 'foo', 6, 0, 'bar')\n\n\nMultiplying a tuple by an integer, as with lists, has the effect of concatenating that many copies of the tuple:\n\n('foo', 'bar') * 4\n\n('foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'bar')\n\n\n\nUnpacking tuples\nIf you try to assign to a tuple-like expression of variables, Python will attempt to unpack the value on the righthand side of the equals sign:\n\ntup = (4, 5, 6)\na, b, c = tup"
  },
  {
    "objectID": "IntroCoding_Lecture1.html",
    "href": "IntroCoding_Lecture1.html",
    "title": "2  Lists and Loops",
    "section": "",
    "text": "Lists\n\nDataCamp, Introduction to Python, Chap 2\n\nLoops\n\nDataCamp, Intermediate Python, Chap 4\n\nConditions\n\nDataCamp, Intermediate Python, Chap 3\n\n\n\nList\nIn contrast with tuples, lists are variable length and their contents can be modified in place. Lists are mutable. You can define them using square brackets [] or using the list type function:\n\nfam = [1.73, 1.68, 1.71, 1.89]\nfam = list((1.73, 1.68, 1.71, 1.89))\nfam\n\n[1.73, 1.68, 1.71, 1.89]\n\n\nYou can sort, append,insert, concatenate, …\n\n#sorting is in place!\nfam.sort()\nfam\n\n\nfam.append(2.05)\nfam\n\n\nfam + fam\n\n[1.73, 1.68, 1.71, 1.89, 1.73, 1.68, 1.71, 1.89]\n\n\nLists can\n\nContain any type\nContain different types\n\n\nfam2 = [\"liz\", 1.73, \"emma\", 1.68, \"mom\", 1.71, \"dad\", 1.89]\nfam2\n\n['liz', 1.73, 'emma', 1.68, 'mom', 1.71, 'dad', 1.89]\n\n\n\n\nList of lists\n\nfam3 = [[\"liz\", 1.73],\n         [\"emma\", 1.68],\n         [\"mom\", 1.71],\n         [\"dad\", 1.89]]\nfam3\n\n[['liz', 1.73], ['emma', 1.68], ['mom', 1.71], ['dad', 1.89]]\n\n\n\n\nSlicing\n\nYou can select sections of most sequence types by using slice notation, which in its basic form consists of start:stop passed to the indexing operator []:\n\n\nfam[1:3]\n\n[1.68, 1.71]\n\n\n\nWhile the element at the start index is included, the stop index is not included, so that the number of elements in the result is stop - start.\n\n\nEither the start or stop can be omitted, in which case they default to the start of the sequence and the end of the sequence, respectively:\n\n\nprint(fam[1:])\nprint(fam[:3])\n\n[1.68, 1.71, 1.89]\n[1.73, 1.68, 1.71]\n\n\nNegative indices slice the sequence relative to the end:\n\nfam[-4:]\n\n[1.73, 1.68, 1.71, 1.89]\n\n\n\nSlicing semantics takes a bit of getting used to, especially if you’re coming from R or MATLAB. See this helpful illustration of slicing with positive and negative integers. In the figure, the indices are shown at the “bin edges” to help show where the slice selections start and stop using positive or negative indices.\n\n\n\n\nIllustration of Python slicing conventions\n\n\n\n\n\nManipulating lists of lists\nThe following list of lists contains names of sections in a house and their area.\n\nExtract the area corresponding to kitchen\nString Tasks:\n\nExtract the first letters of each string\nCapitalize all strings\nReplace all occurrences of “room” with “rm”\ncount the number of “l” in “hallway”\n\nInsert a “home office” with area 10.75 after living room\nAppend the total area to the end of the list\nBoolean operations:\n\nGenerate one True and one False by comparing areas\nGenerate one True and one False by comparing names\n\n\n\nhouse = [['hallway', 11.25],\n ['kitchen', 18.0],\n ['living room', 20.0],\n ['bedroom', 10.75],\n ['bathroom', 9.5]]\n\n\n\nAutomation by iterating\nfor loops are a powerful way of automating MANY otherwise tedious tasks that repeat.\nThe syntax template is (watch the indentation!):\n```` for var in seq : expression\n\n#We can use lists:\nfor f in fam:\n    print(f)\n\n1.73\n1.68\n1.71\n1.89\n\n\n\n#or iterators\nfor i in range(len(fam)):\n    print(fam[i])\n\n1.73\n1.68\n1.71\n1.89\n\n\n\n#or enumerate\nfor i,f in enumerate(fam):\n    print(fam[i], f)\n\n1.73 1.73\n1.68 1.68\n1.71 1.71\n1.89 1.89\n\n\n\nRepeat the tasks 2 and 4 from above by using a for loop\n\nusing enumerate\nusing range\n\nCreate two separates new lists which contain only the names and areas separately\nClever Carl: Compute \\[\n\\sum_{i=1}^{100}{i}\n\\]\n\n\n#funny iterators\nlist(range(5))\n\n[0, 1, 2, 3, 4]\n\n\n\n\nConditions\n\nFind the max of the areas by using if inside a for loop\nPrint those elements of the list with\n\narea \\(> 15\\)\nstrings that contain “room” (or “rm” after your substitution)"
  },
  {
    "objectID": "IntroCoding_Lecture2.html",
    "href": "IntroCoding_Lecture2.html",
    "title": "3  Dictionaries and Functions",
    "section": "",
    "text": "Functions\n\nDataCamp, Introduction to Python, Chap 3\n\nDictionaries\n\nDataCamp, Intermediate Python, Chap 2\n\nIntroduction to numpy\n\nDataCamp, Introduction to Python, Chap 4\n\n\n\nFunctions\nFunctions are essential building blocks to reuse code and to modularize code.\nWe have already seen and used many built-in functions/methods such as print(), len(), max(), round(), index(), capitalize(), etc..\n\nareas = [11.25, 18.0, 20.0, 10.75, 10.75, 9.5]\nprint(max(areas))\nprint(len(areas))\nprint(round(10.75,1))\nprint(areas.index(18.0))\n\n20.0\n6\n10.8\n1\n\n\nBut of course we want to define our own functions as well ! As a rule of thumb, if you anticipate needing to repeat the same or very similar code more than once, it may be worth writing a reusable function. Functions can also help make your code more readable by giving a name to a group of Python statements.\nFor example, we computed the BMI previously as follows:\n\nheight = 1.79\nweight = 68.7\nbmi = weight/height**2\nprint(bmi)\n\n21.44127836209856\n\n\nFunctions are declared with the def keyword. A function contains a block of code with an optional use of the return keyword:\n\ndef compute_bmi(height, weight):\n    return weight/height**2\n\ncompute_bmi(1.79, 68.7)\n\n21.44127836209856\n\n\nEach function can have positional arguments and keyword arguments. Keyword arguments are most commonly used to specify default values or optional arguments. For example:\n\ndef compute_bmi(height, weight, ndigits=2):\n    return round(weight/height**2, ndigits)\n\nprint(compute_bmi(1.79, 68.7))\nprint(compute_bmi(1.79, 68.7,4))\n\n21.44\n21.4413\n\n\n\nMultiple Return Values\nare easily possible in python:\n\ndef compute_bmi(height, weight, ndigits=2):\n    bmi = round(weight/height**2, ndigits)\n    #https://www.cdc.gov/healthyweight/assessing/index.html#:~:text=If%20your%20BMI%20is%20less,falls%20within%20the%20obese%20range.\n    if bmi < 18.5:\n        status=\"underweight\"\n    elif bmi <= 24.9:\n        status=\"healthy\"\n    elif bmi <= 29.9:\n        status=\"underweight\"\n    elif bmi >= 30:#note that a simple else would suffice here!\n        status=\"obese\"\n    return bmi, status\n\nprint(compute_bmi(1.79, 68.7))\nprint(compute_bmi(1.79, 55))\n\n(21.44, 'healthy')\n(17.17, 'underweight')\n\n\nRecall from the previous lab how we\n\nfound the largest room,\ncomputed the sum of integers from 1 to 100\n\n\n#find the maximum area:\nareas = [11.25, 18.0, 20.0, 10.75, 10.75, 9.5]\ncurrentMax = areas[0] # initialize to the first area seen\n\nfor a in areas:\n  if a > currentMax:\n    currentMax = a\n\nprint(\"The max is:\", currentMax)\n\nThe max is: 20.0\n\n\n\n#Clever IDB students: Compute the sum from 1 to 100:\nTotal =0\n\nfor i in range(101):#strictly speaking we are adding the first  0 \n  Total = Total + i\n  #Total += i\n\nprint(Total)\n\n\n\nTasks\nWrite your own function\n\nto find the min and max of a list\nto compute the Gauss sum with defaukt values \\(m=1, n=100\\)\n\n\\[\n\\sum_{i=m}^{n}{i}\n\\]\n\n\nNamespaces and Scope\nFunctions seem straightforward. But one of the more confusing aspects in the beginning is the concept that we can have multiple instances of the same variable!\nFunctions can access variables created inside the function as well as those outside the function in higher (or even global) scopes. An alternative and more descriptive name describing a variable scope in Python is a namespace. Any variables that are assigned within a function by default are assigned to the local namespace. The local namespace is created when the function is called and is immediately populated by the function’s arguments. After the function is finished, the local namespace is destroyed.\nExamples:\n\nheight = 1.79\nweight = 68.7\nbmi = weight/height**2\n#print(\"height, weight, bmi OUTSIDE the function:\",height, weight,bmi)\n\ndef compute_bmi(h, w):\n    height = h\n    weight = w\n    bmi = round(weight/height**2,2)\n    status=\"healthy\"\n    print(\"height, weight, bmi INSIDE the function:\",height, weight,bmi)\n    print(\"status:\", status)\n    return bmi\n\ncompute_bmi(1.55, 50)\n\nprint(\"height, weight, bmi OUTSIDE the function:\",height, weight,bmi)\n#print(status)\n\nheight, weight, bmi INSIDE the function: 1.55 50 20.81\nstatus: healthy\nheight, weight, bmi OUTSIDE the function: 1.79 68.7 21.44127836209856\n\n\n\n\n\nDictionaries\nA dictionary is basically a lookup table. It stores a collection of key-value pairs, where key and value are Python objects. Each key is associated with a value so that a value can be conveniently retrieved, inserted, modified, or deleted given a particular key.\nThe dictionary or dict may be the most important built-in Python data structure. In other programming languages, dictionaries are sometimes called hash maps or associative arrays.\n\n#This was the house defined as a list of lists:\nhouse = [['hallway', 11.25],\n ['kitchen', 18.0],\n ['living room', 20.0],\n ['bedroom', 10.75],\n ['bathroom', 9.5]]\n\n#Remember all the disadvantages of accessing elements\n\n#Better as a lookup table:\nhouse = {'hallway': 11.25,\n    'kitchen': 18.0,\n    'living room': 20.0,\n    'bedroom': 10.75,\n    'bathroom': 9.5}\n\n\neurope = {'spain':'madrid', 'france' : 'paris'}\nprint(europe[\"spain\"])\nprint(\"france\" in europe)\nprint(\"paris\" in europe)#only checks the keys!\neurope[\"germany\"] = \"berlin\"\nprint(europe.keys())\nprint(europe.values())\n\nmadrid\nTrue\nFalse\ndict_keys(['spain', 'france', 'germany'])\ndict_values(['madrid', 'paris', 'berlin'])\n\n\nIf you need to iterate over both the keys and values, you can use the items method to iterate over the keys and values as 2-tuples:\n\n#print(list(europe.items()))\n\nfor country, capital in europe.items():\n    print(capital, \"is the capital of\", country)\n\nmadrid is the capital of spain\nparis is the capital of france\nberlin is the capital of germany\n\n\nNote: You can use integers as keys as well. However -unlike in lists- one should not think of them as positional indices!\n\n#Assume you have a basement:\nhouse[0] = 21.5\nhouse\n\n{'hallway': 11.25,\n 'kitchen': 18.0,\n 'living room': 20.0,\n 'bedroom': 10.75,\n 'bathroom': 9.5,\n 0: 21.5}\n\n\n\n#And there is a difference between the string and the integer index!\nhouse[\"0\"] = 30.5\nhouse\n\n{'hallway': 11.25,\n 'kitchen': 18.0,\n 'living room': 20.0,\n 'bedroom': 10.75,\n 'bathroom': 9.5,\n 0: 21.5}\n\n\nCategorize a list of words by their first letters as a dictionary of lists:\n\nwords = [\"apple\", \"bat\", \"bar\", \"atom\", \"book\"]\n\nby_letter = {}\n\nfor word in words:\n     letter = word[0]\n     if letter not in by_letter:\n        by_letter[letter] = [word]\n     else:\n         by_letter[letter].append(word)\n\n{'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}\n\n\n\nTasks\n\nFind the maximum of the areas of the houses\nRemove the two last entries.\nWrite a function named word_count that takes a string as input and returns a dictionary with each word in the string as a key and the number of times it appears as the value.\n\n\n\n\nIntroduction to numpy\nNumPy, short for Numerical Python, is one of the most important foundational packages for numerical computing in Python.\n\nVectorized, fast mathematical operations.\nKey features of NumPy is its N-dimensional array object, or ndarray\n\n\nheight = [1.79, 1.85, 1.95, 1.55]\nweight = [70, 80, 85, 65]\n\n#bmi = weight/height**2\n\n\nimport numpy as np\n\nheight = np.array([1.79, 1.85, 1.95, 1.55])\nweight = np.array([70, 80, 85, 65])\n\nbmi = weight/height**2\nnp.round(bmi,2)\n\narray([21.84700852, 23.37472608, 22.35371466, 27.05515088])"
  },
  {
    "objectID": "IntroCoding_Lecture3.html",
    "href": "IntroCoding_Lecture3.html",
    "title": "4  Intro to numpy",
    "section": "",
    "text": "In this lecture we will get to know and become experts in:"
  },
  {
    "objectID": "IntroCoding_Lecture3.html#introduction-to-numpy",
    "href": "IntroCoding_Lecture3.html#introduction-to-numpy",
    "title": "4  Intro to numpy",
    "section": "Introduction to numpy",
    "text": "Introduction to numpy\nNumPy, short for Numerical Python, is one of the most important foundational packages for numerical computing in Python.\n\nVectorized, fast mathematical operations.\nKey features of NumPy is its N-dimensional array object, or ndarray\n\n\nheight = [1.79, 1.85, 1.95, 1.55]\nweight = [70, 80, 85, 65]\n\n#bmi = weight/height**2\n\n\nheight = np.array([1.79, 1.85, 1.95, 1.55])\nweight = np.array([70, 80, 85, 65])\n\nbmi = weight/height**2\nbmi\n\narray([21.84700852, 23.37472608, 22.35371466, 27.05515088])\n\n\n\nMultiple Dimensions\nare handled naturally by numpy, e.g.\n\nhw1 = np.array([height, weight])\nprint(hw1)\nprint(hw1.shape)\nhw2 = hw1.transpose()\nprint(hw2)\nprint(hw2.shape)\n\n[[ 1.79  1.85  1.95  1.55]\n [70.   80.   85.   65.  ]]\n(2, 4)\n[[ 1.79 70.  ]\n [ 1.85 80.  ]\n [ 1.95 85.  ]\n [ 1.55 65.  ]]\n(4, 2)\n\n\n\n\nAccessing array elements\nis similar to lists but allows for multidimensional index:\n\nprint(hw2[0,1])\n\n70.0\n\n\n\nprint(hw2[:,0])\n\n[1.79 1.85 1.95 1.55]\n\n\n\nprint(hw2[0])\n#equivalent to\nprint(hw2[0,:])\n#shape:\nprint(hw2[0].shape)\n\n[ 1.79 70.  ]\n[ 1.79 70.  ]\n(2,)\n\n\nTo select a subset of the rows in a particular order, you can simply pass a list or ndarray of integers specifying the desired order:\n\nprint(hw2[[2,0,1]])\n\n[[ 1.95 85.  ]\n [ 1.79 70.  ]\n [ 1.85 80.  ]]\n\n\nNegative indices\n\nprint(hw2)\nprint(\"Using negative indices selects rows from the end:\")\nprint(hw2[[-2,-1]])\n\n[[ 1.79 70.  ]\n [ 1.85 80.  ]\n [ 1.95 85.  ]\n [ 1.55 65.  ]]\nUsing negative indices selects rows from the end:\n[[ 1.95 85.  ]\n [ 1.55 65.  ]]\n\n\nYou can pass multiple slices just like you can pass multiple indexes:\n\nhw2[:2,:1]\n\narray([[1.79],\n       [1.85]])\n\n\n\nReshaping\n\nnp.arange(32).reshape((8, 4))\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15],\n       [16, 17, 18, 19],\n       [20, 21, 22, 23],\n       [24, 25, 26, 27],\n       [28, 29, 30, 31]])\n\n\n\n\nBoolean indexing\n\nheight_gt_185 = hw2[:,0]>1.85\nprint(height_gt_185)\nprint(hw2[height_gt_185,1])\n\n[False False  True False]\n[85.]\n\n\nnumpy arrays cannot contain elements with different types. If you try to build such a list, some of the elements’ types are changed to end up with a homogeneous list. This is known as type coercion.\n\nprint(np.array([True, 1, 2]))\nprint(np.array([\"True\", 1, 2]))\nprint(np.array([1.3, 1, 2]))\n\n[1 1 2]\n['True' '1' '2']\n[1.3 1.  2. ]\n\n\nLots of extra useful functions!\n\nnp.zeros((2,3))\n#np.ones((2,3))\n\narray([[0., 0., 0.],\n       [0., 0., 0.]])\n\n\n\nnp.eye(3)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\nnp.column_stack([height, weight])\n\narray([[ 1.79, 70.  ],\n       [ 1.85, 80.  ],\n       [ 1.95, 85.  ],\n       [ 1.55, 65.  ]])"
  },
  {
    "objectID": "IntroCoding_Lecture3.html#data-summaries-in-numpy",
    "href": "IntroCoding_Lecture3.html#data-summaries-in-numpy",
    "title": "4  Intro to numpy",
    "section": "Data Summaries in numpy",
    "text": "Data Summaries in numpy\nWe can compute simple statistics:\n\nprint(np.mean(hw2))\nprint(np.mean(hw2, axis=0))\n\n38.3925\n[ 1.785 75.   ]\n\n\n\nprint(np.unique([1,1,2,1,2,3,2,2,3]))\n\nprint(np.unique([1,1,2,1,2,3,2,2,3], return_counts=True))\n\n[1 2 3]\n(array([1, 2, 3]), array([3, 4, 2], dtype=int64))"
  },
  {
    "objectID": "IntroCoding_Lecture3.html#introduction-to-simulating-probabilistic-events",
    "href": "IntroCoding_Lecture3.html#introduction-to-simulating-probabilistic-events",
    "title": "4  Intro to numpy",
    "section": "Introduction to Simulating Probabilistic Events",
    "text": "Introduction to Simulating Probabilistic Events\n\nGenerating Data in numpy\nMeet your friends:\n\nnp.random.permutation: Return a random permutation of a sequence, or return a permuted range\nnp.random.integers: Draw random integers from a given low-to-high range\nnp.random.choice: Generates a random sample from a given 1-D array\n\n\n# Do this (new version)\nfrom numpy.random import default_rng\nrng = default_rng()\n\nx= np.arange(10)\nprint(x)\nprint(rng.permutation(x))\nprint(rng.permutation(list('intelligence')))\n\n[0 1 2 3 4 5 6 7 8 9]\n[6 7 9 4 1 0 3 8 2 5]\n['t' 'c' 'n' 'l' 'e' 'n' 'i' 'e' 'e' 'l' 'i' 'g']\n\n\n\nprint(rng.integers(0,10,5))\nprint(rng.integers(0,10,(5,2)))\n\n[7 9 7 9 4]\n[[9 0]\n [8 6]\n [6 7]\n [0 5]\n [1 5]]\n\n\n\nrng.choice(x,4)\n\narray([8, 5, 1, 4])\n\n\n\n\nExamples:\n\nSpotify playlist\nMovie List\n\n\nmovies_list = ['The Godfather', 'The Wizard of Oz', 'Citizen Kane', 'The Shawshank Redemption', 'Pulp Fiction']\n\n# pick a random choice from a list of strings.\nmovie = rng.choice(movies_list,2)\nprint(movie)\n\n['The Shawshank Redemption' 'The Godfather']"
  },
  {
    "objectID": "IntroCoding_Lecture3.html#birthday-paradox",
    "href": "IntroCoding_Lecture3.html#birthday-paradox",
    "title": "4  Intro to numpy",
    "section": "Birthday “Paradox”",
    "text": "Birthday “Paradox”\nPlease enter your birthday on google drive https://forms.gle/CeqyRZ4QzWRmJFvs9\nHow many people do you think will share a birthday? Would that be a rare, highly unusual event?\nHow can we find out how likely it is that across \\(n\\) folks in a room at least two share a birthday?\nHint: can we put our random number generators to task ?\n\n# Can you simulate 25 birthdays?\nfrom numpy.random import default_rng \nrng = default_rng()\n\n\n#initialize it to be the empty list:\nshardBday = []\n\nn = 40\n\nPossibleBDays = np.arange(1,366)\n#now \"draw\" 25 random bDays:\nfor i in range(1000):# is the 1000 an important number ??\n#no it only determines the precision of my estimate !!\n  ran25Bdays = rng.choice(PossibleBDays, n, replace = True)\n  #it is of utmost importance to allow for same birthdays !! \n  #rng.choice(PossibleBDays, 366, replace = False)\n  x , cts = np.unique(ran25Bdays ,return_counts=True)\n  shardBday = np.append(shardBday, np.sum(cts>1))#keep this !!\n  #shardBday = np.sum(cts>1)\n\n\n#np.sum(shardBday>0)/1000\nnp.mean(shardBday > 0)\n\n#shardBday = 2\n\n0.893\n\n\n\n5 != 3 #not equal\n\nTrue\n\n\n\n#Boolean indexing !!\nx[cts > 1]\n\narray([ 71, 192])\n\n\n\nx[23]\n\n182\n\n\n\n#can you design a coin flip with an arbitary probability p = 0.25\n#simulate 365 days with a 1/4 chance of being sunny\n\n#fair coin\ncoins = np.random.randint(0,2,365)\n\nnp.unique(coins, return_counts=True)\n\n(array([0, 1]), array([189, 176], dtype=int64))\n\n\n\n\nTossing dice and coins\nLet us toss many dice or coins to find out: - the average value of a six-faced die - the variation around the mean when averaging - the probability of various “common hands” in the game Liar’s Dice: * Full house: e.g., 66111 * Three of a kind: e.g., 44432 * Two pair: e.g., 22551 * Pair: e.g., 66532\nSome real world problems: 1. Overbooking flights: airlines 2. Home Office days: planning office capacities and minimizing social isolation"
  },
  {
    "objectID": "IntroCoding_Lecture4.html",
    "href": "IntroCoding_Lecture4.html",
    "title": "5  Intro to pandas",
    "section": "",
    "text": "In this Introduction to pandas we will get to know and become experts in:\nRelevant DataCamp lessons:"
  },
  {
    "objectID": "IntroCoding_Lecture4.html#introduction-to-pandas",
    "href": "IntroCoding_Lecture4.html#introduction-to-pandas",
    "title": "5  Intro to pandas",
    "section": "Introduction to pandas",
    "text": "Introduction to pandas\nWhile numpy offers a lot of powerful numerical capabilities it lacks some of the necessary convenience and natural of handling data as we encounter them. For example, we would typically like to - mix data types (strings, numbers, categories, Boolean, …) - refer to columns and rows by names - summarize and visualize data in efficient pivot style manners\nAll of the above (and more) can be achieved easily by extending the concept of an array (or a matrix) to a so called dataframe.\nThere are many ways to construct a DataFrame, though one of the most common is from a dictionary of equal-length lists or NumPy arrays:\n\ndata = {\"state\": [\"Ohio\", \"Ohio\", \"Ohio\", \"Nevada\", \"Nevada\", \"Nevada\"],\n        \"year\": [2000, 2001, 2002, 2001, 2002, 2003],\n        \"pop\": [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}\nframe = pd.DataFrame(data)# creates a dataframe out of the data given!\nframe.head(3)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      state\n      year\n      pop\n    \n  \n  \n    \n      0\n      Ohio\n      2000\n      1.5\n    \n    \n      1\n      Ohio\n      2001\n      1.7\n    \n    \n      2\n      Ohio\n      2002\n      3.6\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nframe[2,1]#too bad\n\n\n#to get the full row: use the .iloc method\nframe.iloc[2]\nframe.iloc[2,1]\n\n2002\n\n\n\nSubsetting/Slicing\nWe first need to understand the attributes index (=rownames) and columns (= column names):\n\nframe.index\n\nRangeIndex(start=0, stop=6, step=1)\n\n\n\n#We can set a column as an index:\nframe2 = frame.set_index(\"year\")\nprint(frame2)\n# \n\n       state  pop\nyear             \n2000    Ohio  1.5\n2001    Ohio  1.7\n2002    Ohio  3.6\n2001  Nevada  2.4\n2002  Nevada  2.9\n2003  Nevada  3.2\n\n\n\n#it would be nice to access elements in the same fashion as numpy\n#frame2[1,1]\nframe[\"pop\"]\n\n0    1.5\n1    1.7\n2    3.6\n3    2.4\n4    2.9\n5    3.2\nName: pop, dtype: float64\n\n\n\nframe.pop\n\n<bound method DataFrame.pop of     state  year  pop\n0    Ohio  2000  1.5\n1    Ohio  2001  1.7\n2    Ohio  2002  3.6\n3  Nevada  2001  2.4\n4  Nevada  2002  2.9\n5  Nevada  2003  3.2>\n\n\n\n\nAsking for rows\nUnfortunately, we cannot use the simple [row,col] notation that we are used to from numpy arrays. (Try asking for frame[0,1])\nInstead, row subsetting can be achieved with either the .loc() or the .iloc() methods. The latter takes integers, the former indices:\n\nframe2.loc[2001] #note that I am not using quotes !!\n#at first glance this looks like I am asking for the row number 2001 !!\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      state\n      pop\n    \n    \n      year\n      \n      \n    \n  \n  \n    \n      2001\n      Ohio\n      1.7\n    \n    \n      2001\n      Nevada\n      2.4\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nframe2.loc[2001,\"state\"]\n\nyear\n2001      Ohio\n2001    Nevada\nName: state, dtype: object\n\n\n\nframe.iloc[0]#first row\n\nstate    Ohio\nyear     2000\npop       1.5\nName: 0, dtype: object\n\n\n\nframe3 = frame.set_index(\"state\", drop=False)\nprint(frame3)\n\n         state  year  pop\nstate                    \nOhio      Ohio  2000  1.5\nOhio      Ohio  2001  1.7\nOhio      Ohio  2002  3.6\nNevada  Nevada  2001  2.4\nNevada  Nevada  2002  2.9\nNevada  Nevada  2003  3.2\n\n\n\nframe3.loc[\"Ohio\"]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      year\n      pop\n    \n    \n      state\n      \n      \n    \n  \n  \n    \n      Ohio\n      2000\n      1.5\n    \n    \n      Ohio\n      2001\n      1.7\n    \n    \n      Ohio\n      2002\n      3.6\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nframe.iloc[2001]# this does not work because we do not have 2001 rows !\n\n\nframe.iloc[0,1]\n\n2000\n\n\n\n\nAsking for columns\n\n#The columns are also an index:\nframe.columns\n\nIndex(['state', 'year', 'pop'], dtype='object')\n\n\nA column in a DataFrame can be retrieved MUCH easier: as a Series either by dictionary-like notation or by using the dot attribute notation:\n\nframe[\"state\"]\n\n0      Ohio\n1      Ohio\n2      Ohio\n3    Nevada\n4    Nevada\n5    Nevada\nName: state, dtype: object\n\n\n\nframe.year#equivalent to frame[\"year\"]\n\n0    2000\n1    2001\n2    2002\n3    2001\n4    2002\n5    2003\nName: year, dtype: int64\n\n\n\n\nSummary Stats\nJust like in numpy you can compute sums, means, counts and many other summaries along rows and columns, by specifying the axis argument:\n\nheight = np.array([1.79, 1.85, 1.95, 1.55])\nweight = np.array([70, 80, 85, 65])\nhw = np.array([height, weight]).transpose()\n\nhw\n\narray([[ 1.79, 70.  ],\n       [ 1.85, 80.  ],\n       [ 1.95, 85.  ],\n       [ 1.55, 65.  ]])\n\n\n\ndf = pd.DataFrame(hw, columns = [\"height\", \"weight\"]) \nprint(df)\n\n   height  weight\n0    1.79    70.0\n1    1.85    80.0\n2    1.95    85.0\n3    1.55    65.0\n\n\n\ndf = pd.DataFrame(hw , columns = [\"height\", \"weight\"],\n                  index = [\"Peter\", \"Matilda\", \"Bee\", \"Tom\"]) \nprint(df)\n\n         height  weight\nPeter      1.79    70.0\nMatilda    1.85    80.0\nBee        1.95    85.0\nTom        1.55    65.0\n\n\nCan you extract:\n\nAll weights\nPeter’s height\nBee’s full info\nthe average height\nget all persons with height greater than 180cm\n\n\n#see Lab5\n\n\nprint(df.mean(axis=0))\nprint(df.mean(axis=1))# are these averages sensible ?\n\nheight     1.785\nweight    75.000\ndtype: float64\nPeter      35.895\nMatilda    40.925\nBee        43.475\nBee        33.275\ndtype: float64\n\n\nSome methods are neither reductions nor accumulations. describe is one such example, producing multiple summary statistics in one shot:\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      height\n      weight\n    \n  \n  \n    \n      count\n      4.000\n      4.000000\n    \n    \n      mean\n      1.785\n      75.000000\n    \n    \n      std\n      0.170\n      9.128709\n    \n    \n      min\n      1.550\n      65.000000\n    \n    \n      25%\n      1.730\n      68.750000\n    \n    \n      50%\n      1.820\n      75.000000\n    \n    \n      75%\n      1.875\n      81.250000\n    \n    \n      max\n      1.950\n      85.000000"
  },
  {
    "objectID": "IntroCoding_Lecture4.html#built-in-data-sets",
    "href": "IntroCoding_Lecture4.html#built-in-data-sets",
    "title": "5  Intro to pandas",
    "section": "Built in data sets",
    "text": "Built in data sets"
  },
  {
    "objectID": "IntroCoding_Lecture4.html#gapminder-data",
    "href": "IntroCoding_Lecture4.html#gapminder-data",
    "title": "5  Intro to pandas",
    "section": "Gapminder Data",
    "text": "Gapminder Data\nhttps://www.gapminder.org/fw/world-health-chart/\nhttps://www.ted.com/talks/hans_rosling_the_best_stats_you_ve_ever_seen#t-241405\n\nYou’ve never seen data presented like this. With the drama and urgency of a sportscaster, statistics guru Hans Rosling debunks myths about the so-called “developing world.”\n\n\n!pip install gapminder\n#!conda install gapminder\nfrom gapminder import gapminder\n#gapminder.to_csv(\"../datasets/gapminder.csv\")\n\n\ngapminder\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      country\n      continent\n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      Asia\n      1952\n      28.801\n      8425333\n      779.445314\n    \n    \n      1\n      Afghanistan\n      Asia\n      1957\n      30.332\n      9240934\n      820.853030\n    \n    \n      2\n      Afghanistan\n      Asia\n      1962\n      31.997\n      10267083\n      853.100710\n    \n    \n      3\n      Afghanistan\n      Asia\n      1967\n      34.020\n      11537966\n      836.197138\n    \n    \n      4\n      Afghanistan\n      Asia\n      1972\n      36.088\n      13079460\n      739.981106\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1699\n      Zimbabwe\n      Africa\n      1987\n      62.351\n      9216418\n      706.157306\n    \n    \n      1700\n      Zimbabwe\n      Africa\n      1992\n      60.377\n      10704340\n      693.420786\n    \n    \n      1701\n      Zimbabwe\n      Africa\n      1997\n      46.809\n      11404948\n      792.449960\n    \n    \n      1702\n      Zimbabwe\n      Africa\n      2002\n      39.989\n      11926563\n      672.038623\n    \n    \n      1703\n      Zimbabwe\n      Africa\n      2007\n      43.487\n      12311143\n      469.709298\n    \n  \n\n1704 rows × 6 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n#find the unique years\n\n#get the years:\ngapminder[\"year\"]\nnp.unique(gapminder.year)\n\narray([1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002,\n       2007])\n\n\n\n#get all rows with year 1952:\n#Hint:\n#either use Boolean subsetting\ngapminder[\"year\"] == 1952\ngapminder[gapminder[\"year\"] == 1952]\n#or use an index !!\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      country\n      continent\n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      Asia\n      1952\n      28.801\n      8425333\n      779.445314\n    \n    \n      12\n      Albania\n      Europe\n      1952\n      55.230\n      1282697\n      1601.056136\n    \n    \n      24\n      Algeria\n      Africa\n      1952\n      43.077\n      9279525\n      2449.008185\n    \n    \n      36\n      Angola\n      Africa\n      1952\n      30.015\n      4232095\n      3520.610273\n    \n    \n      48\n      Argentina\n      Americas\n      1952\n      62.485\n      17876956\n      5911.315053\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1644\n      Vietnam\n      Asia\n      1952\n      40.412\n      26246839\n      605.066492\n    \n    \n      1656\n      West Bank and Gaza\n      Asia\n      1952\n      43.160\n      1030585\n      1515.592329\n    \n    \n      1668\n      Yemen, Rep.\n      Asia\n      1952\n      32.548\n      4963829\n      781.717576\n    \n    \n      1680\n      Zambia\n      Africa\n      1952\n      42.038\n      2672000\n      1147.388831\n    \n    \n      1692\n      Zimbabwe\n      Africa\n      1952\n      48.451\n      3080907\n      406.884115\n    \n  \n\n142 rows × 6 columns"
  },
  {
    "objectID": "IntroCoding_Lecture4.html#handling-files",
    "href": "IntroCoding_Lecture4.html#handling-files",
    "title": "5  Intro to pandas",
    "section": "Handling Files",
    "text": "Handling Files\nGet to know your friends\n\npd.read_csv\npd.read_table\npd.read_excel\n\n\n'''url = \"https://drive.google.com/file/d/1oIvCdN15UEwt4dCyjkArekHnTrivN43v/view?usp=share_link\"\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\ngapminder = pd.read_csv(url, index_col=0)\ngapminder.head()'''\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      country\n      continent\n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      Asia\n      1952\n      28.801\n      8425333\n      779.445314\n    \n    \n      1\n      Afghanistan\n      Asia\n      1957\n      30.332\n      9240934\n      820.853030\n    \n    \n      2\n      Afghanistan\n      Asia\n      1962\n      31.997\n      10267083\n      853.100710\n    \n    \n      3\n      Afghanistan\n      Asia\n      1967\n      34.020\n      11537966\n      836.197138\n    \n    \n      4\n      Afghanistan\n      Asia\n      1972\n      36.088\n      13079460\n      739.981106\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ngapminder.sort_values(by=\"year\").head()\n\n\n\n\n\n  \n    \n      \n      country\n      continent\n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      Asia\n      1952\n      28.801\n      8425333\n      779.445314\n    \n    \n      528\n      France\n      Europe\n      1952\n      67.410\n      42459667\n      7029.809327\n    \n    \n      540\n      Gabon\n      Africa\n      1952\n      37.003\n      420702\n      4293.476475\n    \n    \n      1656\n      West Bank and Gaza\n      Asia\n      1952\n      43.160\n      1030585\n      1515.592329\n    \n    \n      552\n      Gambia\n      Africa\n      1952\n      30.000\n      284320\n      485.230659\n    \n  \n\n\n\n\n\n#How many countries?\nCtryCts = gapminder[\"country\"].value_counts()\nCtryCts\n#note the similarity with np.unique(..., return_counts=True)\n\nAfghanistan          12\nPakistan             12\nNew Zealand          12\nNicaragua            12\nNiger                12\n                     ..\nEritrea              12\nEquatorial Guinea    12\nEl Salvador          12\nEgypt                12\nZimbabwe             12\nName: country, Length: 142, dtype: int64\n\n\n\nfrom numpy.random import default_rng\nrng = default_rng()\nrng.choice(gapminder[\"country\"].unique(),2)\ngapminder[\"year\"].unique()\n\narray([1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002,\n       2007])\n\n\n\n#How meaningful are the column stats?\nprint(gapminder.mean(axis=0))\ngapminder.describe()\n\nyear         1.979500e+03\nlifeExp      5.947444e+01\npop          2.960121e+07\ngdpPercap    7.215327e+03\ndtype: float64\n\n\n/var/folders/h4/k73g68ds6xj791sf8cpmlxlc0000gn/T/ipykernel_33611/633466148.py:2: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n  print(gapminder.mean(axis=0))\n\n\n\n\n\n\n  \n    \n      \n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n  \n  \n    \n      count\n      1704.00000\n      1704.000000\n      1.704000e+03\n      1704.000000\n    \n    \n      mean\n      1979.50000\n      59.474439\n      2.960121e+07\n      7215.327081\n    \n    \n      std\n      17.26533\n      12.917107\n      1.061579e+08\n      9857.454543\n    \n    \n      min\n      1952.00000\n      23.599000\n      6.001100e+04\n      241.165876\n    \n    \n      25%\n      1965.75000\n      48.198000\n      2.793664e+06\n      1202.060309\n    \n    \n      50%\n      1979.50000\n      60.712500\n      7.023596e+06\n      3531.846988\n    \n    \n      75%\n      1993.25000\n      70.845500\n      1.958522e+07\n      9325.462346\n    \n    \n      max\n      2007.00000\n      82.603000\n      1.318683e+09\n      113523.132900\n    \n  \n\n\n\n\nSort the index before you slice!!\nChoose a time range and specific countries\n\ngapminder2 = gapminder.set_index(\"year\").sort_index()\ngap1982_92 = gapminder2.loc[1982:1992].reset_index()\ngap1982_92 = gap1982_92.set_index(\"country\").sort_index()\ngap1982_92.loc[\"Afghanistan\":\"Albania\"]\n\n\n\n\n\n  \n    \n      \n      year\n      continent\n      lifeExp\n      pop\n      gdpPercap\n    \n    \n      country\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      1982\n      Asia\n      39.854\n      12881816\n      978.011439\n    \n    \n      Afghanistan\n      1987\n      Asia\n      40.822\n      13867957\n      852.395945\n    \n    \n      Afghanistan\n      1992\n      Asia\n      41.674\n      16317921\n      649.341395\n    \n    \n      Albania\n      1992\n      Europe\n      71.581\n      3326498\n      2497.437901\n    \n    \n      Albania\n      1987\n      Europe\n      72.000\n      3075321\n      3738.932735\n    \n    \n      Albania\n      1982\n      Europe\n      70.420\n      2780097\n      3630.880722\n    \n  \n\n\n\n\n\ngap1982_92.loc[\"Afghanistan\":\"Albania\",\"lifeExp\"].mean()\n\n56.0585"
  },
  {
    "objectID": "IntroCoding_Lecture5.html",
    "href": "IntroCoding_Lecture5.html",
    "title": "6  Data Summaries",
    "section": "",
    "text": "In this lecture we will get to know and become experts in: 1. Data Manipulation with pandas * Handling Files * Counting and Summary Statistics * Grouped Operations 2. Plotting * matplotlib * pandas\nAnd if you want to delve deeper, look at the Advanced topics\nRelevant DataCamp lessons:"
  },
  {
    "objectID": "IntroCoding_Lecture5.html#data-manipulation-with-pandas",
    "href": "IntroCoding_Lecture5.html#data-manipulation-with-pandas",
    "title": "6  Data Summaries",
    "section": "Data Manipulation with pandas",
    "text": "Data Manipulation with pandas\nWhile we have seen panda’s ability to (i) mix data types (strings, numbers, categories, Boolean, …) and (ii) refer to columns and rows by names, this library offers a lot more powerful tools for efficiently gaining insights from data, e.g.\n\nsummarize/aggregate data in efficient pivot style manners\nhandling missing values\nvisualize/plot data\n\n\n!pip install gapminder\nfrom gapminder import gapminder"
  },
  {
    "objectID": "IntroCoding_Lecture5.html#handling-files",
    "href": "IntroCoding_Lecture5.html#handling-files",
    "title": "6  Data Summaries",
    "section": "Handling Files",
    "text": "Handling Files\nGet to know your friends\n\npd.read_csv\npd.read_table\npd.read_excel\n\nBut before that we need to connect to our Google drives ! (more instructions can be found here)\n\n\"Sam\" + \" Altman\" \n\n'Sam Altman'\n\n\nCounting and Summary Statistics\n\ngapminder.sort_values(by=\"year\").head()\n\n\n\n\n\n  \n    \n      \n      country\n      continent\n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      Asia\n      1952\n      28.801\n      8425333\n      779.445314\n    \n    \n      528\n      France\n      Europe\n      1952\n      67.410\n      42459667\n      7029.809327\n    \n    \n      540\n      Gabon\n      Africa\n      1952\n      37.003\n      420702\n      4293.476475\n    \n    \n      1656\n      West Bank and Gaza\n      Asia\n      1952\n      43.160\n      1030585\n      1515.592329\n    \n    \n      552\n      Gambia\n      Africa\n      1952\n      30.000\n      284320\n      485.230659\n    \n  \n\n\n\n\n\n#How many countries?\nCtryCts = gapminder[\"country\"].value_counts()\nCtryCts\n#note the similarity with np.unique(..., return_counts=True)\n\nAfghanistan          12\nPakistan             12\nNew Zealand          12\nNicaragua            12\nNiger                12\n                     ..\nEritrea              12\nEquatorial Guinea    12\nEl Salvador          12\nEgypt                12\nZimbabwe             12\nName: country, Length: 142, dtype: int64"
  },
  {
    "objectID": "IntroCoding_Lecture5.html#grouped-operations",
    "href": "IntroCoding_Lecture5.html#grouped-operations",
    "title": "6  Data Summaries",
    "section": "Grouped Operations",
    "text": "Grouped Operations\nThe gapminder data is a good example for wanting to apply functions to subsets to data that correspond to categories, e.g. * by year * by country * by continent\nThe powerful pandas .groupby() method enables exactly this goal rather elegantly and efficiently.\nFirst, think how you could possibly compute the average GDP seprataley for each continent. The numpy.mean(..., axis=...) will not help you.\nInstead you will have to manually find all continents and then use Boolean logic:\n\ncontinents =np.unique(gapminder[\"continent\"])\ncontinents\n\narray(['Africa', 'Americas', 'Asia', 'Europe', 'Oceania'], dtype=object)\n\n\n\nAfricaRows = gapminder[\"continent\"]==\"Africa\"\ngapminder[AfricaRows][\"gdpPercap\"].mean()\n\n\n#you could use a for loop instead, of course\ngapminder[gapminder[\"continent\"]==\"Africa\"][\"gdpPercap\"].mean()\ngapminder[gapminder[\"continent\"]==\"Americas\"][\"gdpPercap\"].mean()\ngapminder[gapminder[\"continent\"]==\"Asia\"][\"gdpPercap\"].mean()\ngapminder[gapminder[\"continent\"]==\"Europe\"][\"gdpPercap\"].mean()\ngapminder[gapminder[\"continent\"]==\"Oceania\"][\"gdpPercap\"].mean()\n\n18621.609223333333\n\n\nInstead, we should embrace the concept of grouping by a variable\n\ngapminder.mean()\n\nFutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n  gapminder.mean()\n\n\nyear         1.979500e+03\nlifeExp      5.947444e+01\npop          2.960121e+07\ngdpPercap    7.215327e+03\ndtype: float64\n\n\n\nbyContinent = gapminder.groupby(\"continent\")\nbyContinent.mean()\n\nFutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  byContinent.mean()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n    \n      continent\n      \n      \n      \n      \n    \n  \n  \n    \n      Africa\n      1979.5\n      48.865330\n      9.916003e+06\n      2193.754578\n    \n    \n      Americas\n      1979.5\n      64.658737\n      2.450479e+07\n      7136.110356\n    \n    \n      Asia\n      1979.5\n      60.064903\n      7.703872e+07\n      7902.150428\n    \n    \n      Europe\n      1979.5\n      71.903686\n      1.716976e+07\n      14469.475533\n    \n    \n      Oceania\n      1979.5\n      74.326208\n      8.874672e+06\n      18621.609223\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n#only lifeExp:\nbyContinent[\"lifeExp\"].max()\n#maybe there is more to life than the mean\n\n76.442\n\n\n\nbyContinent[\"gdpPercap\"].agg([min,max, np.mean])\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      min\n      max\n      mean\n    \n    \n      continent\n      \n      \n      \n    \n  \n  \n    \n      Africa\n      241.165876\n      21951.21176\n      2193.754578\n    \n    \n      Americas\n      1201.637154\n      42951.65309\n      7136.110356\n    \n    \n      Asia\n      331.000000\n      113523.13290\n      7902.150428\n    \n    \n      Europe\n      973.533195\n      49357.19017\n      14469.475533\n    \n    \n      Oceania\n      10039.595640\n      34435.36744\n      18621.609223\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n#multiple aggregating functions (no built in function mean)\ngapminder.groupby(\"continent\")[\"gdpPercap\"].agg([min,max, np.mean])\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      min\n      max\n      mean\n    \n    \n      continent\n      \n      \n      \n    \n  \n  \n    \n      Africa\n      241.165876\n      21951.21176\n      2193.754578\n    \n    \n      Americas\n      1201.637154\n      42951.65309\n      7136.110356\n    \n    \n      Asia\n      331.000000\n      113523.13290\n      7902.150428\n    \n    \n      Europe\n      973.533195\n      49357.19017\n      14469.475533\n    \n    \n      Oceania\n      10039.595640\n      34435.36744\n      18621.609223\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nbyContinentYear = gapminder.groupby([\"continent\", \"year\"])[\"gdpPercap\"]\nbyContinentYear.mean()\n\n\n#multiple keys\ngapminder[\"past1990\"] = gapminder[\"year\"] > 1990\nbyContinentYear = gapminder.groupby([\"continent\", \"past1990\"])[\"gdpPercap\"]\nbyContinentYear.mean()\n\ncontinent  past1990\nAfrica     False        1997.008411\n           True         2587.246913\nAmericas   False        6051.047533\n           True         9306.236000\nAsia       False        6713.113041\n           True        10280.225202\nEurope     False       11341.142807\n           True        20726.140986\nOceania    False       15224.015414\n           True        25416.796842\nName: gdpPercap, dtype: float64\n\n\n\n\nTitanic data\n\n# Since pandas does not have any built in data, I am going to \"cheat\" and \n# make use of the `seaborn` library\nimport seaborn as sns \n\ntitanic = sns. load_dataset('titanic')\ntitanic[\"3rdClass\"] = titanic[\"pclass\"]==3\ntitanic[\"male\"] = titanic[\"sex\"]==\"male\"\n\ntitanic\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      survived\n      pclass\n      sex\n      age\n      sibsp\n      parch\n      fare\n      embarked\n      class\n      who\n      adult_male\n      deck\n      embark_town\n      alive\n      alone\n      3rdClass\n      male\n    \n  \n  \n    \n      0\n      0\n      3\n      male\n      22.0\n      1\n      0\n      7.2500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      False\n      True\n      True\n    \n    \n      1\n      1\n      1\n      female\n      38.0\n      1\n      0\n      71.2833\n      C\n      First\n      woman\n      False\n      C\n      Cherbourg\n      yes\n      False\n      False\n      False\n    \n    \n      2\n      1\n      3\n      female\n      26.0\n      0\n      0\n      7.9250\n      S\n      Third\n      woman\n      False\n      NaN\n      Southampton\n      yes\n      True\n      True\n      False\n    \n    \n      3\n      1\n      1\n      female\n      35.0\n      1\n      0\n      53.1000\n      S\n      First\n      woman\n      False\n      C\n      Southampton\n      yes\n      False\n      False\n      False\n    \n    \n      4\n      0\n      3\n      male\n      35.0\n      0\n      0\n      8.0500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      True\n      True\n      True\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      0\n      2\n      male\n      27.0\n      0\n      0\n      13.0000\n      S\n      Second\n      man\n      True\n      NaN\n      Southampton\n      no\n      True\n      False\n      True\n    \n    \n      887\n      1\n      1\n      female\n      19.0\n      0\n      0\n      30.0000\n      S\n      First\n      woman\n      False\n      B\n      Southampton\n      yes\n      True\n      False\n      False\n    \n    \n      888\n      0\n      3\n      female\n      NaN\n      1\n      2\n      23.4500\n      S\n      Third\n      woman\n      False\n      NaN\n      Southampton\n      no\n      False\n      True\n      False\n    \n    \n      889\n      1\n      1\n      male\n      26.0\n      0\n      0\n      30.0000\n      C\n      First\n      man\n      True\n      C\n      Cherbourg\n      yes\n      True\n      False\n      True\n    \n    \n      890\n      0\n      3\n      male\n      32.0\n      0\n      0\n      7.7500\n      Q\n      Third\n      man\n      True\n      NaN\n      Queenstown\n      no\n      True\n      True\n      True\n    \n  \n\n891 rows × 17 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n#overall survival rate\ntitanic.survived.mean()\n\n0.3838383838383838\n\n\nTasks:\n\ncompute the proportion of survived separately for\n\nmale/female\nthe three classes\nPclass and sex\n\ncompute the mean age separately for male/female\n\n\n#I would like to compute the mean survical seprately for each group\nbySex = titanic.groupby(\"sex\")\n#here I am specifically asking for the mean\nbySex[\"survived\"].mean()\n#if you want multiple summaries, you can list them all inside the agg():\nbySex[\"survived\"].agg([min, max, np.mean ])\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      min\n      max\n      mean\n    \n    \n      sex\n      \n      \n      \n    \n  \n  \n    \n      female\n      0\n      1\n      0.742038\n    \n    \n      male\n      0\n      1\n      0.188908\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n#I would like to compute the mean survical seprately for each group\nbySexPclass = titanic.groupby([\"pclass\", \"sex\"])\n#here I am specifically asking for the mean\nbySexPclass[\"survived\"].mean()\n\npclass  sex   \n1       female    0.968085\n        male      0.368852\n2       female    0.921053\n        male      0.157407\n3       female    0.500000\n        male      0.135447\nName: survived, dtype: float64\n\n\n\nbySex = titanic.groupby(\"sex\")\n#here I am specifically asking for the mean\nbySex[\"survived\"].mean()"
  },
  {
    "objectID": "IntroCoding_Lecture5.html#plotting",
    "href": "IntroCoding_Lecture5.html#plotting",
    "title": "6  Data Summaries",
    "section": "Plotting",
    "text": "Plotting\nWe will not spend much time with basic plots in matplotlib but instead move quickly toward the pandas versions of these functions.\n\n#%matplotlib inline\nimport matplotlib.pyplot as plt\n\n#plt.rcParams['figure.dpi'] = 800\nyear = [1950, 1970, 1990, 2010]\npop = [2.519, 3.692, 5.263, 6.972]\nplt.plot(year, pop)\n#plt.bar(year, pop)\n#plt.scatter(year, pop)\nplt.xlabel('Year')\nplt.ylabel('Population')\nplt.title('World Population')\nx = 1\n#plt.show()\n\n\n\n\npandas offers plots directly from its objects\n\ntitanic.age.hist()\nplt.show()\n\n\n\n\nAnd often the axis labels are taken care of\n\n#titanic.groupby(\"pclass\").survived.mean().plot.bar()\nSurvByPclass = titanic.groupby(\"pclass\").survived.mean()\n\nSurvByPclass.plot(kind=\"bar\", title = \"Mean Survival\");\n\n<Axes: title={'center': 'Mean Survival'}, xlabel='pclass'>\n\n\n\n\n\nBut you can customize each plot as you wish:\n\nSurvByPclass.plot(kind=\"bar\", x = \"Passenger Class\", y = \"Survived\", title = \"Mean Survival\");\n\n<Axes: title={'center': 'Mean Survival'}, xlabel='pclass'>\n\n\n\n\n\nTasks:\n\nCompute the avg. life expectancy in the gapminder data for each year\nPlot this as a line plot and give meaningful x and y labels and a title\n\n\nlifeExpbyYear = gapminder.groupby(\"year\")[\"lifeExp\"].mean()\n\nlifeExpbyYear.plot(y= \"avg. life Exp\", title = \"Average life Expectancvy per year\");\n\n<Axes: title={'center': 'Average life Expectancvy per year'}, xlabel='year'>"
  },
  {
    "objectID": "IntroCoding_Lecture5.html#advanced-topics",
    "href": "IntroCoding_Lecture5.html#advanced-topics",
    "title": "6  Data Summaries",
    "section": "Advanced topics",
    "text": "Advanced topics\n\nCreating Dataframes\n\nZip\nFrom list of dicts\n\n\n\nIndexing:\n\nmultilevel indexes\nsorting\nasking for ranges"
  },
  {
    "objectID": "IntroCoding_Lecture5.html#types-of-columns",
    "href": "IntroCoding_Lecture5.html#types-of-columns",
    "title": "6  Data Summaries",
    "section": "Types of columns",
    "text": "Types of columns\n\ncategorical\ndates\n\n\n# Creating Dataframes\n#using zip\n# List1\nName = ['tom', 'krish', 'nick', 'juli']\n  \n# List2\nAge = [25, 30, 26, 22]\n  \n# get the list of tuples from two lists.\n# and merge them by using zip().\nlist_of_tuples = list(zip(Name, Age))\nlist_of_tuples = zip(Name, Age)\n# Assign data to tuples.\n#print(list_of_tuples)\n  \n  \n# Converting lists of tuples into\n# pandas Dataframe.\ndf = pd.DataFrame(list_of_tuples,\n                  columns=['Name', 'Age'])\n  \n# Print data.\ndf\n\n\n\n\n\n  \n    \n      \n      Name\n      Age\n    \n  \n  \n    \n      0\n      tom\n      25\n    \n    \n      1\n      krish\n      30\n    \n    \n      2\n      nick\n      26\n    \n    \n      3\n      juli\n      22\n    \n  \n\n\n\n\n\n#from list of dicts\ndata = [{'a': 1, 'b': 2, 'c': 3},\n        {'a': 10, 'b': 20, 'c': 30}]\n  \n# Creates DataFrame.\ndf = pd.DataFrame(data)\n  \ndf\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1\n      2\n      3\n    \n    \n      1\n      10\n      20\n      30\n    \n  \n\n\n\n\n\n# Indexing:\n\nadvLesson = True\nif advLesson:\n    frame2 = frame.set_index([\"year\", \"state\"])\n    print(frame2)\n    frame3 = frame2.sort_index()\n    print(frame3)\n    print(frame.loc[:,\"state\":\"year\"])\n\n             pop\nyear state      \n2000 Ohio    1.5\n2001 Ohio    1.7\n2002 Ohio    3.6\n2001 Nevada  2.4\n2002 Nevada  2.9\n2003 Nevada  3.2\n             pop\nyear state      \n2000 Ohio    1.5\n2001 Nevada  2.4\n     Ohio    1.7\n2002 Nevada  2.9\n     Ohio    3.6\n2003 Nevada  3.2\n    state  year\n0    Ohio  2000\n1    Ohio  2001\n2    Ohio  2002\n3  Nevada  2001\n4  Nevada  2002\n5  Nevada  2003\n\n\n\nInplace\nNote that I reassigned the objects in the code above. That is because most operations, such as set_index, sort_index, drop, etc. do not operate inplace unless specified!"
  },
  {
    "objectID": "IntroCoding_Lecture6.html",
    "href": "IntroCoding_Lecture6.html",
    "title": "7  Missing Values/Duplicates",
    "section": "",
    "text": "In this lecture we will continue our journey of Data Manipulation with pandas after reviewing some fundamental aspects of the syntax"
  },
  {
    "objectID": "IntroCoding_Lecture6.html#plotting",
    "href": "IntroCoding_Lecture6.html#plotting",
    "title": "7  Missing Values/Duplicates",
    "section": "Plotting",
    "text": "Plotting\nThe “plot type of the day” is one of the most popular ones used to display data distributions, the boxplot.\nBoxplots, also known as box-and-whisker plots, are a statistical visualization tool that provides a concise summary of a dataset’s distribution. They display key descriptive statistics and provide insights into the central tendency, variability, and skewness of the data. Here’s a brief introduction and motivation for using boxplots:\n\nStructure of Boxplots: Boxplots consist of a box and whiskers that represent different statistical measures of the data:\n\nThe box represents the interquartile range (IQR), which spans from the lower quartile (25th percentile) to the upper quartile (75th percentile). The width of the box indicates the spread of the middle 50% of the data.\nA line (whisker) extends from each end of the box to show the minimum and maximum values within a certain range (often defined as 1.5 times the IQR).\nPoints beyond the whiskers are considered outliers and plotted individually.\n\nMotivation for Using Boxplots: Boxplots offer several benefits and are commonly used for the following reasons:\n\nVisualizing Data Distribution: Boxplots provide a concise overview of the distribution of a dataset. They show the skewness, symmetry, and presence of outliers, allowing for quick identification of key features.\nComparing Groups: Boxplots enable easy visual comparison of multiple groups or categories. By placing side-by-side boxplots, you can assess differences in central tendency and variability between groups.\nOutlier Detection: Boxplots explicitly mark outliers, aiding in the identification of extreme values or data points that deviate significantly from the overall pattern.\nData Summary: Boxplots summarize key statistics, including the median, quartiles, and range, providing a quick understanding of the dataset without the need for detailed calculations.\nRobustness: Boxplots are relatively robust to skewed or asymmetric data and can effectively handle datasets with outliers.\n\n\nBoxplots are widely used in various fields, including data analysis, exploratory data visualization, and statistical reporting. They offer a clear and concise representation of data distribution, making them a valuable tool for understanding and communicating the characteristics of a dataset.\n\n!pip install gapminder\nfrom gapminder import gapminder\n\n\ngapminder\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      country\n      continent\n      year\n      lifeExp\n      pop\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      Asia\n      1952\n      28.801\n      8425333\n      779.445314\n    \n    \n      1\n      Afghanistan\n      Asia\n      1957\n      30.332\n      9240934\n      820.853030\n    \n    \n      2\n      Afghanistan\n      Asia\n      1962\n      31.997\n      10267083\n      853.100710\n    \n    \n      3\n      Afghanistan\n      Asia\n      1967\n      34.020\n      11537966\n      836.197138\n    \n    \n      4\n      Afghanistan\n      Asia\n      1972\n      36.088\n      13079460\n      739.981106\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1699\n      Zimbabwe\n      Africa\n      1987\n      62.351\n      9216418\n      706.157306\n    \n    \n      1700\n      Zimbabwe\n      Africa\n      1992\n      60.377\n      10704340\n      693.420786\n    \n    \n      1701\n      Zimbabwe\n      Africa\n      1997\n      46.809\n      11404948\n      792.449960\n    \n    \n      1702\n      Zimbabwe\n      Africa\n      2002\n      39.989\n      11926563\n      672.038623\n    \n    \n      1703\n      Zimbabwe\n      Africa\n      2007\n      43.487\n      12311143\n      469.709298\n    \n  \n\n1704 rows × 6 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nThe pandas way\n\ngapminder.boxplot(column = \"lifeExp\", by=\"continent\");\n\n\n\n\nThe matplotlib way\n\nplt.boxplot(gapminder[\"continent\"], gapminder[\"lifeExp\"]);\n\n\nTask\n\nCreate a boxplot for gdpPercap instead. What do you notice ? Are you happy with how the plot looks? Any “trick” you can think to make this more readable?\nAdvanced: can you create boxplots for gdpPerCap and lifeExp in one command?\n\n\ngapminder.boxplot(column = \"gdpPercap\", by=\"continent\");\nplt.yscale(\"log\")\n\n\n\n\n\nFurther Reading:\n\nPython Plotting With Matplotlib Tutorial.)\n\n\nimport numpy as np\nfrom scipy.stats import entropy\n\np = np.array([1/100, 99/100])\nn=2\n#p = np.array(np.ones)/n\nH = entropy(p, base=2)\nH\n\n0.08079313589591118"
  },
  {
    "objectID": "IntroCoding_Lecture7.html",
    "href": "IntroCoding_Lecture7.html",
    "title": "8  Intro to Models",
    "section": "",
    "text": "In this lecture we will learn about modeling data for the first time. After this lesson, you should know what we generally mean by a “model”, what linear regression is and how to interpret the output. But first we need to introduce a new data type: categorical variables.\nOnline Resources:\nChapter 7.5 of our textbook introduces categorical variables."
  },
  {
    "objectID": "IntroCoding_Lecture7.html#categorical-variables",
    "href": "IntroCoding_Lecture7.html#categorical-variables",
    "title": "8  Intro to Models",
    "section": "Categorical variables",
    "text": "Categorical variables\nAs a motivation, take another look at the gapminder data which contains variables of a mixed type: numeric columns along with string type columns which contain repeated instances of a smaller set of distinct or discrete values which\n\nare not numeric (but could be represented as numbers)\ncannot really be ordered\ntypically take on a finite set of values, or categories.\n\nWe refer to these data types as categorical.\nWe have already seen functions like unique and value_counts, which enable us to extract the distinct values from an array and compute their frequencies.\nBoxplots and grouping operations typically use a categorical variable to compute summaries of a numerical variables for each category separately, e.g.\n\ngapminder.boxplot(column = \"lifeExp\", by=\"continent\",figsize=(5, 2));\nplt.title('Life expectancy by continent')\n# Remove the default suptitle\nplt.suptitle(\"\");\n\n\n\n\npandas has a special Categorical extension type for holding data that uses the integer-based categorical representation or encoding. This is a popular data compression technique for data with many occurrences of similar values and can provide significantly faster performance with lower memory use, especially for string data.\n\ngapminder.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1704 entries, 0 to 1703\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   country    1704 non-null   object \n 1   continent  1704 non-null   object \n 2   year       1704 non-null   int64  \n 3   lifeExp    1704 non-null   float64\n 4   pop        1704 non-null   int64  \n 5   gdpPercap  1704 non-null   float64\ndtypes: float64(2), int64(2), object(2)\nmemory usage: 80.0+ KB\n\n\n\ngapminder['country'] = gapminder['country'].astype('category')\ngapminder.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1704 entries, 0 to 1703\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype   \n---  ------     --------------  -----   \n 0   country    1704 non-null   category\n 1   continent  1704 non-null   object  \n 2   year       1704 non-null   int64   \n 3   lifeExp    1704 non-null   float64 \n 4   pop        1704 non-null   int64   \n 5   gdpPercap  1704 non-null   float64 \ndtypes: category(1), float64(2), int64(2), object(1)\nmemory usage: 75.2+ KB\n\n\nWe will come back to the usefulness of this later."
  },
  {
    "objectID": "IntroCoding_Lecture7.html#tables-as-models",
    "href": "IntroCoding_Lecture7.html#tables-as-models",
    "title": "8  Intro to Models",
    "section": "Tables as models",
    "text": "Tables as models\nFor now let us look at our first “model”:\n\ntitanic = sns.load_dataset('titanic')\ntitanic[\"class3\"] = (titanic[\"pclass\"]==3)\ntitanic[\"male\"] = (titanic[\"sex\"]==\"male\")\ntitanic.head()\n\n\n\n\n\n  \n    \n      \n      survived\n      pclass\n      sex\n      age\n      sibsp\n      parch\n      fare\n      embarked\n      class\n      who\n      adult_male\n      deck\n      embark_town\n      alive\n      alone\n      class3\n      male\n    \n  \n  \n    \n      0\n      0\n      3\n      male\n      22.0\n      1\n      0\n      7.2500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      False\n      True\n      True\n    \n    \n      1\n      1\n      1\n      female\n      38.0\n      1\n      0\n      71.2833\n      C\n      First\n      woman\n      False\n      C\n      Cherbourg\n      yes\n      False\n      False\n      False\n    \n    \n      2\n      1\n      3\n      female\n      26.0\n      0\n      0\n      7.9250\n      S\n      Third\n      woman\n      False\n      NaN\n      Southampton\n      yes\n      True\n      True\n      False\n    \n    \n      3\n      1\n      1\n      female\n      35.0\n      1\n      0\n      53.1000\n      S\n      First\n      woman\n      False\n      C\n      Southampton\n      yes\n      False\n      False\n      False\n    \n    \n      4\n      0\n      3\n      male\n      35.0\n      0\n      0\n      8.0500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      True\n      True\n      True\n    \n  \n\n\n\n\n\nvals1, cts1 = np.unique(titanic[\"class3\"], return_counts=True)\nprint(cts1)\nprint(vals1)\n\n[400 491]\n[False  True]\n\n\n\nprint(\"The mean survival on the Titanic was\", np.mean(titanic.survived))\n\nThe mean survival on the Titanic was 0.3838383838383838\n\n\n\nConTbl = pd.crosstab(titanic[\"sex\"], titanic[\"survived\"])\nConTbl\n\n\n\n\n\n  \n    \n      survived\n      0\n      1\n    \n    \n      sex\n      \n      \n    \n  \n  \n    \n      female\n      81\n      233\n    \n    \n      male\n      468\n      109\n    \n  \n\n\n\n\nWhat are the estimated survival probabilities?\n\n#the good old groupby way:\nbySex = titanic.groupby(\"sex\").survived\nbySex.mean()\n\nsex\nfemale    0.742038\nmale      0.188908\nName: survived, dtype: float64\n\n\n\np3D = pd.crosstab([titanic[\"sex\"], titanic[\"class3\"]], titanic[\"survived\"])\np3D\n\n\n\n\n\n  \n    \n      \n      survived\n      0\n      1\n    \n    \n      sex\n      class3\n      \n      \n    \n  \n  \n    \n      female\n      False\n      9\n      161\n    \n    \n      True\n      72\n      72\n    \n    \n      male\n      False\n      168\n      62\n    \n    \n      True\n      300\n      47\n    \n  \n\n\n\n\nWhat are the estimated survival probabilities?\n\n#the good old groupby way:\nbySex = titanic.groupby([\"sex\", \"class3\"]).survived\nbySex.mean()\n\nsex     class3\nfemale  False     0.947059\n        True      0.500000\nmale    False     0.269565\n        True      0.135447\nName: survived, dtype: float64\n\n\nThe above table can be looked at as a model, which is defined as a function which takes inputs x and “spits out” a prediction:\n\\(y = f(\\mathbf{x})\\)\nIn our case, the inputs are \\(x_1=\\text{sex}\\), \\(x_2=\\text{class3}\\), and the output is the estimated survival probability!\nIt is evident that we could keep adding more input variables and make finer and finer grained predictions.\n\nLinear Models\n\nlsFit = smf.ols('survived ~ sex:class3-1', titanic).fit()\nlsFit.summary().tables[1]\n\n\n\n\n                               coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  sex[female]:class3[False]     0.9471     0.029    32.200  0.000     0.889     1.005\n\n\n  sex[male]:class3[False]       0.2696     0.025    10.660  0.000     0.220     0.319\n\n\n  sex[female]:class3[True]      0.5000     0.032    15.646  0.000     0.437     0.563\n\n\n  sex[male]:class3[True]        0.1354     0.021     6.579  0.000     0.095     0.176\n\n\n\n\n\n\nModeling Missing Values\nWe have already seen how to detect and how to replace missing values. But the latter -until now- was rather crude: we often replaced all values with a “global” average.\nClearly, we can do better than replacing all missing entries in the survived column with the average \\(0.38\\).\n\nrng = default_rng()\n\nmissingRows = rng.integers(0,890,20)\nprint(missingRows)\n#introduce missing values\ntitanic.iloc[missingRows,0] = np.nan\nnp.sum(titanic.survived.isna())\n\n[864 299 857 182 808 817 802 295 255 644   1 685 452 463 303 551 517 502\n 495 412]\n\n\n20\n\n\n\npredSurv = lsFit.predict()\nprint( len(predSurv))\npredSurv[titanic.survived.isna()]\n\n891\n\n\narray([0.94705882, 0.13544669, 0.5       , 0.26956522, 0.94705882,\n       0.94705882, 0.94705882, 0.26956522, 0.26956522, 0.13544669,\n       0.5       , 0.13544669, 0.26956522, 0.5       , 0.26956522,\n       0.26956522, 0.26956522, 0.26956522, 0.26956522, 0.26956522])\n\n\n\nFrom categorical to numerical relations\n\nurl = \"https://drive.google.com/file/d/1UbZy5Ecknpl1GXZBkbhJ_K6GJcIA2Plq/view?usp=share_link\" \nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\nauto = pd.read_csv(url)\nauto.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mpg\n      cylinders\n      displacement\n      horsepower\n      weight\n      acceleration\n      year\n      origin\n      name\n      Manufacturer\n    \n  \n  \n    \n      0\n      18.0\n      8\n      307.0\n      130\n      3504\n      12.0\n      70\n      1\n      chevrolet chevelle malibu\n      chevrolet\n    \n    \n      1\n      15.0\n      8\n      350.0\n      165\n      3693\n      11.5\n      70\n      1\n      buick skylark 320\n      buick\n    \n    \n      2\n      18.0\n      8\n      318.0\n      150\n      3436\n      11.0\n      70\n      1\n      plymouth satellite\n      plymouth\n    \n    \n      3\n      16.0\n      8\n      304.0\n      150\n      3433\n      12.0\n      70\n      1\n      amc rebel sst\n      amc\n    \n    \n      4\n      17.0\n      8\n      302.0\n      140\n      3449\n      10.5\n      70\n      1\n      ford torino\n      ford\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nplt.figure(figsize=(5,3))\nplt.scatter(x=auto[\"weight\"], y=auto[\"mpg\"]);"
  },
  {
    "objectID": "IntroCoding_Lecture7.html#linear-regression",
    "href": "IntroCoding_Lecture7.html#linear-regression",
    "title": "8  Intro to Models",
    "section": "Linear Regression",
    "text": "Linear Regression\nWe can roughly estimate, i.e. “model” this relationship with a straight line:\n\\[\ny = \\beta_0 + \\beta_1 x\n\\]\n\nplt.figure(figsize=(5,3))\ntmp=sns.regplot(x=auto[\"weight\"], y=auto[\"mpg\"], order=1, ci=95, \n                scatter_kws={'color':'b', 's':9}, line_kws={'color':'r'})\n\n\n\n\nRemind yourself of the definition of the slope of a straight line\n\\[\n\\beta_1 = \\frac{\\Delta y}{\\Delta x} =  \\frac{y_2-y_1}{x_2-x_1}\n\\]\n\nest = smf.ols('mpg ~ weight', auto).fit()\nest.summary().tables[1]\n\n\n\n\n               coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept    46.2165     0.799    57.867  0.000    44.646    47.787\n\n\n  weight       -0.0076     0.000   -29.645  0.000    -0.008    -0.007\n\n\n\n\n\nnp.corrcoef(auto[\"weight\"], auto[\"mpg\"])\n\narray([[ 1.        , -0.83224421],\n       [-0.83224421,  1.        ]])\n\n\n\nFurther Reading:"
  },
  {
    "objectID": "DS_Lecture2.html",
    "href": "DS_Lecture2.html",
    "title": "9  Sampling Distributions",
    "section": "",
    "text": "Overview\nImporting libraries\nImporting the Birthweights Dataframe"
  },
  {
    "objectID": "DS_Lecture2.html#operator-overloading",
    "href": "DS_Lecture2.html#operator-overloading",
    "title": "9  Sampling Distributions",
    "section": "Operator Overloading",
    "text": "Operator Overloading\nThe [] operator is overloaded. This means, that depending on the inputs, pandas will do something completely different. Here are the rules for the different objects you pass to just the indexing operator.\n\nstring — return a column as a Series\nlist of strings — return all those columns as a DataFrame\na slice — select rows (can do both label and integer location — confusing!)\na sequence of booleans — select all rows where True\n\nIn summary, primarily just the indexing operator selects columns, but if you pass it a sequence of booleans it will select all rows that are True.\n\ndf = df[(df[ \"dbirwt\"] < 6000) & (df[ \"dbirwt\"] > 2000)] # 2000 < birthweight < 6000\ndf#.head()\n\n\n\n\n\n  \n    \n      \n      sex\n      dbirwt\n    \n  \n  \n    \n      0\n      male\n      2551\n    \n    \n      1\n      male\n      2778\n    \n    \n      2\n      female\n      2976\n    \n    \n      3\n      female\n      3345\n    \n    \n      4\n      female\n      3175\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      4995\n      male\n      4405\n    \n    \n      4996\n      male\n      2764\n    \n    \n      4997\n      female\n      2776\n    \n    \n      4998\n      female\n      3615\n    \n    \n      4999\n      male\n      3379\n    \n  \n\n4909 rows × 2 columns\n\n\n\n\nBoxplot of weight vs. sex\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      dbirwt\n    \n  \n  \n    \n      count\n      4909.000000\n    \n    \n      mean\n      3480.557344\n    \n    \n      std\n      529.280103\n    \n    \n      min\n      2012.000000\n    \n    \n      25%\n      3146.000000\n    \n    \n      50%\n      3486.000000\n    \n    \n      75%\n      3827.000000\n    \n    \n      max\n      5981.000000\n    \n  \n\n\n\n\n\ntmp=df.boxplot( \"dbirwt\",\"sex\")\nplt.tight_layout()\n\n\n\n\n\nWe notice a small difference in the average weight, which is more clearly visible when we plot overlaying densities for male/female\n\nbwghtBySex = np.round(df[[\"dbirwt\",\"sex\"]].groupby(\"sex\")[[\"dbirwt\"]].mean())\n\nprint(bwghtBySex, '\\n')\nprint('mean: ',bwghtBySex.mean())\n\n        dbirwt\nsex           \nfemale  3427.0\nmale    3533.0 \n\nmean:  dbirwt    3480.0\ndtype: float64\n\n\n\ntmp=df[[\"dbirwt\",\"sex\"]].groupby(\"sex\")[\"dbirwt\"].plot(kind='density', legend=True)"
  },
  {
    "objectID": "DS_Lecture2.html#ab-testing",
    "href": "DS_Lecture2.html#ab-testing",
    "title": "9  Sampling Distributions",
    "section": "A/B Testing",
    "text": "A/B Testing\nLet us hypothesize that one wanted to classify babies into male/female solely based on their weight. What would its accuracy be if we applied the following simple rule:\nif dbirwt > 3480 y = male else y = female\nThis would be the equivalent of testing for global warming by measuring the temperature on one day. We all know that it took a long time (= many samples) to reliably detect a small difference like 0.5 degrees buried in the noise. Let us apply the same idea here. Maybe we can build a high-accuracy classifier if we weighed enough babies separately for each sex.\n\nConfusion Matrix for simple classifier\n\ndf[\"predMale\"] = (df[\"dbirwt\"] > 3480)\nConfMat = pd.crosstab(df[\"predMale\"], df[\"sex\"])\nConfMat\n\n\n\n\n\n  \n    \n      sex\n      female\n      male\n    \n    \n      predMale\n      \n      \n    \n  \n  \n    \n      False\n      1331\n      1105\n    \n    \n      True\n      1100\n      1373\n    \n  \n\n\n\n\n\nN = np.sum(ConfMat.values)\nacc1 = np.round( (ConfMat.values[0,0]+ConfMat.values[1,1]) / N, 3)\n\n#Acc0 = (1331+1373)/5000\nprint(\"Accuracy of lame classifier:\", acc1)\n#Think about the baseline accuracy\n\nAccuracy of lame classifier: 0.551"
  },
  {
    "objectID": "DS_Lecture2.html#distributions",
    "href": "DS_Lecture2.html#distributions",
    "title": "9  Sampling Distributions",
    "section": "Distributions",
    "text": "Distributions\n\nMean Density Comparison Function\nWrite a function which:\n\ndraws repeated (e.g. M=500) random samples of size n (e.g. 40, 640) from each sex from the data\nComputes the stdevs for the sample means of each sex separately\nRepeats the above density plot for the sample mean distributions\ncomputes the confusion matrix/accuracy of a classifier that applies the rule \\(\\bar{x} > 3480\\).\n\nHint: np.random.choice(df[\"dbirwt\"],2)\n\ndef mean_density_comparison(df_cleaned, M=500, n=10):\n    \n    #Generate a sex iteration array\n    sex_iter = ['male', 'female']\n    \n    #Create an empty DataFrame with 'sex' and 'dbirwt' column\n    columns = ['sex', 'dbirwt']\n    df_new = pd.DataFrame(columns=columns)\n    \n    #Create an empty array to store the standard deviation of the differnt sex 'male' = std_dev[0], 'female' = std_dev[1]\n    std_dev = np.empty(2)\n    \n    #Iterate over sex and create a specific data subset\n    for ind,v in enumerate(sex_iter):\n        subset = df_cleaned[df_cleaned.sex == v]\n        \n        #create M random sample means of n samples and add it to df_new\n        for i in range(M):\n            rand_samples = np.random.choice(subset.dbirwt, n)\n            x = np.mean(rand_samples)#sample mean per sex\n            df_new.loc[len(df_new)+1] = [v, x]\n        \n        #plot male and female data and calculate the standard deviation of the data\n        plot_data = df_new[df_new.sex == v]\n        std_dev[ind] = np.std(plot_data['dbirwt'])  \n        \n        plot_data.dbirwt.plot.density()\n        plt.xlabel('dbirwt')\n        plt.legend(sex_iter)\n        #plt.grid()\n        #plt.title(\"n=\" + str(n))\n        \n    #return the sample mean data\n    return df_new\n \n        \n\n\nTesting the Function\n\nSM10 = mean_density_comparison(df, M=500, n=10)\nSM640 = mean_density_comparison(df, M=500, n=640)\n\n\n\n\n\nSM10[\"predMale\"] = (SM10[\"dbirwt\"] > 3480)\nConfMat10 = pd.crosstab(SM10[\"predMale\"], SM10[\"sex\"])\nConfMat10\n\n\n\n\n\n  \n    \n      sex\n      female\n      male\n    \n    \n      predMale\n      \n      \n    \n  \n  \n    \n      False\n      320\n      182\n    \n    \n      True\n      180\n      318\n    \n  \n\n\n\n\n\nSM640[\"predMale\"] = (SM640[\"dbirwt\"] > 3480)\nConfMat640 = pd.crosstab(SM640[\"predMale\"], SM640[\"sex\"])\nConfMat640\n\n\n\n\n\n  \n    \n      sex\n      female\n      male\n    \n    \n      predMale\n      \n      \n    \n  \n  \n    \n      False\n      498\n      0\n    \n    \n      True\n      2\n      500\n    \n  \n\n\n\n\n\ngrouped640 = SM640[\"dbirwt\"].groupby(SM640[\"sex\"])\n\nprint(\"n=640, means:\", grouped640.mean())\nprint()\nprint(\"n=640, SESMs:\", grouped640.std())\n\nn=640, means: sex\nfemale    3427.276397\nmale      3533.081803\nName: dbirwt, dtype: float64\n\nn=640, SESMs: sex\nfemale    20.937106\nmale      19.819436\nName: dbirwt, dtype: float64\n\n\n\nSM40 = mean_density_comparison(df, M=500, n=40)\n\n\n\n\n\ngrouped40 = SM40[\"dbirwt\"].groupby(SM40[\"sex\"])\n\nprint(\"n=40, means:\", grouped40.mean())\nprint()\nprint(\"n=40, SESMs:\", grouped40.std())\n\nn=40, means: sex\nfemale    3423.30740\nmale      3527.34015\nName: dbirwt, dtype: float64\n\nn=40, SESMs: sex\nfemale    82.787145\nmale      84.921927\nName: dbirwt, dtype: float64\n\n\nHow much smaller is \\(\\sigma_{\\bar{x},640}\\) than \\(\\sigma_{\\bar{x},40}\\) ? Compare that factor to the ratio of the sample sizes \\(640/40 = 16\\)\n\n\n\nEmpirical Cumulative Distribution Function\nThe density -like a histogram- has a few complications that include the arbitrary choice of bin width (kernel width for density) and the loss of information. Welcome to the empirical cumulative distribution function ecdf\nECDF Function\n\ndef ecdf(data):\n    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n\n    # Number of data points: n\n    n = len(data)\n\n    # x-data for the ECDF: x\n    x = np.sort(data)\n\n    # y-data for the ECDF: y\n    y = np.arange(1, n+1) / n\n\n    return x, y\n\nECDF Plot\n\n# Compute ECDF for sample size 40: m_40, f_40\nmale40 = SM40[SM40.sex == \"male\"][\"dbirwt\"]\nfemale40 = SM40[SM40.sex == \"female\"][\"dbirwt\"]\n\nmx_40, my_40 = ecdf(male40)\nfx_40, fy_40 = ecdf(female40)\n\n# Plot all ECDFs on the same plot\nfig, ax = plt.subplots()\n_ = ax.plot(mx_40, my_40, marker = '.', linestyle = 'none')\n_ = ax.plot(fx_40, fy_40, marker = '.', linestyle = 'none')\n\n# Make nice margins\nplt.margins(0.02)\n\n# Annotate the plot\nplt.legend(('male', 'female'), loc='lower right')\n_ = plt.xlabel('birth weight(g)')\n_ = plt.ylabel('ECDF')\n\n# Display the plot\nplt.grid()\nplt.show()\n\n\n\n\n\n\nWhat is the relationship to quantiles/percentiles ?\nFind the IQR !\nSketch the densities just from the ecdf.\n\n\n\n\nChecking Normality of sample mean distribution\n\n# Compute mean and standard deviation: mu, sigma\nmu = np.mean(male40)\nsigma = np.std(male40)\n\n# Sample out of a normal distribution with this mu and sigma: samples\nsamples = np.random.normal(mu, sigma, 10000)\n\n# Get the CDF of the samples and of the data\nx_theor, y_theor = ecdf(samples)\n\n# Plot the CDFs and show the plot\n_ = plt.plot(mx_40, my_40, marker='.', linestyle='none')\n_ = plt.plot(x_theor, y_theor)\n\nplt.margins(0.02)\n_ = plt.xlabel('birth weight (g)')\n_ = plt.ylabel('CDF')\n_ = plt.title('CDF of Birthweight')\n\nplt.grid()\nplt.show()\n\n\n\n\nTasks\n\nFind the “5% tails” which are just the (0.05, 0.95) quantiles\nRead up on theoretical quantiles: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy.stats.norm\nstone age: get the “5% tails” from a normal table.\nHow many stdevs do you need to cover the 90% sample interval ?\nCan you replace the “empirical theoretical cdf” from above with the exact line without sampling 10000 random numbers from a normal distribution ?\n\n\nLet us recap what we observed when sampling from a “population”: The sample mean distribution gets narrower with increasing sample size n, SESM =\\(\\sigma_{\\bar{x}} = \\sigma/\\sqrt{n}\\). Take a look at this interactive applet for further understanding.\nHow is this useful ? And how is it relevant because in reality we would only have one sample, not hundreds !\nSmall Tasks\n\nChoose one random sample of size n=40 from the male babies and compute \\(\\bar{x}\\), \\(\\hat{\\sigma}\\). Assume all that is known to you, are these two summary statistics. In particular, we do not know the true mean \\(\\mu\\)!\nArgue intuitively with the ecdf plot about plausible values of \\(\\mu\\).\nMore precisely: what interval around \\(\\bar{x}\\) would contain \\(\\mu\\) with 90% probability ?"
  },
  {
    "objectID": "DS_Lecture2.html#hacker-statistic",
    "href": "DS_Lecture2.html#hacker-statistic",
    "title": "9  Sampling Distributions",
    "section": "Hacker Statistic",
    "text": "Hacker Statistic\nThe ability to draw new samples from a population with a known mean is a luxury that we usually do not have. Is there any way to “fake” new samples using just the one “lousy” sample we have at hand ? This might sound like an impossible feat analogously to “pulling yourself up by your own bootstraps”!\nBootstrap Ilustration\nBut that is exactly what we will try now:\nTasks\n\nLook up the help for np.random.choice() \nDraw repeated samples of size n=40 from the sample above.\nCompute the mean of each sample and store it an array.\nPlot the histogram\nCompute the stdev of this distribution and compare to the SEM. \nWrite a function that computes bootstrap replicates of the mean from a sample.\nGeneralize this function to accept any summary statistic, not just the mean."
  },
  {
    "objectID": "DS_Lecture3.html",
    "href": "DS_Lecture3.html",
    "title": "10  Hypothesis Tests",
    "section": "",
    "text": "Overview"
  },
  {
    "objectID": "DS_Lecture3.html#warmupreview-exercises",
    "href": "DS_Lecture3.html#warmupreview-exercises",
    "title": "10  Hypothesis Tests",
    "section": "WarmUp/Review Exercises",
    "text": "WarmUp/Review Exercises\nLoad the Auto.csv data (into a dataframe cars) for the following tasks and create a scatter plot mpg ~ weight. Take a look at row with index \\(25\\) (i.e. row 26); we will refer to that data point as \\(x_{25}\\) from now on.\n\nCompute the standard score for\n\n\\(x_{25}\\)[“mpg”]\n\\(x_{25}\\)[“weight”]\n\nCompute the product of these standard scores (call it \\(p_{25}\\)).\nIf you repeated this process for all rows of the cars dataframe and averaged all products \\(p_{i}\\), what would the resulting number tell you ? (What is it called?)\nTake a bootstrap sample from cars.mpg and compute the mean\n\n\nImporting Standard Libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n#pd.options.mode.chained_assignment = None # disable chained assignment warning\nimport seaborn as sns\n\n\nLoading our Functions\n\n%run ./ourFunctions.py\n%precision 3\n\n'%.3f'\n\n\n\nRead in the Cars DF\n\n#cars = pd.read_csv('../data/Auto.csv')\ncars = pd.read_csv(\"https://raw.githubusercontent.com/markusloecher/DataScience-HWR/main/data/Auto.csv\")\n#d\ncars.shape\n\n(392, 10)\n\n\n\ncars.head()\n\n\n\n\n\n  \n    \n      \n      mpg\n      cylinders\n      displacement\n      horsepower\n      weight\n      acceleration\n      year\n      origin\n      name\n      Manufacturer\n    \n  \n  \n    \n      0\n      18.0\n      8\n      307.0\n      130\n      3504\n      12.0\n      70\n      1\n      chevrolet chevelle malibu\n      chevrolet\n    \n    \n      1\n      15.0\n      8\n      350.0\n      165\n      3693\n      11.5\n      70\n      1\n      buick skylark 320\n      buick\n    \n    \n      2\n      18.0\n      8\n      318.0\n      150\n      3436\n      11.0\n      70\n      1\n      plymouth satellite\n      plymouth\n    \n    \n      3\n      16.0\n      8\n      304.0\n      150\n      3433\n      12.0\n      70\n      1\n      amc rebel sst\n      amc\n    \n    \n      4\n      17.0\n      8\n      302.0\n      140\n      3449\n      10.5\n      70\n      1\n      ford torino\n      ford\n    \n  \n\n\n\n\n\nprint(cars.iloc[25,])\n\nmpg                  10.0\ncylinders               8\ndisplacement        360.0\nhorsepower            215\nweight               4615\nacceleration         14.0\nyear                   70\norigin                  1\nname            ford f250\nManufacturer         ford\nName: 25, dtype: object\n\n\n\nplt.scatter(\"weight\", \"mpg\",data=cars)\nplt.xlabel(\"weight\")\nplt.ylabel(\"mpg\")\nplt.show()"
  },
  {
    "objectID": "DS_Lecture3.html#hypothesis-tests",
    "href": "DS_Lecture3.html#hypothesis-tests",
    "title": "10  Hypothesis Tests",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nWe have learned about the bootstrap as a slick way of resampling your data to obtain sampling distributions of various measures of interest, Without having to learn highly specific distributions (such as the \\(\\chi^2\\), Poisson, Binomial or F-distribution) the bootstrap enables us to\n\nget confidence intervals\nperform one-sample tests\nperform two-sample tests\n\nImagine the EPA requiring the average mpg for 4-cylinder cars to be at least \\(\\mu_0 = 30\\) and needs to decide -based on this sample only- whether the manufacturers need to implement some improvements. In statistical jargon: can the EPA reject the claim that the true mean is at least 30 ?\n\ncars4=cars[cars[\"cylinders\"]==4]\nempirical_mean = np.mean(cars4.mpg)\nempirical_mean\n\n29.28391959798995\n\n\n\nempirical_sd = np.std(cars4.mpg)\nempirical_sd\n\n5.656280603443601\n\n\nCompute a confidence interval of the true mpg of 4-cyl cars via the bootstrap ! (Is there just “THE CI” ??)\n\nmpg_bs = draw_bs_reps(cars4.mpg, func = np.mean, size=1000)\n\n\nnp.percentile(mpg_bs, [2.5, 97.5])\n\narray([28.498, 30.093])\n\n\n\nfrom scipy.stats import norm\nnorm.ppf(0.975)\n\n1.959963984540054\n\n\n\nThere, we have it, the empirical mean is in fact below 30. Is this sufficient evidence for the EPA to demand improvements ? Does \\(\\bar{x} < \\mu_0\\) “prove” that \\(\\mu < \\mu_0\\) ??\nThink about an experiment which tries to establish whether a pair of dice in a casino is biased (“gezinkt” in German). You toss the dice \\(50\\) times and observe an average number of pips of \\(\\bar{x} = 3.8\\) which is clearly “way above” the required value of \\(\\mu_0 = 3.5\\) for a clean pair of dice. Should you go to the authorities and file a complaint ? Since no one really knows the true value of \\(\\mu\\) for these dice, how would a court decide whether they are unbiased ?\n\nInnocent until proven guilty: Always assume the opposite of what you want to illustrate ! That way you can gather evidence against this Null hypothesis, e.g. \\(H_0: \\mu = 3.5 (\\equiv \\mu_0)\\).\nDefine a test statistic which is pertinent to the question you are seeking to answer (computed entirely from your sample). In this case, your test statistics could be e.g. the (scaled?) difference between the observed sample mean and the claim: \\(\\bar{x} - \\mu_0\\).\nWe define the p-value as the probability of observing a test statistic equally or more extreme than the one observed, given that \\(H_0\\) is true.\nIf the observed value of your test statistic seems very unlikely under \\(H_0\\) we would favor the alternative hypothesis which is usually complementary, e.g. \\(H_A: \\mu \\neq 3.5\\)\n\nNotice that we used vague expressions such as unlikely in the procedure above. In a legal context, we would want to make statements such as “beyond a reasonable doubt”. Well, in statistics we ry to quantifiy that notion by limiting the type-I error probability \\(\\alpha\\) to be at most e.g. \\(0.01\\) or \\(0.05\\). So none our decisions are ever “proofs” or free of error.\nIt also means that you, the person conducting the hypothesis test, need to specify a value of \\(\\alpha\\), which clearly introduces a certain amount of subjectivity. Later on, we will discuss the inherent tradeoffs between type-I and type-II errors: only then will one fully understand the non-arbitrary choice of \\(\\alpha\\).\n\nParametric Tests\n\nFrom the sample, compute \\(\\bar{x}\\) and \\(\\hat{\\sigma}\\).\nCompute the test statistic\n\n\\[t = \\frac{\\bar{x} - \\mu_0}{\\hat{\\sigma}/\\sqrt{n}}\\]\n\nReject \\(H_0\\)\n\nTwo-sided test: if \\(|t| > t_{n-1, 1-\\alpha/2}\\)\nLeft-sided test: if \\(t < t_{n-1, \\alpha}\\)\nRight-sided test: if \\(t > t_{n-1, 1-\\alpha}\\)\nIf \\(n>50\\) one may replace the t distribution with the normal\n\nDefine the p-value as twice the tail area of a t distribution\nAlternatively one rejects the Null if p-val \\(< \\alpha\\).\n\nBack to the casino example, assume the following empirical data:\n\\(\\bar{x} = 3.8, n = 100, \\hat{\\sigma} = 1.7\\)\n\\(H_0: \\mu = 3.5\\), \\(H_A: \\mu \\neq 3.5\\), \\(\\alpha = 0.01\\)\n\nfrom scipy.stats import norm\n# that is the critical value that we compare our test statistic with\nprint('two-sided cv: ',norm.ppf(1-0.01/2)) # returns the norm score\nprint('one-sided cv: ',norm.ppf(1-0.01))\n\ntwo-sided cv:  2.5758293035489004\none-sided cv:  2.3263478740408408\n\n\n\nTwo sided test\n\n# We care about both sides since we do not know how the casino is using the biased dice.\na=0.01\nn=200\n\ncritValue = norm.ppf(1-a/2)\nz = np.abs((3.8-3.5)/(1.7/np.sqrt(n))) \n\nif (z < critValue):\n    s = \"not\"\n    s2 = \"fail to\"\nelse:\n    s = \"\"   \n    s2 = \"\"\n    \nprint(\"The test statistic t =\", np.round(z,3), \"is\", s ,\"larger than the critical value\",np.round(critValue,3), \"so we\",s2,\"reject the Null\" ) \nnorm.cdf(1-a/2)\n\nThe test statistic t = 2.496 is not larger than the critical value 2.576 so we fail to reject the Null\n\n\n0.840131867824506\n\n\n\n\nOne sided test!\n\n# We care only about one side since we do know that the casino makes \n# money off upward biased dice.\n\ncritValue = norm.ppf(1-a)\nz = (3.8-3.5)/(1.7/np.sqrt(n))\nif (z < critValue):\n    s = \"not\"\n    s2 = \"fail to\"\nelse:\n    s = \"\"   \n    s2 = \"\"\n    \nprint(\"The test statistic t =\", np.round(z,3), \"is\", s ,\"larger than the critical value\",np.round(critValue,3), \"so we\",s2,\"reject the Null\" ) \n#norm.cdf(1-a)\n\nThe test statistic t = 2.496 is  larger than the critical value 2.326 so we  reject the Null\n\n\n\n\np-value\n\n#p-value is the right tail probability of values equal or larger than your test statistic\npVal = 1-norm.cdf(z)\nprint(\"The p value \", np.round(pVal,3), \"is less than alpha\", a, \"so we reject the Null\")\n\nThe p value  0.006 is less than alpha 0.01 so we reject the Null\n\n\n\n\n\n\nNon parametric Tests\nParametric Tests require many assumptions, that may not always be true, and are also a bit abstract. It often helps (i) robustness and (ii) the understanding of the testing process to use simulations instead. We now learn about two different ways of such simulations, each following the basic “pipeline”:\n\nClearly define the Null hypothesis.\nDefine the test statistic.\nGenerate many sets of simulated data (bootstrap replicates) assuming the null hypothesis is true\nCompute the test statistic for each simulated data set\nThe p-value is the fraction of your simulated data sets for which the test statistic is at least as extreme as for the real data\n\n\nBootstrap Hypothesis Tests\nBesides computing confidence intervals we can also use the bootstrap to perform hypothesis test. We need to “fake” the process of drawing new samples again and again under the Null hypothesis, which might appear impossible since our one sample likely will have a sample mean not equal to \\(\\mu_0\\).\nThe trick is to “shift” our data such that \\(\\bar{x} = \\mu_0\\)! As a test statistic we use the difference of the mean of the bootstrap value minus \\(\\mu_0\\). Notice that there is no need to scale this difference!\nWe illustrate this with our casino example.\n\nnp.random.choice(6,20)+1\n\narray([5, 6, 4, 5, 2, 2, 3, 2, 5, 6, 4, 1, 3, 4, 5, 5, 6, 5, 5, 4])\n\n\n\nnp.random.seed(123)\nn=400\n#x = np.random.choice(6,n,p=[0.14,0.14,0.15,0.18,0.19,0.20])+1 #add a bias\nx = np.random.choice(6,n)+1 \n#cheating:\nx = x + 0.2 #add a bias\na=0.01\n\nmu0=3.5\nxBar = np.round(np.mean(x),2)\nobsDiff = np.round(mu0-xBar,2)\nprint(\"The sample mean is\", xBar, \", so we shift our data by\", obsDiff)\n\nThe sample mean is 3.65 , so we shift our data by -0.15\n\n\n\n\n#Alternatively, if we had not shifted our data\n#TS = bs_mean_dice-xBar\n\n#The test statistic\nbs_mean_dice = draw_bs_reps(x+obsDiff,np.mean,10000)\nTS = bs_mean_dice-mu0\nplt.hist(TS);\n\n\n\n\n\n#two sided p value\npVal = np.mean( abs(TS) > abs(obsDiff))\nprint(\"The two sided p value of\", np.round(pVal,3), \"is not smaller than alpha=\",a,\"so we fail to reject the Null.\" ) \n\nThe two sided p value of 0.012 is not smaller than alpha= 0.01 so we fail to reject the Null.\n\n\n\n\n\nTasks\n\nTest \\(H_0: \\mu \\geq 30, H_A: \\mu < 30\\) for the mean mpg of 4-cylinder cars\n\nusing bootstrap replicates\nvia standard testing theory.\n\nCompute the corresponding p-values.\n\n\nmu0=30\nempirical_mean = np.mean(cars4.mpg)\n\n\n#assume the claim is true !!\nshift = mu0-empirical_mean\ncars4Shifted= cars4.mpg+shift\nbs_mean_mpg = draw_bs_reps(cars4Shifted,np.mean,10000)\n\n#The test statistic\nTS = bs_mean_mpg-mu0\npVal = np.mean( TS < -shift)\n\n\n#assume the claim is true !!\nshift = mu0-empirical_mean\ncars4Shifted= cars4.mpg+shift\nbs_mean_mpg = draw_bs_reps(cars4Shifted,np.mean,10000)\n\n#The test statistic\nTS = bs_mean_mpg-mu0\npVal = np.mean( TS < -shift)\n\nplt.hist(TS);\n\n#np.percentile(bs_mean_mpg,5)\n#p value is simply the left tail beyond xBar\n#np.mean(bs_mean_mpg < empirical_mean)\n\n\n\n\n\nprint('shift =',shift)\nprint(\"pValue =\", pVal)\n\nshift = 0.7160804020100429\npValue = 0.0375\n\n\n\n\n\n\nFrom one sample to 2 samples\nFrom Auto to birth weights\n\npreg = pd.read_csv('data/pregNSFG.csv.gz', compression='gzip')\n\n#only look at live births\nfirsts = preg[(preg.outcome == 1) & (preg.birthord == 1)]\n#live[live.babysex == 1].babysex = \"male\"\n\n#we reduce the sample size further by conditioning on \n#the mother's age at the end of pregnancy\nfirsts = firsts[(firsts.agepreg < 30) & (firsts.prglngth >= 30)]\nbwt = firsts[[\"babysex\",\"totalwgt_lb\"]] \n\nbwt.babysex.replace([1.0],\"male\",inplace=True)\nbwt.babysex.replace([2.0],\"female\",inplace=True)\nbwt = bwt.dropna()\n\n\nprint('shape:',bwt.shape)\nbwt.head()\n\nshape: (3758, 2)\n\n\n\n\n\n\n  \n    \n      \n      babysex\n      totalwgt_lb\n    \n  \n  \n    \n      2\n      male\n      9.1250\n    \n    \n      5\n      male\n      8.5625\n    \n    \n      8\n      male\n      7.5625\n    \n    \n      10\n      male\n      7.8125\n    \n    \n      11\n      female\n      7.0000\n    \n  \n\n\n\n\n\ngrouped = bwt[\"totalwgt_lb\"].groupby(bwt[\"babysex\"])\ngrouped.mean()\n\nbabysex\nfemale    7.103830\nmale      7.378682\nName: totalwgt_lb, dtype: float64\n\n\n\ntmp=grouped.plot(kind='density', legend=True);\n\n\n\n\n\n\nA two-sample bootstrap hypothesis test for difference of means\nA one sample test compares a data set to one fixed number !\nWe now want to compare two sets of data, both of which are samples! In particular test the hypothesis that male and female babies have the same biological weight (but not necessarily the same distribution).\n\\(H_0: \\mu_m = \\mu_f, H_A: \\mu_m \\neq \\mu_f\\)\nTo do the two-sample bootstrap test, we shift both arrays to have the same mean, since we are simulating the hypothesis that their means are, in fact, equal (equal to what value ??). We then draw bootstrap samples out of the shifted arrays and compute the difference in means. This constitutes a bootstrap replicate, and we generate many of them. The p-value is the fraction of replicates with a difference in means greater than or equal to what was observed.\n\nmeanNull = np.mean(bwt.totalwgt_lb)# pooled mean\nw_m = bwt[bwt[\"babysex\"]==\"male\"].totalwgt_lb\nw_f = bwt[bwt[\"babysex\"]==\"female\"].totalwgt_lb\nempirical_diff_means = np.mean(w_m)-np.mean(w_f) \n#shift:\n\nw_m_shifted = w_m - np.mean(w_m) + meanNull\nw_f_shifted = w_f - np.mean(w_f) + meanNull\n\n\n# Compute 10,000 bootstrap replicates from shifted arrays\nM=1000\nbs_replicates_m = draw_bs_reps(w_m_shifted, np.mean, M)\nbs_replicates_f = draw_bs_reps(w_f_shifted, np.mean, M)\n\n# Get replicates of difference of means: bs_replicates\nbs_replicates = bs_replicates_m - bs_replicates_f\n\ntmp=plt.hist(bs_replicates)\n# Compute and print p-value: p\n\n\n\n\n\n#debugging:\nif 0:\n    meanNull\n    #plt.hist(w_f_shifted)\n    #plt.hist(bs_replicates_m)\n    #bs_replicates_m\n    w_m_bs = np.random.choice(w_m, size=len(w_m))\n    np.nanmean(w_m_bs)\n    np.argwhere(np.isnan(w_m_bs))\n    np.argwhere(np.isnan(w_m))\n\n#p-value (one-sided):\npVal = np.mean(bs_replicates> empirical_diff_means)\n#cutoff right tail\n#np.percentile(bs_replicates, 95)\nprint(\"The one sided p value of\", np.round(pVal,3), \"is much smaller than alpha=\",a,\", so we fail to reject the Null\" ) \n\nprint(\"-> The observed difference of \", np.round(empirical_diff_means,5), \"is exremely unlikely to have occurred by chance alone\")\n\nThe one sided p value of 0.0 is much smaller than alpha= 0.01 , so we fail to reject the Null\n-> The observed difference of  0.275 is exremely unlikely to have occurred by chance alone"
  },
  {
    "objectID": "DS_Lecture4.html",
    "href": "DS_Lecture4.html",
    "title": "11  AB Testing",
    "section": "",
    "text": "Permutation Tests\nBinary Processes\n\nAB Tests\nBinomial Distribution\nImporting Our Functions"
  },
  {
    "objectID": "DS_Lecture4.html#permutation-2-sample-test",
    "href": "DS_Lecture4.html#permutation-2-sample-test",
    "title": "11  AB Testing",
    "section": "Permutation 2-sample test",
    "text": "Permutation 2-sample test\nWe have used the bootstrap to compare two sets of data, both of which are samples. In particular, we can test two-sample hypotheses such as\n\\(H_0: \\mu_m = \\mu_f, H_A: \\mu_m \\neq \\mu_f\\)\nor the one-sided versions:\n\\(H_0: \\mu_m = \\mu_f, H_A: \\mu_m > \\mu_f\\)\n\\(H_0: \\mu_m = \\mu_f, H_A: \\mu_m < \\mu_f\\)\nAnother way to compare 2 distributions (in some ways much more straightforward than the bootstrap) is permutation sampling. It directly simulates the hypothesis that two variables have identical probability distributions.\nA permutation sample of two arrays having respectively \\(n_1\\) and \\(n_2\\) entries is constructed by concatenating the arrays together, scrambling the contents of the concatenated array, and then taking the first \\(n_1\\) entries as the permutation sample of the first array and the last \\(n_2\\) entries as the permutation sample of the second array.\nAt DataCamp the first example offers a nice visualization of this process:\n\n\n\nTake a look at the code in ourFunctions.py to run a permutation test\n\nLet us apply our first permutation sampling on the Titanic data. (First, we explore the data a bit)\n\ntitanic = sns.load_dataset('titanic')\ntitanic.head()\n\n\n\n\n\n  \n    \n      \n      survived\n      pclass\n      sex\n      age\n      sibsp\n      parch\n      fare\n      embarked\n      class\n      who\n      adult_male\n      deck\n      embark_town\n      alive\n      alone\n    \n  \n  \n    \n      0\n      0\n      3\n      male\n      22.0\n      1\n      0\n      7.2500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      False\n    \n    \n      1\n      1\n      1\n      female\n      38.0\n      1\n      0\n      71.2833\n      C\n      First\n      woman\n      False\n      C\n      Cherbourg\n      yes\n      False\n    \n    \n      2\n      1\n      3\n      female\n      26.0\n      0\n      0\n      7.9250\n      S\n      Third\n      woman\n      False\n      NaN\n      Southampton\n      yes\n      True\n    \n    \n      3\n      1\n      1\n      female\n      35.0\n      1\n      0\n      53.1000\n      S\n      First\n      woman\n      False\n      C\n      Southampton\n      yes\n      False\n    \n    \n      4\n      0\n      3\n      male\n      35.0\n      0\n      0\n      8.0500\n      S\n      Third\n      man\n      True\n      NaN\n      Southampton\n      no\n      True\n    \n  \n\n\n\n\n\nPclassSurv = titanic.groupby(['pclass', 'survived'])\nPclassSurv.size()\n\npclass  survived\n1       0            80\n        1           136\n2       0            97\n        1            87\n3       0           372\n        1           119\ndtype: int64\n\n\n\npd.crosstab(titanic.pclass, titanic.survived,margins=True)\n\n\n\n\n\n  \n    \n      survived\n      0\n      1\n      All\n    \n    \n      pclass\n      \n      \n      \n    \n  \n  \n    \n      1\n      80\n      136\n      216\n    \n    \n      2\n      97\n      87\n      184\n    \n    \n      3\n      372\n      119\n      491\n    \n    \n      All\n      549\n      342\n      891\n    \n  \n\n\n\n\n\nWomenOnly = titanic[titanic[\"sex\"]==\"female\"]\npd.crosstab(WomenOnly.pclass, WomenOnly.survived,margins=True)\n\n\n\n\n\n  \n    \n      survived\n      0\n      1\n      All\n    \n    \n      pclass\n      \n      \n      \n    \n  \n  \n    \n      1\n      3\n      91\n      94\n    \n    \n      2\n      6\n      70\n      76\n    \n    \n      3\n      72\n      72\n      144\n    \n    \n      All\n      81\n      233\n      314\n    \n  \n\n\n\n\n\nTest the claim that the survival chances of women in 1st and 2nd class were pretty much the same.\n\nWrite down the Null hypothesis and test statistic\nWrite code that generates permutation samples from two data sets\nGenerate many permutation replicates for the relevant Titanic subset\nCompute a p-value\n\nWe could choose alpha = 0.05, but keep in mind the following - would you step into a plane that has a 5% crash probability ? - Would you buy a drug that has a 5% chance of severe side effects ?\nWhat is the difference between these two methods (bootstrap, permutation) ?\nTesting the hypothesis that two samples have the same distribution may be done with a bootstrap test, but a permutation test is preferred because it is more accurate (exact, in fact). But a permutation test is not as versatile as the bootstrap.\nWe often want to test the hypothesis that population A and population B have the same mean, but not necessarily the same distribution. This is difficult with a permutation test as it assumes exchangeability.\nWe will get back to this topic!\nMore info.."
  },
  {
    "objectID": "DS_Lecture4.html#sample-t-test",
    "href": "DS_Lecture4.html#sample-t-test",
    "title": "11  AB Testing",
    "section": "2-sample t test",
    "text": "2-sample t test\nOf course there is an equivalent fully parametric 2-sample test, the t-test.\nWe first read in the The National Survey of Family Growth data from the think stats book.\nLook at section 1.7 for a description of the variables.\n\n#preg = pd.read_hdf('data/pregNSFG.h5', 'df')\npreg = pd.read_csv('data/pregNSFG.csv.gz', compression='gzip')\n#only look at live births\nlive = preg[preg.outcome == 1]\n\n#define first babies\nfirsts = live[live.birthord == 1]\n\n#and all others\nothers = live[live.birthord != 1]\n\n\ntRes = stats.ttest_ind(firsts.prglngth.values, others.prglngth.values)\np = pd.Series([tRes.pvalue,tRes.statistic], index = ['p-value:', 'test statistic:'])\np\n\np-value:           0.167554\ntest statistic:    1.380215\ndtype: float64\n\n\n\n#ttest_ind often underestimates p for unequal variances:\ntRes = stats.ttest_ind(firsts.prglngth.values, others.prglngth.values, equal_var = False)\np = pd.Series([tRes.pvalue,tRes.statistic], index = ['p-value:', 'test statistic:'])\np\n\np-value:           0.168528\ntest statistic:    1.377059\ndtype: float64\n\n\n\nCan you reproduce the first p-value from the test statistic ?\n\n\n\n\n\n\n## Random Walks\n\n\n\n\n### Simulating Many Random Walks at Once\n\n\nIf your goal was to simulate many random walks, say 5,000 of them, you can generate all of the random walks with minor modifications to the above code. The numpy.random functions if passed a 2-tuple will generate a 2D array of draws, and we can compute the cumulative sum across the rows to compute all 5,000 random walks in one shot\n\n\n::: {.cell execution_count=15}\n\n\n::: {.cell execution_count=16}\n\n\n::: {.cell-output .cell-output-display execution_count=16}\n\n\n::: {.cell execution_count=17} ``` {.python .cell-code} nwalks = 5000 nsteps = 10000 steps = random.choice([-1,1], size=(nwalks, nsteps), p=(0.9,0.1)) # 0 or 1 #steps = np.where(draws > 0, 1, 0) #steps = np.where(np.random.rand() <= 0.1,1,-1)\n\n\nwalks = steps.cumsum(1) ``` :::\n\n\n::: {.cell execution_count=18}\n\n\n::: {.cell-output .cell-output-display execution_count=18}\n\n\n::: {.cell execution_count=16}\n\n\n::: {.cell-output .cell-output-stdout}\n\n\n::: {.cell execution_count=26} ``` {.python .cell-code} fig = plt.figure() ax = fig.add_subplot(1, 1, 1)\n\n\nax.plot(walks[1,:], ‘k’, label=‘one’, linewidth=0.25) ax.plot(walks[2,:], label=‘two’, linestyle =‘–’, linewidth=0.25) ax.plot(walks[3,:], label=‘three’, linestyle =‘–’, linewidth=0.25) ax.plot(walks[4,:], label=‘four’, linestyle =‘–’, linewidth=0.25) ax.grid()\n\n\n#a very useless legend just because we can ax.legend(loc=‘best’); ```\n\n\n::: {.cell-output .cell-output-display}  ::: :::\n\n\n\n\nThe \\(\\sqrt{n}\\) law again !\nCompute the variance and/or standard deviation at times \\(1000, 4000, 9000\\).\n\nprint('std_1000:', np.std(walks[:,1000]))\nprint('std_4000:', np.std(walks[:,4000]))\nprint('std_9000:', np.std(walks[:,9000]))\n\nstd_1000: 31.985538009544253\nstd_4000: 63.65415401998522\nstd_9000: 95.417731496824\n\n\nExtra Credit\nOut of these walks, let’s compute the minimum crossing time to 30 or -30. This is slightly tricky because not all 5,000 of them reach 30. We can check this using the any method.\nWe can then use this boolean array to select out the rows of walks that actually cross the absolute 30 level and call argmax across axis 1 to get the crossing times:\n\nhits30 = (np.abs(walks) >= 30).any(1)\nhits30\nhits30.sum() # Number that hit 30 or -30\n\ncrossing_times = (np.abs(walks[hits30]) >= 30).argmax(1)\ncrossing_times.mean()\n\n892.4928\n\n\n\n\n\n\n\n\n## Tasks\n\n\n\n\n### Binomial Probability Distribution\n\n\n1. Explore the binom function from scipy.stats\n\n\n2. Size matters: insurance company A insures 100 cars, company B 400 cars. The probability of a car being stolen is 10%. Compute the probabilities that more than 15% of the respective fleets are stolen.\n\n\n4. Faced with a mutliple choice test containing 20 question with 4 choices each you decide in desparation to just guess all answers. What is the probability that you will pass, i.e. get at least 10 correct answers?\n\n\n5. Think about nonparametric versions of the above answers\n\n\n::: {.cell execution_count=29}\n\n\n\n\n\nA/B Testing\n\nPerform a permutation test on the DataCamp example:\n\n\n\n\n\nWhat does A/B testing have to do with random walks?\n\n# Construct arrays of data: campaigns A and B\nclickthroughA = np.array([True] * 45 + [False] * (500-45))\nclickthroughB = np.array([True] * 67 + [False] * (500-67))\n\nobsDiff = np.mean(clickthroughB)-np.mean(clickthroughA)\n\n# Acquire permutation samples: perm_replicates\nperm_replicates = draw_perm_reps(clickthroughB, clickthroughA, diff_of_means, 10000)\n\n\n#p-value:\nnp.mean(perm_replicates >= obsDiff)\n\n0.0185\n\n\n\n# Construct arrays of data: campaigns A and B\nclickthroughA = np.array([True] * 45 + [False] * (500-45))\nclickthroughB = np.array([True] * 67 + [False] * (500-67))\n\nobsDiff = np.mean(clickthroughB)-np.mean(clickthroughA)\n\n# Acquire permutation samples: perm_replicates\nperm_replicates = draw_perm_reps(clickthroughB, clickthroughA, diff_of_means, 10000)\n\nplt.hist(perm_replicates)\nplt.vlines(obsDiff,0,3000,colors=\"red\")\nplt.vlines(-obsDiff,0,3000, colors=\"red\");"
  },
  {
    "objectID": "DS_Lecture5_part2.html",
    "href": "DS_Lecture5_part2.html",
    "title": "12  Multiple Linear Regression",
    "section": "",
    "text": "(Chapter 3 ISL Book) How can we extend our analysis of the advertising data in order to accommodate these two additional predictors? One option is to run three separate simple linear regressions, each of which uses a different advertising medium as a predictor.\nImporting Standard Libs"
  },
  {
    "objectID": "DS_Lecture5_part2.html#other-considerations-in-the-regression-model",
    "href": "DS_Lecture5_part2.html#other-considerations-in-the-regression-model",
    "title": "12  Multiple Linear Regression",
    "section": "Other Considerations in the Regression Model",
    "text": "Other Considerations in the Regression Model\n\nCollinearity\n\n“Exact Collinearity”\nTasks\n\nYou try to impress your investors with the number of advertising channels that you have investigated. So you make a copy of the TV column, call it facebook and refit the linear multiple regression.\nMulti-Collinearity: You have learned from your foolish mistakes and are more ambitiously creating fake data: You create a new column instagram by adding TV and Radio and subtracting 3 times Newspaper. Now, refit the linear multiple regression.\n\n\nadvertising[\"facebook\"] = advertising.TV\n#advertising.info()\n\nest = smf.ols('Sales ~ TV + Radio + Newspaper + facebook', advertising).fit()\nes = est.summary()\nes.tables[1]\n\n\n\n\n               coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept     2.9389     0.312     9.422  0.000     2.324     3.554\n\n\n  TV            0.0229     0.001    32.809  0.000     0.022     0.024\n\n\n  Radio         0.1885     0.009    21.893  0.000     0.172     0.206\n\n\n  Newspaper    -0.0010     0.006    -0.177  0.860    -0.013     0.011\n\n\n  facebook      0.0229     0.001    32.809  0.000     0.022     0.024\n\n\n\n\nMulticollinearity is treated VERY differently in statsmodels than in R as discussed in this post on medium as well on stackoverflow. Worthwhile to inspect the condition number:\n\nes.tables[2]\n\n\n\n\n  Omnibus:       60.414   Durbin-Watson:         2.084\n\n\n  Prob(Omnibus):  0.000   Jarque-Bera (JB):    151.241\n\n\n  Skew:          -1.327   Prob(JB):           1.44e-33\n\n\n  Kurtosis:       6.332   Cond. No.           2.61e+16\n\n\n\n\n\nadvertising[\"instagram\"] = advertising.TV +advertising.Radio - 3*advertising.Newspaper\n#advertising.info()\n\nest = smf.ols('Sales ~ TV + Radio + Newspaper + instagram', advertising).fit()\nes = est.summary()\nes.tables[1]\n\n\n\n\n               coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept     2.9389     0.312     9.422  0.000     2.324     3.554\n\n\n  TV            0.0260     0.002    11.660  0.000     0.022     0.030\n\n\n  Radio         0.1687     0.008    22.469  0.000     0.154     0.184\n\n\n  Newspaper     0.0583     0.002    27.223  0.000     0.054     0.063\n\n\n  instagram     0.0198     0.002    10.679  0.000     0.016     0.023\n\n\n\n\n\nes.tables[2]\n\n\n\n\n  Omnibus:       60.414   Durbin-Watson:         2.084\n\n\n  Prob(Omnibus):  0.000   Jarque-Bera (JB):    151.241\n\n\n  Skew:          -1.327   Prob(JB):           1.44e-33\n\n\n  Kurtosis:       6.332   Cond. No.           2.32e+16\n\n\n\n\nFigure 3.6\nWhat about non-exact collinearities?\n\ncredit = pd.read_csv('../data/Credit.csv', usecols=list(range(1,12)))\ncredit['Student2'] = credit.Student.map({'No':0, 'Yes':1})\ncredit.head()\n\n\n\n\n\n  \n    \n      \n      Income\n      Limit\n      Rating\n      Cards\n      Age\n      Education\n      Gender\n      Student\n      Married\n      Ethnicity\n      Balance\n      Student2\n    \n  \n  \n    \n      0\n      14.891\n      3606\n      283\n      2\n      34\n      11\n      Male\n      No\n      Yes\n      Caucasian\n      333\n      0\n    \n    \n      1\n      106.025\n      6645\n      483\n      3\n      82\n      15\n      Female\n      Yes\n      Yes\n      Asian\n      903\n      1\n    \n    \n      2\n      104.593\n      7075\n      514\n      4\n      71\n      11\n      Male\n      No\n      No\n      Asian\n      580\n      0\n    \n    \n      3\n      148.924\n      9504\n      681\n      3\n      36\n      11\n      Female\n      No\n      No\n      Asian\n      964\n      0\n    \n    \n      4\n      55.882\n      4897\n      357\n      2\n      68\n      16\n      Male\n      No\n      Yes\n      Caucasian\n      331\n      0\n    \n  \n\n\n\n\n\nsns.pairplot(credit[['Balance','Age','Cards','Education','Income','Limit','Rating']]);\n\n\n\n\nFigure 3.14\n\nfig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\n\n# Left plot\nax1.scatter(credit.Limit, credit.Age, facecolor='None', edgecolor='r')\nax1.set_ylabel('Age')\n\n# Right plot\nax2.scatter(credit.Limit, credit.Rating, facecolor='None', edgecolor='r')\nax2.set_ylabel('Rating')\n\nfor ax in fig.axes:\n    ax.set_xlabel('Limit')\n    ax.set_xticks([2000,4000,6000,8000,12000])\n\n\n\n\n\n\n“High Collinearity”\nCollinearity refers to the situation in which two or more predictor variables are closely related to one another. The concept of collinearity is illustrated in Figure 3.14 using the Credit data set. In the left-hand panel of Figure 3.14, the two predictors limit and age appear to have no obvious relationship. In contrast, in the right-hand panel of Figure 3.14, the predictors limit and rating are very highly correlated with each other, and we say that they are collinear. The presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response. In other words, since limit and rating tend to increase or decrease together, it can be difficult to determine how each one separately is associated with the response, balance.\nFigure 3.15\n\ny = credit.Balance\n\n# Regression for left plot\nX = credit[['Age', 'Limit']].values\nregr1 = skl_lm.LinearRegression()\nregr1.fit(scale(X.astype('float'), with_std=False), y)\nprint('Age/Limit\\n',regr1.intercept_)\nprint(regr1.coef_)\n\n# Regression for right plot\nX2 = credit[['Rating', 'Limit']].values\nregr2 = skl_lm.LinearRegression()\nregr2.fit(scale(X2.astype('float'), with_std=False), y)\nprint('\\nRating/Limit\\n',regr2.intercept_)\nprint(regr2.coef_)\n\nAge/Limit\n 520.0150000000001\n[-2.29148553  0.17336497]\n\nRating/Limit\n 520.015\n[2.20167217 0.02451438]\n\n\n\n\n\nFigure 3.15\n\n\nFigure 3.15 illustrates some of the difficulties that can result from collinearity. The left-hand panel of Figure 3.15 is a contour plot of the RSS (3.22) associated with different possible coefficient estimates for the regression of balance on limit and age. Each ellipse represents a set of coefficients that correspond to the same RSS, with ellipses nearest to the center taking on the lowest values of RSS. The dots and associated dashed lines represent the coefficient estimates that result in the smallest possible RSS—in other words, these are the least squares estimates. The axes for limit and age have been scaled so that the plot includes possible coefficient estimates that are up to four standard errors on either side of the least squares estimates. Thus the plot includes all plausible values for the coefficients. For example, we see that the true limit coefficient is almost certainly somewhere between 0.15 and 0.20. In contrast, the right-hand panel of Figure 3.15 displays contour plots of the RSS associated with possible coefficient estimates for the regression of balance onto limit and rating, which we know to be highly collinear. Now the contours run along a narrow valley; there is a broad range of values for the coefficient estimates that result in equal values for RSS. Hence a small change in the data could cause the pair of coefficient values that yield the smallest RSS—that is, the least squares estimates—to move anywhere along this valley. This results in a great deal of uncertainty in the coefficient estimates. Notice that the scale for the limit coefficient now runs from roughly −0.2 to 0.2; this is an eight-fold increase over the plausible range of the limit coefficient in the regression with age.\n\n\n\n\n\n\n\n\n\nTable 3.11 compares the coefficient estimates obtained from two separate multiple regression models. The first is a regression of balance on age and limit, and the second is a regression of balance on rating and limit. In the first regression, both age and limit are highly significant with very small pvalues. In the second, the collinearity between limit and rating has caused the standard error for the limit coefficient estimate to increase by a factor of 12 and the p-value to increase to 0.701. In other words, the importance of the limit variable has been masked due to the presence of collinearity. To avoid such a situation, it is desirable to identify and address potential collinearity problems while fitting the model.\n\n\n\n\n\n\nQualitative/Categorical Variables\n\nDummy Coding\nInterpretation of coefficients\n\nTable 3.7\n\n\nest = smf.ols('Balance ~ Gender ', credit).fit()\nest.summary().tables[1]\n\n\n\n\n                      coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept          509.8031    33.128    15.389  0.000   444.675   574.931\n\n\n  Gender[T.Female]    19.7331    46.051     0.429  0.669   -70.801   110.267\n\n\n\n\nTable 3.8\n\nnp.unique(credit[\"Ethnicity\"])\ncredit.groupby(\"Ethnicity\")[\"Balance\"].mean()\n\nEthnicity\nAfrican American    531.000000\nAsian               512.313725\nCaucasian           518.497487\nName: Balance, dtype: float64\n\n\n\nest = smf.ols('Balance ~ Ethnicity', credit).fit()\nest.summary().tables[1]\n\n\n\n\n                            coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept                531.0000    46.319    11.464  0.000   439.939   622.061\n\n\n  Ethnicity[T.Asian]       -18.6863    65.021    -0.287  0.774  -146.515   109.142\n\n\n  Ethnicity[T.Caucasian]   -12.5025    56.681    -0.221  0.826  -123.935    98.930\n\n\n\n\n\n\nInteractions\nTable 3.9\n\nest = smf.ols('Sales ~ TV + Radio + TV*Radio', advertising).fit()\nest.summary().tables[1]\n\n\n\n\n               coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept     6.7502     0.248    27.233  0.000     6.261     7.239\n\n\n  TV            0.0191     0.002    12.699  0.000     0.016     0.022\n\n\n  Radio         0.0289     0.009     3.241  0.001     0.011     0.046\n\n\n  TV:Radio      0.0011  5.24e-05    20.727  0.000     0.001     0.001\n\n\n\n\n\nInteraction between qualitative and quantitative variables\n\ncredit[\"Income2\"] = credit[\"Income\"] + scipy.stats.norm.rvs(400)\n\n\nest1 = smf.ols('Balance ~ Income + Income2 + C(Student)', credit).fit()\nregr1 = est1.params\nest2 = smf.ols('Balance ~ Income + Income*C(Student)', credit).fit()\nregr2 = est2.params\n\nprint('Regression 1 - without interaction term')\nest1.summary().tables[1]\n\nRegression 1 - without interaction term\n\n\n\n\n\n                       coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept            -0.0123     0.002    -7.143  0.000    -0.016    -0.009\n\n\n  C(Student)[T.Yes]   382.6705    65.311     5.859  0.000   254.272   511.069\n\n\n  Income                5.4557     0.621     8.779  0.000     4.234     6.677\n\n\n  Income2               0.5287     0.081     6.506  0.000     0.369     0.688\n\n\n\n\n\nnp.mean(credit[\"Income\"])\n\n45.218885000000036\n\n\n\nest2 = smf.ols('Balance ~ Income + Income*C(Ethnicity)*C(Student)', credit).fit()\nest2.summary().tables[1]\n\n\n\n\n                                                        coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept                                            145.1346    64.922     2.236  0.026    17.492   272.777\n\n\n  C(Ethnicity)[T.Asian]                                -15.5992    94.976    -0.164  0.870  -202.332   171.133\n\n\n  C(Ethnicity)[T.Caucasian]                            122.8200    80.930     1.518  0.130   -36.297   281.937\n\n\n  C(Student)[T.Yes]                                    427.5858   246.321     1.736  0.083   -56.706   911.877\n\n\n  C(Ethnicity)[T.Asian]:C(Student)[T.Yes]              281.3665   297.683     0.945  0.345  -303.906   866.639\n\n\n  C(Ethnicity)[T.Caucasian]:C(Student)[T.Yes]         -109.7842   309.373    -0.355  0.723  -718.041   498.473\n\n\n  Income                                                 7.1950     1.069     6.728  0.000     5.093     9.298\n\n\n  Income:C(Ethnicity)[T.Asian]                           0.0963     1.667     0.058  0.954    -3.181     3.374\n\n\n  Income:C(Ethnicity)[T.Caucasian]                      -2.0995     1.371    -1.531  0.127    -4.796     0.597\n\n\n  Income:C(Student)[T.Yes]                              -0.0693     3.716    -0.019  0.985    -7.375     7.236\n\n\n  Income:C(Ethnicity)[T.Asian]:C(Student)[T.Yes]        -4.6311     4.475    -1.035  0.301   -13.429     4.167\n\n\n  Income:C(Ethnicity)[T.Caucasian]:C(Student)[T.Yes]    -0.9514     5.431    -0.175  0.861   -11.630     9.727\n\n\n\n\n\nprint('\\nRegression 2 - with interaction term')\nest2.summary().tables[1]\n\n\nRegression 2 - with interaction term\n\n\n\n\n\n                                                        coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept                                            145.1346    64.922     2.236  0.026    17.492   272.777\n\n\n  C(Ethnicity)[T.Asian]                                -15.5992    94.976    -0.164  0.870  -202.332   171.133\n\n\n  C(Ethnicity)[T.Caucasian]                            122.8200    80.930     1.518  0.130   -36.297   281.937\n\n\n  C(Student)[T.Yes]                                    427.5858   246.321     1.736  0.083   -56.706   911.877\n\n\n  C(Ethnicity)[T.Asian]:C(Student)[T.Yes]              281.3665   297.683     0.945  0.345  -303.906   866.639\n\n\n  C(Ethnicity)[T.Caucasian]:C(Student)[T.Yes]         -109.7842   309.373    -0.355  0.723  -718.041   498.473\n\n\n  Income                                                 7.1950     1.069     6.728  0.000     5.093     9.298\n\n\n  Income:C(Ethnicity)[T.Asian]                           0.0963     1.667     0.058  0.954    -3.181     3.374\n\n\n  Income:C(Ethnicity)[T.Caucasian]                      -2.0995     1.371    -1.531  0.127    -4.796     0.597\n\n\n  Income:C(Student)[T.Yes]                              -0.0693     3.716    -0.019  0.985    -7.375     7.236\n\n\n  Income:C(Ethnicity)[T.Asian]:C(Student)[T.Yes]        -4.6311     4.475    -1.035  0.301   -13.429     4.167\n\n\n  Income:C(Ethnicity)[T.Caucasian]:C(Student)[T.Yes]    -0.9514     5.431    -0.175  0.861   -11.630     9.727\n\n\n\n\n\n# Income (x-axis)\nincome = np.linspace(0,150)\n\n# Balance without interaction term (y-axis)\nstudent1 = np.linspace(regr1['Intercept']+regr1['C(Student)[T.Yes]'],\n                       regr1['Intercept']+regr1['C(Student)[T.Yes]']+150*regr1['Income'])\nnon_student1 =  np.linspace(regr1['Intercept'], regr1['Intercept']+150*regr1['Income'])\n\n# Balance with iteraction term (y-axis)\nstudent2 = np.linspace(regr2['Intercept']+regr2['C(Student)[T.Yes]'],\n                       regr2['Intercept']+regr2['C(Student)[T.Yes]']+\n                       150*(regr2['Income']+regr2['Income:C(Student)[T.Yes]']))\nnon_student2 =  np.linspace(regr2['Intercept'], regr2['Intercept']+150*regr2['Income'])\n\n# Create plot\nfig, (ax1,ax2) = plt.subplots(1,2, figsize=(12,5))\nax1.plot(income, student1, 'r', income, non_student1, 'k')\n\nax2.plot(income, student2, 'r', income, non_student2, 'k')\n\nfor ax in fig.axes:\n    ax.legend(['student', 'non-student'], loc=2)\n    ax.set_xlabel('Income')\n    ax.set_ylabel('Balance')\n    ax.set_ylim(ymax=1550)\n\n\n\n\n\n\n\nNon-linear relationships\nFigure 3.8\n\nauto = pd.read_csv('./data/Auto.csv', na_values='?').dropna()\n\n\nsns.regplot(x='weight', y='mpg', data=auto,fit_reg=True, order=2);\n\n<AxesSubplot:xlabel='weight', ylabel='mpg'>\n\n\n\n\n\n\n# With Seaborn's regplot() you can easily plot higher order polynomials.\nplt.scatter(x=auto.horsepower, y=auto.mpg, facecolors='None', edgecolors='k', alpha=.5) \nsns.regplot(x=auto.horsepower, y=auto.mpg, ci=None, label='Linear', scatter=False, color='orange')\nsns.regplot(x=auto.horsepower, y=auto.mpg, ci=None, label='Degree 2', order=2, scatter=False, color='lightblue')\nsns.regplot(x=auto.horsepower, y=auto.mpg, ci=None, label='Degree 5', order=5, scatter=False, color='g')\nplt.legend()\nplt.ylim(5,55)\nplt.xlim(40,240);\n\n\n\n\nTable 3.10\n\nauto['horsepower2'] = auto.horsepower**2\nauto.head(3)\n\n\n\n\n\n  \n    \n      \n      mpg\n      cylinders\n      displacement\n      horsepower\n      weight\n      acceleration\n      year\n      origin\n      name\n      Manufacturer\n      horsepower2\n    \n  \n  \n    \n      0\n      18.0\n      8\n      307.0\n      130\n      3504\n      12.0\n      70\n      1\n      chevrolet chevelle malibu\n      chevrolet\n      16900\n    \n    \n      1\n      15.0\n      8\n      350.0\n      165\n      3693\n      11.5\n      70\n      1\n      buick skylark 320\n      buick\n      27225\n    \n    \n      2\n      18.0\n      8\n      318.0\n      150\n      3436\n      11.0\n      70\n      1\n      plymouth satellite\n      plymouth\n      22500\n    \n  \n\n\n\n\n\nest = smf.ols('mpg ~ horsepower + horsepower2', auto).fit()\nest.summary().tables[1]\n\n\n\n\n                 coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept      56.9001     1.800    31.604  0.000    53.360    60.440\n\n\n  horsepower     -0.4662     0.031   -14.978  0.000    -0.527    -0.405\n\n\n  horsepower2     0.0012     0.000    10.080  0.000     0.001     0.001"
  },
  {
    "objectID": "DS_Homework6.html",
    "href": "DS_Homework6.html",
    "title": "13  Sample Splitting",
    "section": "",
    "text": "This challenge is based on the Boston data."
  },
  {
    "objectID": "DS_Homework6.html#find-the-best-fit",
    "href": "DS_Homework6.html#find-the-best-fit",
    "title": "13  Sample Splitting",
    "section": "Find the best fit",
    "text": "Find the best fit\nDataset Description:\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. ‘Hedonic prices and the demand for clean air’.\nFeatures Description: - CRIM: per capita crime rate by town - ZN: proportion of residential land zoned for lots over 25,000 sq.ft. - INDUS: proportion of non-retail business acres per town - CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) - NOX: nitric oxides concentration (parts per 10 million) - RM: average number of rooms per dwelling - AGE: proportion of owner-occupied units built prior to 1940 - DIS: weighted distances to five Boston employment centres - RAD: index of accessibility to radial highways - TAX: full-value property-tax rate per 10,000 dollars - PTRATIO: pupil-teacher ratio by town - BLACK: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town - LSTAT: percentage lower status of the population - MEDV: Median value of owner-occupied homes in 1000’s dollars\n\nboston = pd.read_csv('./data/Boston.csv')\n\n# independent variables\nX = boston.drop('medv', axis=1)\n\n# dependent variable\ny = boston['medv']\nprint(boston.shape)\nboston.head()\n\n\n\n\n\n  \n    \n      \n      crim\n      zn\n      indus\n      chas\n      nox\n      rm\n      age\n      dis\n      rad\n      tax\n      ptratio\n      black\n      lstat\n      medv\n    \n  \n  \n    \n      0\n      0.00632\n      18.0\n      2.31\n      0\n      0.538\n      6.575\n      65.2\n      4.0900\n      1\n      296\n      15.3\n      396.90\n      4.98\n      24.0\n    \n    \n      1\n      0.02731\n      0.0\n      7.07\n      0\n      0.469\n      6.421\n      78.9\n      4.9671\n      2\n      242\n      17.8\n      396.90\n      9.14\n      21.6\n    \n    \n      2\n      0.02729\n      0.0\n      7.07\n      0\n      0.469\n      7.185\n      61.1\n      4.9671\n      2\n      242\n      17.8\n      392.83\n      4.03\n      34.7\n    \n    \n      3\n      0.03237\n      0.0\n      2.18\n      0\n      0.458\n      6.998\n      45.8\n      6.0622\n      3\n      222\n      18.7\n      394.63\n      2.94\n      33.4\n    \n    \n      4\n      0.06905\n      0.0\n      2.18\n      0\n      0.458\n      7.147\n      54.2\n      6.0622\n      3\n      222\n      18.7\n      396.90\n      5.33\n      36.2\n    \n  \n\n\n\n\n\nboston.describe()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      crim\n      zn\n      indus\n      chas\n      nox\n      rm\n      age\n      dis\n      rad\n      tax\n      ptratio\n      black\n      lstat\n      medv\n    \n  \n  \n    \n      count\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n      506.000000\n    \n    \n      mean\n      3.613524\n      11.363636\n      11.136779\n      0.069170\n      0.554695\n      6.284634\n      68.574901\n      3.795043\n      9.549407\n      408.237154\n      18.455534\n      356.674032\n      12.653063\n      22.532806\n    \n    \n      std\n      8.601545\n      23.322453\n      6.860353\n      0.253994\n      0.115878\n      0.702617\n      28.148861\n      2.105710\n      8.707259\n      168.537116\n      2.164946\n      91.294864\n      7.141062\n      9.197104\n    \n    \n      min\n      0.006320\n      0.000000\n      0.460000\n      0.000000\n      0.385000\n      3.561000\n      2.900000\n      1.129600\n      1.000000\n      187.000000\n      12.600000\n      0.320000\n      1.730000\n      5.000000\n    \n    \n      25%\n      0.082045\n      0.000000\n      5.190000\n      0.000000\n      0.449000\n      5.885500\n      45.025000\n      2.100175\n      4.000000\n      279.000000\n      17.400000\n      375.377500\n      6.950000\n      17.025000\n    \n    \n      50%\n      0.256510\n      0.000000\n      9.690000\n      0.000000\n      0.538000\n      6.208500\n      77.500000\n      3.207450\n      5.000000\n      330.000000\n      19.050000\n      391.440000\n      11.360000\n      21.200000\n    \n    \n      75%\n      3.677083\n      12.500000\n      18.100000\n      0.000000\n      0.624000\n      6.623500\n      94.075000\n      5.188425\n      24.000000\n      666.000000\n      20.200000\n      396.225000\n      16.955000\n      25.000000\n    \n    \n      max\n      88.976200\n      100.000000\n      27.740000\n      1.000000\n      0.871000\n      8.780000\n      100.000000\n      12.126500\n      24.000000\n      711.000000\n      22.000000\n      396.900000\n      37.970000\n      50.000000\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nboston.corr()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      crim\n      zn\n      indus\n      chas\n      nox\n      rm\n      age\n      dis\n      rad\n      tax\n      ptratio\n      black\n      lstat\n      medv\n    \n  \n  \n    \n      crim\n      1.000000\n      -0.200469\n      0.406583\n      -0.055892\n      0.420972\n      -0.219247\n      0.352734\n      -0.379670\n      0.625505\n      0.582764\n      0.289946\n      -0.385064\n      0.455621\n      -0.388305\n    \n    \n      zn\n      -0.200469\n      1.000000\n      -0.533828\n      -0.042697\n      -0.516604\n      0.311991\n      -0.569537\n      0.664408\n      -0.311948\n      -0.314563\n      -0.391679\n      0.175520\n      -0.412995\n      0.360445\n    \n    \n      indus\n      0.406583\n      -0.533828\n      1.000000\n      0.062938\n      0.763651\n      -0.391676\n      0.644779\n      -0.708027\n      0.595129\n      0.720760\n      0.383248\n      -0.356977\n      0.603800\n      -0.483725\n    \n    \n      chas\n      -0.055892\n      -0.042697\n      0.062938\n      1.000000\n      0.091203\n      0.091251\n      0.086518\n      -0.099176\n      -0.007368\n      -0.035587\n      -0.121515\n      0.048788\n      -0.053929\n      0.175260\n    \n    \n      nox\n      0.420972\n      -0.516604\n      0.763651\n      0.091203\n      1.000000\n      -0.302188\n      0.731470\n      -0.769230\n      0.611441\n      0.668023\n      0.188933\n      -0.380051\n      0.590879\n      -0.427321\n    \n    \n      rm\n      -0.219247\n      0.311991\n      -0.391676\n      0.091251\n      -0.302188\n      1.000000\n      -0.240265\n      0.205246\n      -0.209847\n      -0.292048\n      -0.355501\n      0.128069\n      -0.613808\n      0.695360\n    \n    \n      age\n      0.352734\n      -0.569537\n      0.644779\n      0.086518\n      0.731470\n      -0.240265\n      1.000000\n      -0.747881\n      0.456022\n      0.506456\n      0.261515\n      -0.273534\n      0.602339\n      -0.376955\n    \n    \n      dis\n      -0.379670\n      0.664408\n      -0.708027\n      -0.099176\n      -0.769230\n      0.205246\n      -0.747881\n      1.000000\n      -0.494588\n      -0.534432\n      -0.232471\n      0.291512\n      -0.496996\n      0.249929\n    \n    \n      rad\n      0.625505\n      -0.311948\n      0.595129\n      -0.007368\n      0.611441\n      -0.209847\n      0.456022\n      -0.494588\n      1.000000\n      0.910228\n      0.464741\n      -0.444413\n      0.488676\n      -0.381626\n    \n    \n      tax\n      0.582764\n      -0.314563\n      0.720760\n      -0.035587\n      0.668023\n      -0.292048\n      0.506456\n      -0.534432\n      0.910228\n      1.000000\n      0.460853\n      -0.441808\n      0.543993\n      -0.468536\n    \n    \n      ptratio\n      0.289946\n      -0.391679\n      0.383248\n      -0.121515\n      0.188933\n      -0.355501\n      0.261515\n      -0.232471\n      0.464741\n      0.460853\n      1.000000\n      -0.177383\n      0.374044\n      -0.507787\n    \n    \n      black\n      -0.385064\n      0.175520\n      -0.356977\n      0.048788\n      -0.380051\n      0.128069\n      -0.273534\n      0.291512\n      -0.444413\n      -0.441808\n      -0.177383\n      1.000000\n      -0.366087\n      0.333461\n    \n    \n      lstat\n      0.455621\n      -0.412995\n      0.603800\n      -0.053929\n      0.590879\n      -0.613808\n      0.602339\n      -0.496996\n      0.488676\n      0.543993\n      0.374044\n      -0.366087\n      1.000000\n      -0.737663\n    \n    \n      medv\n      -0.388305\n      0.360445\n      -0.483725\n      0.175260\n      -0.427321\n      0.695360\n      -0.376955\n      0.249929\n      -0.381626\n      -0.468536\n      -0.507787\n      0.333461\n      -0.737663\n      1.000000\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nAdditive Models first\n\n#import statsmodel and sklearn Library\nimport sklearn.linear_model as skl_lm\nimport statsmodels.formula.api as smf\n\n# Create a Baseline Multiple Linear Regression\nest = smf.ols('medv ~ lstat + black + ptratio ', boston).fit()\nest.summary().tables[1]\n\ny_pred = est.predict()\n\nsklearn.metrics.r2_score(y, y_pred)\n\n0.6098453143448523\n\n\n\n# Create a Multiple Linear Regression with all columns\nest = smf.ols('medv ~ lstat + black + ptratio + tax + rad + dis + age + rm + nox +chas + indus + zn +crim ', boston).fit()\nest.summary().tables[1]\n\ny_pred = est.predict()\n\nsklearn.metrics.r2_score(y, y_pred)\n\n0.7406426641094095\n\n\n\n# Try out with Variables which have a higher correlation with medv (> 0.3)\nest = smf.ols('medv ~ lstat + black + ptratio + tax  + rad + age + rm + nox + indus + zn +crim ', boston).fit()\n\ny_pred = est.predict()\n\nsklearn.metrics.r2_score(y, y_pred)\n\n0.7062733492875524\n\n\n\n\nInteractions\nFind Variables which correlate and add their Interactions to the model\n\n# add Interactions between Variables\nest = smf.ols('medv ~ lstat + black + ptratio + tax + rad + dis + age + rm + nox +chas + indus + zn +crim + rad * crim + crim * tax + dis * zn + chas * indus + nox * indus + age * indus + dis * indus + rad * indus + tax * indus + lstat * indus + nox * dis + nox * age + nox * rad + nox * tax + rm * lstat + age * dis + age * lstat + rad * tax', boston).fit()\ny_pred = est.predict()\n\nsklearn.metrics.r2_score(y, y_pred)\n\n0.8276188554359157\n\n\nFind the Categorical Variables and dummify\n\nprint('rad: ' , boston['rad'].nunique())\nprint('zn: ' ,boston['zn'].nunique())\nprint('indus: ' ,boston['indus'].nunique())\nprint('chas: ' ,boston['chas'].nunique())\nprint('dis: ' ,boston['dis'].nunique())\nprint('tax: ' ,boston['tax'].nunique())\n\nrad:  9\nzn:  26\nindus:  76\nchas:  2\ndis:  412\ntax:  66\n\n\n\n# add Categoricals rad and chas\nest = smf.ols('medv ~ lstat + black + ptratio + tax + C(rad) + dis + age + rm + nox + C(chas) + indus + zn + crim + crim * zn + crim * indus + crim * chas + crim * nox + crim *rm + crim * age + crim * dis + crim * rad + crim * tax + crim * ptratio + crim * black + crim * lstat + zn * indus + zn * chas + zn * nox + zn * rm + zn * age + zn * dis + zn * rad + zn * tax + zn * ptratio + zn * black + zn * lstat + indus * chas + indus * nox + indus * rm + indus * age + indus * dis + indus * rad + indus * tax + indus * ptratio + indus * black + indus * lstat + chas * nox + chas * rm + chas * age + chas * dis + chas * rad + chas * tax + chas * ptratio + chas * black + chas * lstat + nox * rm + nox * age + nox * dis + nox * rad + nox * tax + nox * ptratio + nox * black + nox * lstat + rm * age + rm * dis + rm *rad + rm * tax + rm * ptratio + rm *black + rm * lstat + age * dis + age * rad + age * tax + age * ptratio + age * black + age * lstat + dis * rad + dis * tax + dis *ptratio + dis * black + dis * lstat + rad * tax + rad * ptratio + rad * black + rad * lstat + tax * ptratio + tax * black + tax * lstat + black * lstat ', boston).fit()\ny_pred = est.predict()\n\nsklearn.metrics.r2_score(y, y_pred)\n\n0.9247310711792476\n\n\nmost likely Overfitted Model with \\(R^2\\) of 0.92\n\n\nThe perfect fit\nMaximal Overfitted Model all predicting Variables as Categories + all possible Relationships\n\nest = smf.ols('medv ~ C(lstat) + C(black) + C(ptratio) + C(tax) + C(rad) + C(dis) + C(age) + C(rm) + C(nox) + C(chas) + C(indus) + C(zn) + C(crim) + crim * zn + crim * indus + crim * chas + crim * nox + crim *rm + crim * age + crim * dis + crim * rad + crim * tax + crim * ptratio + crim * black + crim * lstat + zn * indus + zn * chas + zn * nox + zn * rm + zn * age + zn * dis + zn * rad + zn * tax + zn * ptratio + zn * black + zn * lstat + indus * chas + indus * nox + indus * rm + indus * age + indus * dis + indus * rad + indus * tax + indus * ptratio + indus * black + indus * lstat + chas * nox + chas * rm + chas * age + chas * dis + chas * rad + chas * tax + chas * ptratio + chas * black + chas * lstat + nox * rm + nox * age + nox * dis + nox * rad + nox * tax + nox * ptratio + nox * black + nox * lstat + rm * age + rm * dis + rm *rad + rm * tax + rm * ptratio + rm *black + rm * lstat + age * dis + age * rad + age * tax + age * ptratio + age * black + age * lstat + dis * rad + dis * tax + dis *ptratio + dis * black + dis * lstat + rad * tax + rad * ptratio + rad * black + rad * lstat + tax * ptratio + tax * black + tax * lstat + black * lstat ', boston).fit()\ny_pred = est.predict()\n\nsklearn.metrics.r2_score(y, y_pred)\n\n1.0\n\n\n\n\nTrain Test Split\nHow do we quantify the notion of overfitting, i.e. the (obvious?) impression that the model is “too wiggly”, or too complex ?\nThe \\(R^2\\) on the data we used to fit the model is useless for this purpose because it seems to only improve the more complex the model becomes!\nOne useful idea seems the following: if the orange line does not really capture the “true model”, i.e. has adapted too much to the noise, its performance on a test set would be worse than a simpler model.\n\nLet us examine this idea by using a Train Test Split\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n#from sklearn.linear_model import LinearRegression\nboston.head()\n\n\n\n\n\n  \n    \n      \n      crim\n      zn\n      indus\n      chas\n      nox\n      rm\n      age\n      dis\n      rad\n      tax\n      ptratio\n      black\n      lstat\n      medv\n    \n  \n  \n    \n      0\n      0.00632\n      18.0\n      2.31\n      0\n      0.538\n      6.575\n      65.2\n      4.0900\n      1\n      296\n      15.3\n      396.90\n      4.98\n      24.0\n    \n    \n      1\n      0.02731\n      0.0\n      7.07\n      0\n      0.469\n      6.421\n      78.9\n      4.9671\n      2\n      242\n      17.8\n      396.90\n      9.14\n      21.6\n    \n    \n      2\n      0.02729\n      0.0\n      7.07\n      0\n      0.469\n      7.185\n      61.1\n      4.9671\n      2\n      242\n      17.8\n      392.83\n      4.03\n      34.7\n    \n    \n      3\n      0.03237\n      0.0\n      2.18\n      0\n      0.458\n      6.998\n      45.8\n      6.0622\n      3\n      222\n      18.7\n      394.63\n      2.94\n      33.4\n    \n    \n      4\n      0.06905\n      0.0\n      2.18\n      0\n      0.458\n      7.147\n      54.2\n      6.0622\n      3\n      222\n      18.7\n      396.90\n      5.33\n      36.2\n    \n  \n\n\n\n\n\nCompute the \\(R^2\\) for the test portion\nCompare with the adjusted \\(R^2\\).\nThink about the model complexity parameter in OLS.\nHow would one choose which variables should be part of the model?\n\n\ntrain, test = train_test_split(boston, test_size = 0.1, random_state=45)\n\ny_train = train[\"medv\"]\ny_test = test[\"medv\"]\n\n#will be useful later\nX_train = train.drop(\"medv\", axis=1)\nX_test = test.drop(\"medv\", axis=1)\n\n\n#Repeat one of the earlier amazing fits:\nformula = 'medv ~ lstat + black + ptratio + tax + C(rad) + dis + age + rm + nox + C(chas) + indus + zn + crim + crim * zn + crim * indus + crim * chas + crim * nox + crim *rm + crim * age + crim * dis + crim * rad + crim * tax + crim * ptratio + crim * black + crim * lstat + zn * indus + zn * chas + zn * nox + zn * rm + zn * age + zn * dis + zn * rad + zn * tax + zn * ptratio + zn * black + zn * lstat + indus * chas + indus * nox + indus * rm + indus * age + indus * dis + indus * rad + indus * tax + indus * ptratio + indus * black + indus * lstat + chas * nox + chas * rm + chas * age + chas * dis + chas * rad + chas * tax + chas * ptratio + chas * black + chas * lstat + nox * rm + nox * age + nox * dis + nox * rad + nox * tax + nox * ptratio + nox * black + nox * lstat + rm * age + rm * dis + rm *rad + rm * tax + rm * ptratio + rm *black + rm * lstat + age * dis + age * rad + age * tax + age * ptratio + age * black + age * lstat + dis * rad + dis * tax + dis *ptratio + dis * black + dis * lstat + rad * tax + rad * ptratio + rad * black + rad * lstat + tax * ptratio + tax * black + tax * lstat + black * lstat '\n\nest = smf.ols(formula, train).fit()\ny_pred_train = est.predict()\ny_pred_test = est.predict(X_test)\n\nR2_train = sklearn.metrics.r2_score(y_train, y_pred_train)\nR2_test = sklearn.metrics.r2_score(y_test, y_pred_test)\n\nprint(\"train R2:\", R2_train)\nprint(\"test R2:\", R2_test)\n\nMSE_test = mean_squared_error(y_test, y_pred_test)\nMSE_train = mean_squared_error(y_train, y_pred_train)\n\nprint(\"MSE_train:\", MSE_train)\nprint(\"MSE_test:\", MSE_test)\n\ntrain R2: 0.9274106134480603\ntest R2: 0.871592468610564\nMSE_train: 5.96861557537797\nMSE_test: 13.241124958441453"
  },
  {
    "objectID": "DS_Homework6.html#cross-validation",
    "href": "DS_Homework6.html#cross-validation",
    "title": "13  Sample Splitting",
    "section": "Cross Validation",
    "text": "Cross Validation\nDrawbacks of validation set approach\n\nThe validation estimate of the test error can be highly variable, depending on precisely which observations are included in the training/validation set.\nIn the validation approach, only a subset of the observations - those that are included in the training set rather than in the validation set - are used to fit the model.\nThis suggests that the validation set error may tend to overestimate the test error for the model fit on the entire data set.\n\nK-fold Cross-validation\n\nrandomly divide the data into K equal-sized parts. We leave out part k, fit the model to the other K-1 parts (combined), and then obtain predictions for the left-out kth part.\nThis is done in turn for each part \\(k = 1,2, \\ldots, K\\), and then the results are combined.\n\n\n\nDesign Matrix\nGet to know your friendly helper library patsy\n\nfrom patsy import dmatrices\n\ny_0, X_wide = dmatrices(formula, boston)#, return_type='dataframe')\nX_wide.shape\n\n(506, 99)\n\n\n\n\nScoring Metrics\nModel selection and evaluation using tools, such as model_selection.GridSearchCV and model_selection.cross_val_score, take a scoring parameter that controls what metric they apply to the estimators evaluated.\nFind the options here\n\nfrom sklearn.model_selection import cross_val_score\n\nreg_all = LinearRegression()\n#The unified scoring API always maximizes the score, \n# so scores which need to be minimized are negated in order for the unified scoring API to work correctly.\ncv_results = cross_val_score(reg_all, X_wide, y_0, cv=10, scoring='neg_mean_squared_error')\n\nprint(\"CV results: \", -cv_results)\nprint(\"CV results mean: \", np.mean(-cv_results))\n\nCV results:  [ 24.61090588  10.30866313  62.18781304  44.2004828  107.53306211\n  17.14942076  18.69173435 241.76832017 148.09771969  40.90216317]\nCV results mean:  71.54502851092425\n\n\nComments\n\nFor non-equal fold sizes, we need to compute the weighted mean!\nSetting \\(K = n\\) yields n-fold or leave-one out cross-validation (LOOCV).\nWith least-squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit! The following formula holds:\n\n\\[\nCV_n = \\frac{1}{n} \\sum{\\left( \\frac{y_i - \\hat{y}_i}{1-h_i} \\right)^2}\n\\]\nwhere \\(\\hat{y}_i\\) is the ith fitted value from the original least squares fit, and \\(h_i\\) is the leverage (see ISLR book for details.) This is like the ordinary MSE, except the ith residual is divided by \\(1-h_i\\).\n\n\nTask\n\nSketch the code for your own CV function.\nReproduce the left panel of Fig. 5.6, i.e. the right panel of Fig 2.9 from the ISLR book"
  },
  {
    "objectID": "DS_Lecture7.html",
    "href": "DS_Lecture7.html",
    "title": "14  Classification",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d\nimport seaborn as sns\n\nfrom sklearn.preprocessing import scale\nimport sklearn.linear_model as skl_lm\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nfrom scipy import stats\n#in response to: module 'scipy.stats' has no attribute 'chisqprob' \n#stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n#this one I inserted just before class, sorry !!\nfrom sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score #, log_loss\n\nfrom scipy import special\n%matplotlib inline\nsns.set_style('white')\n\n%precision 3\n\n'%.3f'\nLoad data"
  },
  {
    "objectID": "DS_Lecture7.html#logistic-regression",
    "href": "DS_Lecture7.html#logistic-regression",
    "title": "14  Classification",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nRecall our fit to the Titanic data from last week and the dilemma that some predictions and interpretations (such as the intercept) often led to survival probabilities outside the range \\([0,1]\\).\n\nest = smf.ols('survived ~ age + C(pclass) + C(sex)', titanic).fit()\nprint(est.summary().tables[1])\n\n==================================================================================\n                     coef    std err          t      P>|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          1.1250      0.051     22.202      0.000       1.026       1.225\nC(pclass)[T.2]    -0.2077      0.042     -4.983      0.000      -0.290      -0.126\nC(pclass)[T.3]    -0.4066      0.038    -10.620      0.000      -0.482      -0.331\nC(sex)[T.male]    -0.4795      0.031    -15.608      0.000      -0.540      -0.419\nage               -0.0055      0.001     -5.039      0.000      -0.008      -0.003\n==================================================================================\n\n\n\nest = smf.logit('survived ~ age + C(pclass) + C(sex)', data=titanic)\nprint(est.fit().summary())\n\nOptimization terminated successfully.\n         Current function value: 0.453279\n         Iterations 6\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:               survived   No. Observations:                  714\nModel:                          Logit   Df Residuals:                      709\nMethod:                           MLE   Df Model:                            4\nDate:                Sun, 30 May 2021   Pseudo R-squ.:                  0.3289\nTime:                        20:26:47   Log-Likelihood:                -323.64\nconverged:                       True   LL-Null:                       -482.26\nCovariance Type:            nonrobust   LLR p-value:                 2.074e-67\n==================================================================================\n                     coef    std err          z      P>|z|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          3.7770      0.401      9.416      0.000       2.991       4.563\nC(pclass)[T.2]    -1.3098      0.278     -4.710      0.000      -1.855      -0.765\nC(pclass)[T.3]    -2.5806      0.281     -9.169      0.000      -3.132      -2.029\nC(sex)[T.male]    -2.5228      0.207    -12.164      0.000      -2.929      -2.116\nage               -0.0370      0.008     -4.831      0.000      -0.052      -0.022\n==================================================================================\n\n\nThis is not the only shortcoming of linear regression (LR) for binary outcomes! Other problems include heteroskedasticity and incorrect scaling of probabilities even inside the range \\([0,1]\\).\nOne solution is to transform the linear output of the (LR) to an S-shape via the sigmoidal function \\(s(z) = 1/(1+exp(-z))\\), which is the strategy taken by logistic regression (example: Figure 4.2 from the ISLR book):\n\n#passenger Joe, aged 25, Pclass3:\noddsJoe = np.exp(3.777 -2.5806  -2.5228  -0.0370*25)\nprint(\"The odds of survival for Joe are\", str(oddsJoe))\nsurvProbJoe = oddsJoe/(1+oddsJoe)\nsurvProbJoe\nprint(\"The probability of survival for Joe are\", str(survProbJoe))\n\nThe odds of survival for Joe are 0.1052517688905321\nThe probability of survival for Joe are 0.09522877217033127\n\n\n\nfrom sklearn.metrics import classification_report\ny_true = y\n#The 50% cutoff/threshold is chosen by the user !!\ny_pred = prob_train[:,0] > 0.99\n#y_pred\n\nprint(classification_report(y_true, y_pred))#, target_names=['class 0', 'class 1']))\n\n              precision    recall  f1-score   support\n\n           0       0.16      0.00      0.00      9667\n           1       0.03      0.92      0.06       333\n\n    accuracy                           0.03     10000\n   macro avg       0.10      0.46      0.03     10000\nweighted avg       0.16      0.03      0.00     10000\n\n\n\n\npd.crosstab(y_true, y_pred)\n\n\n\n\n\n  \n    \n      col_0\n      False\n      True\n    \n    \n      default2\n      \n      \n    \n  \n  \n    \n      0\n      42\n      9625\n    \n    \n      1\n      100\n      233\n    \n  \n\n\n\n\n\n#Based on the confusion matrix:\n#for \"1=positive class\" we get\n#precision TP/(TP+FP):\nprint(\"precision TP/(TP+FP)\", str(np.round(233/(9625+233),3)))\n#precision TP/(TP+FN):\nprint(\"recall TP/(TP+FN)\", str(np.round(233/(100+233),3)))\n\n#for \"0=positive class\" we get\n#precision TP/(TP+FP):\nprint(\"precision TP/(TP+FP)\", str(np.round(42/(100+42),3)))\n#precision TP/(TP+FN):\nprint(\"recall TP/(TP+FN)\", str(np.round(42/(9625+42),3)))\n\nprecision TP/(TP+FP) 0.024\nrecall TP/(TP+FN) 0.7\nprecision TP/(TP+FP) 0.296\nrecall TP/(TP+FN) 0.004\n\n\n\nest = smf.logit('default2 ~ balance', data=df)\nprint(est.fit().summary())\n\nOptimization terminated successfully.\n         Current function value: 0.079823\n         Iterations 10\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:               default2   No. Observations:                10000\nModel:                          Logit   Df Residuals:                     9998\nMethod:                           MLE   Df Model:                            1\nDate:                Sun, 30 May 2021   Pseudo R-squ.:                  0.4534\nTime:                        20:26:48   Log-Likelihood:                -798.23\nconverged:                       True   LL-Null:                       -1460.3\nCovariance Type:            nonrobust   LLR p-value:                6.233e-290\n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -10.6513      0.361    -29.491      0.000     -11.359      -9.943\nbalance        0.0055      0.000     24.952      0.000       0.005       0.006\n==============================================================================\n\nPossibly complete quasi-separation: A fraction 0.13 of observations can be\nperfectly predicted. This might indicate that there is complete\nquasi-separation. In this case some parameters will not be identified.\n\n\n\n#Mini Tasks: fit a logistic regression to the Titanic data\n#Try to make sense of the coefficients!\nest = smf.logit('survived ~  C(pclass) + C(sex)', data=titanic)\nprint(est.fit().summary().tables[1])\n\nOptimization terminated successfully.\n         Current function value: 0.464023\n         Iterations 6\n==================================================================================\n                     coef    std err          z      P>|z|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          2.2971      0.219     10.490      0.000       1.868       2.726\nC(pclass)[T.2]    -0.8380      0.245     -3.424      0.001      -1.318      -0.358\nC(pclass)[T.3]    -1.9055      0.214     -8.898      0.000      -2.325      -1.486\nC(sex)[T.male]    -2.6419      0.184    -14.350      0.000      -3.003      -2.281\n==================================================================================\n\n\n\npHat = est.fit().predict()\npred = pHat > 0.5\npd.crosstab(pred, titanic.survived)\n\nOptimization terminated successfully.\n         Current function value: 0.464023\n         Iterations 6\n\n\n\n\n\n\n  \n    \n      survived\n      0\n      1\n    \n    \n      row_0\n      \n      \n    \n  \n  \n    \n      False\n      468\n      109\n    \n    \n      True\n      81\n      233\n    \n  \n\n\n\n\n\nCoefficients as Odds\nFor “normal regression” we know that the value of \\(\\beta_j\\) simply gives us \\(\\Delta y\\) if \\(x_j\\) is increased by one unit.\nIn order to fully understand the exact meaning of the coefficients for a LR model we need to first warm up to the definition of a link function and the concept of probability odds.\nUsing linear regression as a starting point\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\ldots +\\beta_k x_{k,i} + \\epsilon_i\n\\]\nwe modify the right hand side such that (i) the model is still basically a linear combination of the \\(x_j\\)s but (ii) the output is -like a probability- bounded between 0 and 1. This is achieved by “wrapping” a sigmoid function \\(s(z) = 1/(1+exp(-z))\\) around the weighted sum of the \\(x_j\\)s:\n\\[\ny_i = s(\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\ldots +\\beta_k x_{k,i} + \\epsilon_i)\n\\]\nThe sigmoid function, depicted below to the left, transforms the real axis to the interval \\((0;1)\\) and can be interpreted as a probability.\n\nThe inverse of the sigmoid is the logit (depicted above to the right), which is defined as \\(log(p/(1-p))\\). For the case where p is a probability we call the ratio \\(p/(1-p)\\) the probability odds. Thus, the logit is the log of the odds and logistic regression models these log-odds as a linear combination of the values of x.\nFinally, we can interpret the coefficients directly: the odds of a positive outcome are multiplied by a factor of \\(exp(\\beta_j)\\) for every unit change in \\(x_j\\). (In that light, logistic regression is reminiscient of linear regression with logarithmically transformed dependent variable which also leads to multiplicative rather than additive effects.)\nSummary\n\\[\np(x) = \\frac{e^{\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k}}{1 + e^{\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k}}\n\\]\nOdds\n\\[\n\\frac{p(x)}{1-p(x)} = e^{\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_k x_k}\n\\]\nThis post has a more detailed view on the interpretations of the coefficients:\nhttps://blog.hwr-berlin.de/codeandstats/interpretation-of-the-coefficients-in-logistic-regression/\n\nComments\n\nWhen your data are linearly separable there is (ironically) a fitting problem ! See iris example below\nLogistic regression preserves the marginal probabilities. The sum of the predicted probability scores for any subgroup of the training data (which includes all of it) will be equal to the number of positives.\nWhat is deviance ? Deviance (also referred to as log loss) is a measure of how well the model fits the data. It is 2 times the negative log likelihood of the dataset, given the model.\n\n\\[\ndev = - \\sum_i{y_i \\cdot \\log p_i + (1-y_i) \\cdot \\log (1-p_i)}\n\\]\nIn Python, you can use the log_loss function from scikit-learn, with documentation found here. If you think of deviance as analogous to variance, then the null deviance is similar to the variance of the data around the average rate of positive examples. The residual deviance is similar to the variance of the data around the model. As an exercise we will calculate the deviances in a homework.\n\nPseudo \\(R^2\\) McFadden’s \\(R^2\\) is defined as \\(1−LL_{mod}/LL_0\\), where \\(LL_{mod}\\) is the log likelihood value for the fitted model and \\(LL_{0}\\) is the log likelihood for the null model which includes only an intercept as predictor (so that every individual is predicted the same probability of ‘success’).\n\nFor a logistic regression model the log likelihood value is always negative (because the likelihood contribution from each observation is a probability between 0 and 1). If your model doesn’t really predict the outcome better than the null model, \\(LL_{mod}\\) will not be much larger than \\(LL_{0}\\) , and so \\(LL_{mod}/LL_0 \\sim 1\\) , and McFadden’s pseudo-R2 is close to 0 (your model has no predictive value).\nConversely if your model was really good, those individuals with a success (1) outcome would have a fitted probability close to 1, and vice versa for those with a failure (0) outcome. In this case if you go through the likelihood calculation the likelihood contribution from each individual for your model will be close to zero, such that \\(LL_{mod}\\) is close to zero, and McFadden’s pseudo-R2 squared is close to 1, indicating very good predictive ability."
  },
  {
    "objectID": "DS_Lecture7.html#roc-curves",
    "href": "DS_Lecture7.html#roc-curves",
    "title": "14  Classification",
    "section": "ROC curves",
    "text": "ROC curves\n\n\nfpr, tpr, thresholds = metrics.roc_curve(y, scores)\nroc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n#or\n\nfrom sklearn.metrics import RocCurveDisplay\n\nRocCurveDisplay.from_predictions(y_test, y_pred)\nplt.show()\n\n\n\ndef logloss(true_label, predicted, eps=1e-15):\n    p = np.clip(predicted, eps, 1 - eps)\n    if true_label == 1:\n        return -np.log(p)\n    else:\n        return -np.log(1 - p)\n\n\np= np.linspace(0.001,1,100)#.reshape(-1,1)\nplt.plot(logloss(1,p), \"b-\");\n\n\n\n\n\nThink Stats Data\nThe NSFG dataset includes 244 variables about each pregnancy and another 3087 variables about each respondent. Maybe some of those variables have predictive power. To find out which ones are most useful, why not try them all? Testing the variables in the pregnancy table is easy, but in order to use the variables in the respondent table, we have to match up each pregnancy with a respondent. In theory we could iterate through the rows of the pregnancy table, use the caseid to find the corresponding respondent, and copy the values from the correspondent table into the pregnancy table. But that would be slow.\nA better option is to recognize this process as a join operation as defined in SQL and other relational database languages (see). Join is implemented as a DataFrame method, so we can perform the operation like this:\ndo not run this cell, for completeness, I show how the joined dataframe was created:\nfrom __future__ import print_function, division\nimport numpy as np\nimport nsfg\n\npreg = pd.read_hdf('../data/pregNSFG.h5', 'df')\n#only look at live births\nlive = preg[preg.outcome == 1]\n\nlive = live[live.prglngth>30]\nresp = nsfg.ReadFemResp()\nresp.index = resp.caseid\njoin = live.join(resp, on='caseid', rsuffix='_r')\n#save to native python format:\n#http://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.to_hdf.html\njoin.to_hdf('JoinedpregNSFG.h5', key='df', format='table',complevel =9)\n\nlive = pd.read_hdf('../data/JoinedpregNSFG.h5','df')\nlive.head()\n\n\n#define first babies\nfirsts = live[live.birthord == 1]\n#and all others:\nothers = live[live.birthord != 1]\n\n\n# from this discussion, it seems that statsmodels still uses the defunct\n# chisqprob, so we have to define it ourselves:\n# https://github.com/statsmodels/statsmodels/issues/3931\nfrom scipy import stats\nstats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\nstats.chisqprob(10,3)\n\nThe mother’s age seems to have a small, non significant effect.\n\nlive['boy'] = (live.babysex==1).astype(int)\nSexvsAge = smf.logit('boy ~ agepreg', data=live)\nresults = SexvsAge.fit()\nprint(results.summary())\n\n\nlive[\"fmarout5\"].value_counts()\n\n\n\nThe Trivers-Willard hypothesis\nExercise 11.2 The Trivers-Willard hypothesis suggests that for many mammals the sex ratio depends on “maternal condition”; that is, factors like the mother’s age, size, health, and social status. See. Some studies have shown this effect among humans, but results are mixed. As an exercise, use a data mining approach to test the other variables in the pregnancy and respondent files.\nIn the solution for exercise 11.2 the author uses a data mining approach to find the “best” model:\n(Task: can we find out the meaning of the 2 new variables??)\n\nformula='boy ~ agepreg + fmarout5==5 + infever==1'\nmodel = smf.logit(formula, data=live)\nresults = model.fit()\nprint(results.summary())\n\n\n\nTasks\n\nCompute the ROC curve and AUC for the NSFG data\nUse cross validation to estimate some accuracy measure of classification for the\n\nTitanic survival\nsex prediction for the NSFG data\n\nTranslate the coefficient for Pclass 3 into both odds and probability of survival (compared to the reference level Pclass 1).\nCompute the survival probability of the first passenger in the data set.\n\n\nimport patsy \n\ny, X = patsy.dmatrices('survived ~ age + C(pclass) + C(sex)', titanic)\n#y = titanic[\"survived\"]\n\n\nfrom sklearn.model_selection import cross_val_score\nclf = skl_lm.LogisticRegression() # solver='newton-cg')\n\ncv_results = cross_val_score(clf, X, np.ravel(y), cv=10)\nprint(np.round(cv_results,2))\n\n[0.78 0.74 0.81 0.88 0.75 0.79 0.79 0.73 0.82 0.82]\n\n\n\n\nThe Iris dataset\n\n\nfrom sklearn import datasets\n\nplt.style.use('ggplot')\niris = datasets.load_iris()\nprint(iris.data.shape)\n\nX = iris.data\ny = iris.target\ndf = pd.DataFrame(X, columns=iris.feature_names)\n_ = pd.plotting.scatter_matrix(df, c = y, figsize = [8, 8],s=150, marker = 'D');\n\n(150, 4)\n\n\n\n\n\nLooks like we could build a perfect classifier with just petal width ?\n\n\niris.feature_names\n\n['sepal length (cm)',\n 'sepal width (cm)',\n 'petal length (cm)',\n 'petal width (cm)']\n\n\n\ndf[\"setosa\"]= (y==0)\ndf.head()\ndf.columns\n\nIndex(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n       'petal width (cm)', 'setosa'],\n      dtype='object')\n\n\n\nX = iris[\"data\"][:,3:]  # petal width\n\nlogit = sm.Logit((iris[\"target\"]==0).astype(np.int), X)\nlogit.fit().params\n\nOptimization terminated successfully.\n         Current function value: 0.354801\n         Iterations 7\n\n\narray([-1.847])\n\n\n\n#sklearn\nfrom sklearn.linear_model import LogisticRegression\n\ndef LogReg(xCol=3, target=2,penalty=\"l2\"):\n    X = iris[\"data\"][:,xCol:]  # petal width\n    y = (iris[\"target\"]==target).astype(np.int)\n\n    log_reg = LogisticRegression(penalty=penalty)\n    log_reg.fit(X,y)\n\n    X_new = np.linspace(0,3,1000).reshape(-1,1)\n    y_proba = log_reg.predict_proba(X_new)\n\n    flowerType=[\"setosa\", \"versicolor\", \"virginica\"]\n \n    plt.plot(X,y,\"b.\")\n    plt.plot(X_new,y_proba[:,1],\"g-\",label=flowerType[target])\n    plt.plot(X_new,y_proba[:,0],\"b--\",label=\"not \" + flowerType[target])\n    plt.xlabel(iris.feature_names[xCol], fontsize=14)\n    plt.ylabel(\"Probability\", fontsize=14)\n    plt.legend(loc=\"upper left\", fontsize=14)\n    plt.show()\n    \n    return log_reg\n\nlog_reg = LogReg()\n\nlog_reg.predict([[1.7],[1.5]])\n\n\n\n\narray([1, 0])\n\n\n\nlog_reg = LogReg(target=0)"
  },
  {
    "objectID": "DS_Lecture7.html#other-classifiers",
    "href": "DS_Lecture7.html#other-classifiers",
    "title": "14  Classification",
    "section": "Other Classifiers",
    "text": "Other Classifiers\n\nK Nearest Neighbors\n\nfrom sklearn.neighbors import KNeighborsClassifier\niris = datasets.load_iris()\nknn = KNeighborsClassifier(n_neighbors=6)\nknn.fit(iris['data'], iris['target'])\n\n\nKNeighborsClassifier(n_neighbors=6)\n\n\nTask: Split the iris data into training and test. Predict on test\n\n#prediction = knn.predict(X_test)\n#print('Prediction {}’.format(prediction))\n\n\n\nMultinomial Logistic Regression\n\nimport statsmodels.api as st\n#different way of importing data\niris = st.datasets.get_rdataset('iris', 'datasets')\n \ny = iris.data.Species\n \ny.head(3)\n\nx = iris.data.iloc[:, 0]\n \nx = st.add_constant(x, prepend = False)\n \nx.head()\n\n\n\n\n\n  \n    \n      \n      Sepal.Length\n      const\n    \n  \n  \n    \n      0\n      5.1\n      1.0\n    \n    \n      1\n      4.9\n      1.0\n    \n    \n      2\n      4.7\n      1.0\n    \n    \n      3\n      4.6\n      1.0\n    \n    \n      4\n      5.0\n      1.0\n    \n  \n\n\n\n\n\nmdl = st.MNLogit(y, x)\n \nmdl_fit = mdl.fit()\n\nprint(mdl_fit.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.606893\n         Iterations 8\n                          MNLogit Regression Results                          \n==============================================================================\nDep. Variable:                Species   No. Observations:                  150\nModel:                        MNLogit   Df Residuals:                      146\nMethod:                           MLE   Df Model:                            2\nDate:                Sun, 30 May 2021   Pseudo R-squ.:                  0.4476\nTime:                        20:27:58   Log-Likelihood:                -91.034\nconverged:                       True   LL-Null:                       -164.79\nCovariance Type:            nonrobust   LLR p-value:                 9.276e-33\n=====================================================================================\nSpecies=versicolor       coef    std err          z      P>|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nSepal.Length           4.8157      0.907      5.310      0.000       3.038       6.593\nconst                -26.0819      4.889     -5.335      0.000     -35.665     -16.499\n--------------------------------------------------------------------------------------\nSpecies=virginica       coef    std err          z      P>|z|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nSepal.Length          6.8464      1.022      6.698      0.000       4.843       8.850\nconst               -38.7590      5.691     -6.811      0.000     -49.913     -27.605\n=====================================================================================\n\n\n\n### marginal effects ###\n \nmdl_margeff = mdl_fit.get_margeff()\n \nprint(mdl_margeff.summary())\n\n       MNLogit Marginal Effects      \n=====================================\nDep. Variable:                Species\nMethod:                          dydx\nAt:                           overall\n=====================================================================================\n    Species=setosa      dy/dx    std err          z      P>|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nSepal.Length          -0.3785      0.003   -116.793      0.000      -0.385      -0.372\n--------------------------------------------------------------------------------------\nSpecies=versicolor      dy/dx    std err          z      P>|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nSepal.Length           0.0611      0.022      2.778      0.005       0.018       0.104\n--------------------------------------------------------------------------------------\nSpecies=virginica      dy/dx    std err          z      P>|z|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nSepal.Length          0.3173      0.022     14.444      0.000       0.274       0.360\n====================================================================================="
  }
]